[[ch-features]]
== DRBDの機能

本章ではDRBDの有用な機能とその背景にある情報も紹介します。ほとんどのユーザにとって非常に重要な機能もありますが、特定の利用目的に対して重要な機能もあります。これらの機能を使うために必要な設定方法については、「<<ch-admin>>」および「<<ch-troubleshooting>>」を参照してください。

[[s-single-primary-mode]]
===  シングルプライマリモード

シングルプライマリモードでは、個々の<<s-resources,リソース>>は、任意の時点でクラスタメンバのどれか1台のみプライマリになれます。どれか1台のクラスタノードのみがデータを更新できることが保証されるため、従来の一般的なファイルシステム(ext3、ext4、XFSなど)を操作するのに最適な処理モードと言えます。

一般的なハイアベイラビリティクラスタ(フェイルオーバタイプ)を実現する場合、DRBDをシングルプライマリモードで設定してください。

[[s-dual-primary-mode]]
=== デュアルプライマリモード

デュアルプライマリモードでは、すべてのリソースが任意の時点で両方のノード上でプライマリロールになれます。両方のノードから同一のデータに同時にアクセスできるため、分散ロックマネージャを持つ共有クラスタファイルシステムの利用がこのモードに必須です。利用できるファイルシステムには<<ch-gfs,GFS>>および<<ch-ocfs2,OCFS2>>があります。

2つのノード経由でのデータへの同時アクセスが必要なクラスタシステムの負荷分散をはかりたい場合、デュアルプライマリモードが適しています。このモードはデフォルトでは無効になっており、DRBD設定ファイルで明示的に有効にする必要があります。

特定のリソースに対して有効にする方法については、<<s-enable-dual-primary>>を参照してください。

[[s-replication-protocols]]
=== レプリケーションのモード

DRBDは3種類のレプリケーションモードをサポートしています。

[[fp-protocol-a]]
.Protocol A
非同期レプリケーションプロトコル。プライマリノードでのディスクへの書き込みは、自機のディスクに書き込んだ上でレプリケーションパケットを自機のTCP送信バッファに送った時点で、完了したと判断されます。システムクラッシュなどの強制的なフェイルオーバが起こると、データを紛失する可能性があります。クラッシュが原因となったフェイルオーバが起こった場合、待機系ノードのデータは整合性があると判断されますが、クラッシュ直前のアップデート内容が反映されない可能性があります。プロトコルAは、遠隔地へのレプリケーションに最も適しています。DRBD
Proxyと組み合わせて使用すると、効果的なディザスタリカバリソリューションとなります。詳しくは<<s-drbd-proxy>>を参照してください。


[[fp-protocol-b]]
.Protocol B
メモリ同期(半非同期)レプリケーションプロトコル。プライマリノードでのディスクへの書き込みは、自機のディスクに書き込んだ上でレプリケーションパケットが他機に届いた時点で、完了したと判断されます。通常、システムクラッシュなどの強制的なフェイルオーバでのデータ紛失は起こりません。しかし、両ノードに同時に電源障害が起こり、プライマリノードのストレージに復旧不可能な障害が起きると、プライマリ側にのみ書き込まれたデータを失う可能性があります。

[[fp-protocol-c]]
.Protocol C
同期レプリケーションプロトコル。プライマリノードでのディスクへの書き込みは、両ノードのディスクへの書き込みが終わった時点で完了したと判断されます。このため、どちらかのノードでデータを失っても、系全体としてのデータ紛失には直結しません。当然ながら、このプロトコルを採用した場合であっても、両ノードまたはそのストレージサブシステムに復旧できない障害が同時に起こると、データは失われます。

このような特質にもとづき、もっとも一般的に使われているプロトコルはCです。

レプリケーションプロトコルを選択するときに考慮しなければならない要因が2つあります。データ保護とレイテンシ(待ち時間)です。一方で、レプリケーションプロトコルの選択はスループットにはほとんど影響しません。

レプリケーションプロトコルの設定例については、<<s-configure-resource>>を参照してください。

[[s-replication-transports]]
=== 複数の転送プロトコル

DRBDのレプリケーションと同期フレームワークのソケットレイヤは、複数のトランスポートプロトコルをサポートします。

.TCP over IPv4
標準的かつDRBDのデフォルトのプロトコルです。IPv4が有効なすべてのシステムで利用できます。

.TCP over IPv6
レプリケーションと同期用のTCPソケットの設定においては、IPv6もネットワークプロトコルとして使用できます。アドレシング方法が違うだけで、動作上もパフォーマンス上もIPv4と変わりはありません。

.SDP
SDPは、InfiniBandなどのRDMAに対応するBSD形式ソケットです。最新のディストリビューションでは、SDPはOFEDスタックのの一部として利用できます。SDPはIPv4形式のアドレシングに使用します。インフィニバンドを内部接続に利用すると、SDPによる高スループット、低レイテンシのDRBDレプリケーションネットワークを実現することができます。

.SuperSockets
スーパーソケットはTCP/IPスタック部分と置き換え可能なソケット実装で、モノリシック、高効率、RDMA対応などの特徴を持っています。きわめてレイテンシが低いレプリケーションを実現できるプロトコルとして、DRBDはSuperSocketsをサポートしています。現在のところ、SuperSocketsはDolphin
Interconnect Solutionsが販売するハードウェアの上でのみ利用できます。

[[s-resync]]
=== 効率的なデータ同期

同期ならびに再同期は、レプリケーションとは区別されます。レプリケーションは、プライマリノードでのデータの書き込みに伴って実施されますが、同期はこれとは無関係です。同期はデバイス全体の状態に関わる機能です。

プライマリノードのダウン、セカンダリノードのダウン、レプリケーション用ネットワークのリンク中断など、さまざまな理由によりレプリケーションが一時中断した場合、同期が必要になります。DRBDの同期は、もともとの書き込み順序ではなくリニアに書き込むロジックを採用しているため、効率的です。

* 何度も書き込みが行われたブロックの場合でも、同期は1回の書き込みですみます。このため、同期は高速です。

* ディスク上のブロックレイアウトを考慮して、わずかなシークですむよう、同期は最適化されています。

* 同期実行中は、スタンバイノードの一部のデータブロックの内容は古く、残りは最新の状態に更新されています。この状態のデータは _inconsistent_
  (不一致)と呼びます。

DRBDでは、同期はバックグラウンドで実行されるので、アクティブノードのサービスは同期によって中断されることはありません。

IMPORTANT: データに不一致箇所が残っているノードは、多くの場合サービスに利用できません。このため、不一致である時間を可能な限り短縮することが求められます。そのため、DRBDは同期直前のLVMスナップショットを自動で作成するLVM統合機能を実装しています。これは同期中であっても対向ノードと
_consistent_ (一致する)一致するコピーを保証します。この機能の詳細については<<s-lvm-snapshots>>を参照してください。

[[s-variable-rate-sync]]
==== 可変レート同期

可変レート同期(デフォルト)の場合、DRBDは同期のネットワーク上で利用可能な帯域幅を検出し、それと、フォアグランドのアプリケーションI/Oからの入力とを比較する、完全自動制御ループに基づいて、最適な同期レートを選択します。

可変レート同期に関する設定の詳細については、<<s-configure-sync-rate-variable>>を参照してください。

[[s-fixed_rate_synchronization]]
==== 固定レート同期

固定レート同期の場合、同期ホストに対して送信される1秒あたりのデータ量(同期速度)には設定可能な静的な上限があります。この上限に基づき、同期に必要な時間は、次の簡単な式で予測できます。

[[eq-resync-time]]
[equation]
.同期時間
image::images/resync-time.svg[]

_t~sync~_ は同期所要時間の予測値です。 _D_
は同期が必要なデータ量で、リンクが途絶えていた間にアプリケーションによって更新されたデータ量です。 _R_
は設定ファイルに指定した同期速度です。ただし実際の同期速度はネットワークやI/Oサブシステムの性能による制約を受けます。

固定レート同期に関する設定の詳細については<<s-configure-sync-rate>>を参照してください。

[[s-checksum-sync]]
==== チェックサムベース同期

[[p-checksum-sync]]
DRBDの同期アルゴリズムは、データダイジェスト(チェックサム)を使うことによりさらに効率化されています。チェックサムベースの同期を行うことで、より効率的に同期対象ブロックの書き換えが行われます。DRBDは同期を行う前にブロックを読み込みディスク上のコンテンツのハッシュを計算します。このハッシュと、相手ノードの同じセクタのハッシュを比較して、値が同じであれば、そのブロックを同期での書き換え対象から外します。これにより、DRBDが切断モードから復旧し再同期するときなど、同期時間が劇的に削減されます。

同期に関する設定の詳細は<<s-configure-checksum-sync>>を参照してください。


[[s-suspended-replication]]
=== レプリケーションの中断

DRBDが正しく設定されていれば、DRBDはレプリケーションネットワークが輻輳していることを検出し、レプリケーションを中断します。この場合、プライマリノードはセカンダリから引き離され一時的に同期しない状態になりますが、セカンダリでは一致するコピーを保持したままです。帯域幅が確保されると、自動で同期が再開し、バックグラウンド同期が行われます。

レプリケーションの中断は、データセンタやクラウドインスタンス間との共有接続で遠隔地レプリケーションを行うような、可変帯域幅での接続の場合に通常利用されます。

輻輳のポリシーとレプリケーションの停止についてほ詳細は<<s-configure-congestion-policy>>をご参照ください。

[[s-online-verify]]
=== オンライン照合

オンライン照合機能を使うと、2ノードのデータの整合性を、ブロックごとに効率的な方法で確認できます。

ここで効率的というのはネットワーク帯域幅を効率的に利用することを意味しています。また、照合によって冗長性が損なわれることはありません。しかしオンライン照合はCPU使用率やシステム負荷を高めます。この意味では、オンライン照合はリソースを必要とします。

一方のノード(照合ソース)で、低レベルストレージデバイスのブロックごとのダイジェストを計算します。DRBDはダイジェストを他方のノード(照合ターゲット)に転送し、そこでローカルの対応するブロックのダイジェストと照合します。ダイジェストが一致しないブロックはout-of-syncとマークされ、後で同期が行われます。DRBDが転送するのはダイジェストであって、ブロックのデータそのものではありません。このため、オンライン照合はネットワーク帯域幅をきわめて効率的に活用します。

このプロセスは、照合対象のDRBDリソースを利用したまま実行できます。これがオンラインの由来です。照合によるパフォーマンス低下は避けられませんが、照合およびその後の同期作業全体を通じてサービスの停止やシステム全体を停止する必要はありません。

オンライン照合は、週または月に1回程度の頻度でcronデーモンから実行するのが妥当です。オンライン照合機能を有効にして実行する方法や、これを自動化する方法については、<<s-use-online-verify>>を参照してください。

[[s-integrity-check]]
=== レプリケーション用トラフィックの整合性チェック

DRBDは、暗号手法にもとづくMD5、SHA-1またはCRD-32Cを使って、ノード間のメッセージの整合性をチェックできます。

DRBD自身はメッセージダイジェストアルゴリズムを備えていません。Linuxカーネルの暗号APIが提供する機能を単に利用するだけです。したがって、カーネルが備えるアルゴリズムであれば、どのようなものでも利用可能です。

本機能を有効にすると、レプリケート対象のすべてのデータブロックごとのメッセージダイジェストが計算されます。レプリケート先のDRBDは、レプリケーション用パケットの照合にこのメッセージダイジェストを活用します。データの照合が失敗したら、レプリケート先のDRBDは、失敗したブロックに関するパケットの再送を求めます。この機能を使うことで、データの損失を起こす可能性がある次のようなさまざまな状況への備えが強化され、DRBDによるレプリーションが保護されます。

* 送信側ノードのメインメモリとネットワークインタフェースの間で生じたビット単位エラー(ビット反転)。この種のエラーは、多くのシステムにおいてTCPレベルのチェックサムでは検出できません。

* 受信側ノードのネットワークインタフェースとメインメモリの間で生じたビット反転。TCPチェックサムが役に立たないのは前項と同じです。

* 何らかのリソース競合やネットワークインタフェースまたはそのドライバのバグなどによって生じたデータの損傷。

* ノード間のネットワークコンポーネントが再編成されるときなどに生じるビット反転やデータ損傷。このエラーの可能性は、ノード間をネットワークケーブルで直結しなかった場合に考慮する必要があります。

レプリケーショントラフィックの整合性チェックを有効にする方法については、<<s-configure-integrity-check>>をご参照ください。

[[s-split-brain-notification-and-recovery]]
===  スプリットブレインの通知と自動修復

クラスタノード間のすべての通信が一時的に中断され、クラスタ管理ソフトウェアまたは人為的な操作ミスによって両方のノードがプライマリになった場合に、スプリットブレインの状態に陥ります。それぞれのノードでデータの書き換えが行われることが可能になってしまうため、この状態はきわめて危険です。つまり、2つの分岐したデータセットが作られてしまう軽視できない状況に陥る可能性が高くなります。

クラスタのスプリットブレインは、Heartbeatなどが管理するホスト間の通信がすべて途絶えたときに生じます。これとDRBDのスプリットブレインは区別して考える必要があります。このため、本書では次のような記載方法を使うことにします。

* スプリットブレインは、DRBDのスプリットブレインと表記します。

* クラスタノード間のすべての通信の断絶のことをクラスタ断絶と表記します。これはクラスタのスプリットブレインのことです。

スプリットブレインに陥ったことを検出すると、DRBDは電子メールまたは他の方法によって管理者に自動的に通知できます。この機能を有効にする方法については<<s-split-brain-notification>>を参照してください。

スプリットブレインへの望ましい対処方法は、<<s-resolve-split-brain,手動回復>>を実施した後、根本原因を取り除くことです。しかし、ときにはこのプロセスを自動化する方がいい場合もあります。自動化のために、DRBDは以下のいくつかのアルゴリズムを提供します。

* *「若い」プライマリ側の変更を切り捨てる方法* ネットワークの接続が回復してスプリットブレインを検出すると、DRBDは最後にプライマリに切り替わったノードのデータを切り捨てます。

* *「若い」プライマリ側の変更を切り捨てる方法* DRBDは最初にプライマリに切り替わったノードの変更を切り捨てます。

* *変更が少ないプライマリ側の変更を切り捨てる方法* DRBDは2つのノードでどちらが変更が少ないかを調べて、少ない方のノードのすべてを切り捨てます。

* *片ノードに変更がなかった場合の正常回復* もし片ノードにスプリットブレインの間にまったく変更がなかった場合、DRBDは正常に回復し、修復したと判断します。しかし、こういった状況はほとんど考えられません。仮にリードオンリーでファイルシステムをマウントしただけでも、デバイスへの書き換えが起きるた
  めです。

自動修復機能をa使うべきかどうかの判断は、個々のアプリケーションに強く依存します。変更量が少ないノードのデータを切り捨てるアプローチは、ある種のWebアプリケーションの場合適しているかもしれません。一方で、金融関連のデータベースアプリケーションでは、どのような変更であっても自動的に切り捨てるようなことは受け入れがたく、どのようなスプリットブレインの場合でも手動回復が望ましいでしょう。スプリットブレイン自動修復機能を使う場合、アプリケーションの特性を十分に考慮してください。

DRBDのスプリットブレイン自動修復機能を設定する方法については、<<s-automatic-split-brain-recovery-configuration>>を参照してください。

[[s-disk-flush-support]]
=== ディスクフラッシュのサポート

ローカルディスクやRAID論理ディスクでライトキャッシュが有効な場合、キャッシュにデータが記録された時点でデバイスへの書き込みが完了したと判断されます。このモードは一般にライトバックモードと呼ばれます。このような機能がない場合の書き込みはライトスルーモードです。ライトバックモードで運用中に電源障害が起きると、最後に書き込まれたデータはディスクにコミットされず、データを紛失する可能性があります。

この影響を緩和するために、DRBDはディスクフラッシュを活用します。ディスクフラッシュは書き込みオペレーションのひとつで、対象のデータが安定した(不揮発性の)ストレージに書き込まれるまで完了しません。すなわち、キャッシュへの書き込みではなくディスクへの書き込みを保証します。DRBDは、レプリケートするデータとそのメタデータをディスクに書き込むときに、フラッシュ命令を発行します。実際には、DRBDは<<s-activity-log,アクティビティログ>>の更新時や書き込みに依存性がある場合などにはライトキャッシュへの書き込みを迂回します。このことにより、電源障害の可能性に対する信頼性が高まっています。

しかしDRBDがディスクフラッシュを活用できるのは、直下のディスクデバイスがこの機能をサポートしている場合に限られることに注意してください。最近のカーネルは、ほとんどのSCSIおよびSATAデバイスに対するフラッシュをサポートしています。LinuxソフトウェアRAID
(md)は、直下のデバイスがサポートする場合に限り、RAID-1に対してフラッシュをサポートします。デバイスマッパ(LVM2、dm-raid、マルチパス)もフラッシュをサポートしています。

電池でバックアップされた書き込みキャッシュ(BBWC)は、電池からの給電による消失しないストレージです。このようなデバイスは、電源障害から回復したときに中断していたディスクへの書き込みをディスクにフラッシュできます。このため、キャッシュへの書き込みは、事実上安定したストレージへの書き込みと同等とみなせます。この種のデバイスが使える場合、DRBDの書き込みパフォーマンス向上させるためにフラッシュを無効に設定できます。詳細は<<s-disable-flushes>>を参照ください。

[[s-handling-disk-errors]]
=== ディスクエラー処理ストラテジー

どちらかのノードのDRBD下位ブロックデバイスがI/Oエラーを返したときに、DRBDがそのエラーを上位レイヤ(多くの場合ファイルシステム)に伝えるかどうかを制御できます。

[[fp-io-error-pass-on]]
.I/Oエラーを伝える
pass
onを指定すると、下位レベルのエラーをDRBDはそのまま上位レイヤに伝えます。したがって、そのようなエラーへの対応(ファイルシステムをリードオンリーでマウントしなおすなど)は上位レイヤに任されます。このモードはサービスの継続性を損ねることがあるので、多くの場合推奨できない設定だといえます。

[[fp-io-error-detach]]
.I/Oエラーを伝えない
_detach_
を設定すると、最初の下位レイヤでのI/Oエラーに対して、DRBDは自動的にそのレイヤを切り離します。上位レイヤにI/Oエラーは伝えられず、該当ブロックのデータはネットワーク越しに対向ノードに書き込まれます。その後DRBDはディスクレスモードと呼ばれる状態になり、すべてのI/Oは対向ノードに対して読み込んだり、書き込むようになります。このモードでは、パフォーマンスは犠牲になりますが、サービスは途切れることなく継続できます。また、都合のいい任意の時点でサービスを対向ノードに移動させることができます。

I/Oエラー処理方針を設定する方法については<<s-configure-io-error-behavior>>を参照してください。.

[[s-outdate]]
=== 無効データの処理ストラテジー

DRBDはデータの _inconsistent(不整合状態)_ と _outdated(無効状態)_
を区別します。不整合とは、いかなる方法でもアクセスできずしたがって利用できないデータ状態です。たとえば、進行中の同期先のデータが不整合データの例です。この場合、ノードのデータは部分的に古く、部分的に新しくなっており、ノード間の同期は不可能になります。下位デバイスの中にファイルシステムが入っていたら、このファイルシステムは、マウントはもちろんチェックも実行できません。

無効データは、セカンダリノード上のデータで、整合状態にあるもののプライマリ側と同期していない状態のデータをさします。一時的か永続的かを問わず、レプリケーションリンクが途切れたときに、この状態が生じます。リンクが切れている状態でのセカンダリ側の無効データは、クリーンではあるものの、対向ノードのデータ更新が反映されず古いデータ状態になっている可能性があります。サービスが無効データを使ってしまうことを防止するために、DRBDは無効データを<<s-resource-roles,プライマリに切り替える>>ことを許可しません。

ネットワークの中断時にセカンダリノードのデータを無効に設定するためのインタフェースをDRBDは提供しています。このための通信には、DRBDのレプリケーションリンクとは別のネットワーク通信チャネルを使います。DRBDは無効データをアプリケーションが使ってしまうことを防止するために、このノードがプライマリになることを拒絶します。本機能の完全は実装は、DRBDレプリケーションリンクから独立した通信経路を使用する<<ch-pacemaker,クラスタ管理フレームワーク>>用になされていますが、
しかしこのAPIは汎用的なので、他のクラスタ管理アプリケーションでも容易に本機能を利用できます。

レプリケーションリンクが復活すると、無効に設定されたリソースの無効フラグは自動的にクリアされます。そして<<s-resync,バックグラウンド同期>>が実行されます。

誤って無効データを使ってしまうことを防止するための設定例については、<<s-pacemaker-fencing-dopd,DRBD無効化デーモン(dopd)>>を参照してください。

[[s-three-way-repl]]
=== 3ノードレプリケーション

NOTE: この機能はDRBDバージョン8.3.0以上で使用可能です。

3ノードレプリケーションとは、2ノードクラスタに3番目のノードを追加してDRBDでレプリケーションするものです。この方法は、バックアップやディザスタリカバリのために使われます。この構成においては、<<s-drbd-proxy>>の内容も関係しています。

3ノードレプリケーション既存のDRBDリソースの上にもうひとつのDRBDリソースを積み重ねることによって実現されます。次の図を参照してください。

.DRBDリソースの積み重ね
image::images/drbd-resource-stacking.svg[]

下位リソースのレプリケーションには同期モード(DRBDプロトコルC)を使いますが、上位リソースは非同期レプリケーション(DRBDプロトコルA)で動作させます。

3ノードレプリケーションは、常時実行することも、オンデマンドで実行することもできます。常時レプリケーションでは、クラスタ側のデータが更新されると、ただちに3番目のノードにもレプリケートされます。オンデマンドレプリケーションでは、クラスタシステムとバックアップサイトの通信はふだんは停止しておき、cronなどによって定期的に夜間などに同期をはかります。

[[s-drbd-proxy]]
=== DRBD Proxyによる遠距離レプリケーション

NOTE: この機能はDRBD 8.2.7以降で利用可能です。

DRBDの<<s-replication-protocols,protocol A>>は非同期モードです。しかし、ソケットの出力バッファが一杯になると(
`drbd.conf` マニュアルページの sndbuf-size
を参照ください。)、アプリケーションからの書き込みはブロックされてしまいます。帯域幅が狭いネットワークを通じて書き込みデータが対向ノードに送られるまで、そのデータを書き込んだアプリケーションは待たなければなりません。

平均的な書き込み帯域幅は、利用可能なネットワークの帯域幅によって制約されます。ソケットの出力バッファに収まるデータ量までのバースト的な書き込みは、問題なく処理されます。

オプション製品のDRBD Proxyのバッファリング機構を使って、この制約を緩和できます。DRBDプライマリノードからの書き込みデータは、DRBD
Proxyのバッファに格納されます。DRBD Proxyのバッファサイズは、アドレス可能空間や搭載メモリの範囲内で自由に設定できます。

データ圧縮を行うように設定することも可能です。圧縮と展開は、応答時間をわずかに増やしてしまいます。しかしネットワークの帯域幅が制約要因になっているのなら、転送時間の短縮効果は、圧縮と展開によるオーバヘッドを打ち消します。

圧縮展開機能は複数CPUによるSMPシステムを想定して実装され、複数CPUコアをうまく活用できます。

多くの場合、ブロックI/Oデータの圧縮率は高く、帯域幅の利用効率は向上します。このため、DRBD
Proxyを使うことによって、DRBDプロトコルBまたはCを使うことも現実的なものとなります。

DRBD Proxyの設定については<<s-using-drbd-proxy>>を参照ください。

NOTE: DRBD ProxyはオープンソースライセンスによらないDRBDプロダクトファミリの製品になります。評価や購入については
sales@3ware.co.jp へご連絡ください。

[[s-truck-based-replication]]
=== トラック輸送によるレプリケーション

トラック輸送(またはディスク輸送)によるレプリケーションは、ストレージメディアを遠隔サイトに物理的に輸送することによるレプリケーションです。以下の制約がある場合に、この方法はとくに有効です。

* レプリケート対象データ領域がかなり大きい(数百GB以上)

* レプリケートするデータの期待される変更レートは巨大ではない

* 利用可能なサイト間のネットワーク帯域幅が限られている

このような状況にある場合、トラック輸送を使わなければ、きわめて長期間(数日から数週間またはそれ以上)の初期同期が必要になってしまいます。トラック輸送でデータを遠隔サイトに輸送する場合、初期同期時間を劇的に短縮できます。詳細は<<s-using-truck-based-replication>>をご覧ください。

[[s-floating-peers]]
=== 動的対向ノード

NOTE: この記述方法はDRBDバージョン8.3.2以上で使用できます。

DRBDのやや特殊な使用方法となる設定値として、動的対向ノードがあります。動的対向ノードを設定すると、DRBDのペア同士は特定の名前のホストに接続せず、いくつかのホスト間を動的に選択して接続するする能力を持ちます。この設定において、DRBDは相手ノードをホスト名ではなくIPアドレスで識別します。

動的対向ノードの設定については<<s-pacemaker-floating-peers>>を参照ください。
