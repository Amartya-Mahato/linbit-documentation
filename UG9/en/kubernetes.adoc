[[ch-kubernetes]]
== LINSTOR Volumes in Kubernetes

indexterm:[Kubernetes]This chapter describes the usage of LINSTOR in Kubernetes
as managed by the operator and with volumes provisioned using the
https://github.com/LINBIT/linstor-csi[LINSTOR CSI plugin].

[[s-kubernetes-overview]]
=== Kubernetes Overview

_Kubernetes_ is a container orchestrator. Kubernetes defines the behavior of
containers and related services via declarative specifications. In this guide,
we'll focus on on using `kubectl` to manipulate `.yaml` files that define the
specifications of Kubernetes objects.

[[s-kubernetes-deploy]]
=== Deploying LINSTOR on Kubernetes

[[s-kubernetes-deploy-linstor-operator]]
==== Deploying with the LINSTOR Operator

LINBIT provides a LINSTOR operator to commercial support customers.
The operator eases deployment of LINSTOR on Kubernetes by installing DRBD,
managing Satellite and Controller pods, and other related functions.

The operator itself is installed using a Helm v3 chart as follows:

* Label the worker nodes with `linstor.linbit.com/linstor-node=true`
by running:
+
----
kubectl label node <NODE_NAME> linstor.linbit.com/linstor-node=true
----

* Create a kubernetes secret containing your my.linbit.com credentials:
+
----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>
----
+
The name of this secret must match the one specified in the Helm values,
by default `drbdiocred`.

* Configure storage for the LINSTOR etcd instance. There are various options
for configuring the etcd instance for LINSTOR:
** Use an existing storage provisioner with a default `StorageClass`.
** <<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>.
** Disable persistence for basic testing. This can be done by adding `--set
etcd.persistence.enabled=false` to the `helm install` command below.

* Configure the LVM VG and LV names by setting the following values in a local
values file or by using `--set` with the `helm install` command below.
+
----
operator.nodeSet.spec.lvmPoolVgName=drbdpool # <- Volume Group name of the thick storage pool
operator.nodeSet.spec.lvmThinPoolVgName=drbdpool # <- Volume Group name of the thin storage pool
operator.nodeSet.spec.lvmThinPoolLvName=thinpool # <- Logical Volume name of the thin pool
----

* Select the appropriate kernel module injector using `--set` with the `helm
install` command in the final step.

** Choose the injector according to the distribution you are using. Replace
`rhel7` with `bionic` or `rhel8` as appropriate in the following value. The
`rhel8` image should also be used for RHCOS (OpenShift).
+
----
operator.nodeSet.spec.kernelModImage=drbd.io/drbd9-rhel7
----

** Disable kernel module injection if you are installing DRBD by other means.
+
----
operator.nodeSet.spec.drbdKernelModuleInjectionMode=None
----

* Finally create a Helm deployment named `linstor-op` that will set up
everything.
+
----
helm repo add linstor https://charts.linstor.io
helm install linstor-op linstor/linstor
----

[[s-kubernetes-etcd-hostpath-persistence]]
===== LINSTOR etcd `hostPath` persistence

You can use the `pv-hostpath` Helm templates to create `hostPath` persistent
volumes. Create as many PVs as needed to satisfy your configured etcd
`replicaCount` (default 3).

Create the `hostPath` persistent volumes, substituting cluster node
names accordingly in the `nodes=` option:

----
helm repo add linstor https://charts.linstor.io
helm install linstor-etcd linstor/pv-hostpath --set "nodes={<NODE0>,<NODE1>,<NODE2>}"
----

Persistence for etcd is enabled by default. The
`etcd.volumePermissions.enabled` key in the Helm values is also set so that the
`hostPath` volumes have appropriate permissions.

[[s-kubernetes-helm-terminate]]
===== Terminating Helm deployment

The LINSTOR deployment can be terminated with:

----
helm delete linstor-op
----

However due to the Helm’s current policy, the newly created Custom Resource
Definitions named `linstorcontrollerset` and `linstornodeset` will *not* be
deleted by the command. This will also cause the instances of those CRD’s named
`linstor-op-ns` and `linstor-op-cs` to remain running.

To terminate those instances after the `helm delete` command, run

----
kubectl patch linstorcontrollerset linstor-op-cs -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl patch linstornodeset linstor-op-ns -p '{"metadata":{"finalizers":[]}}' --type=merge
----

After that, all the instances created by the Helm deployment will be
terminated.

More information regarding Helm’s current position on CRD’s can be found
https://helm.sh/docs/topics/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you[here].

[[s-kubernetes-deploy-piraeus-operator]]
==== Deploying with the Piraeus Operator

The community supported edition of the LINSTOR deployment in Kubernetes is
called Piraeus. The Piraeus project provides
https://github.com/piraeusdatastore/piraeus-operator[an operator] for
deployment.

[[s-kubernetes-linstor-interacting]]
=== Interacting with LINSTOR in Kubernetes

The Controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.
For instance:

----
kubectl exec linstor-op-cs-controller-0 -- linstor storage-pool list
----

This should only be necessary for investigating problems and accessing advanced functionality.
Regular operation such as creating volumes should be achieved via the
<<s-kubernetes-basic-configuration-and-deployment,Kubernetes integration>>.

[[s-kubernetes-linstor-csi-plugin-deployment]]
=== LINSTOR CSI Plugin Deployment

The operator Helm chart deploys the LINSTOR CSI plugin for you so if you used
that, you can skip this section.

If you are integrating LINSTOR using a different method, you will need to install the LINSTOR CSI plugin.
Instructions for deploying the CSI plugin can be found on the
https://github.com/LINBIT/linstor-csi[project's github]. This will result in a
linstor-csi-controller _StatefulSet_ and a linstor-csi-node _DaemonSet_ running in the
kube-system namespace.

----
NAME                       READY   STATUS    RESTARTS   AGE     IP              NODE
linstor-csi-controller-0   5/5     Running   0          3h10m   191.168.1.200   kubelet-a
linstor-csi-node-4fcnn     2/2     Running   0          3h10m   192.168.1.202   kubelet-c
linstor-csi-node-f2dr7     2/2     Running   0          3h10m   192.168.1.203   kubelet-d
linstor-csi-node-j66bc     2/2     Running   0          3h10m   192.168.1.201   kubelet-b
linstor-csi-node-qb7fw     2/2     Running   0          3h10m   192.168.1.200   kubelet-a
linstor-csi-node-zr75z     2/2     Running   0          3h10m   192.168.1.204   kubelet-e
----

[[s-kubernetes-basic-configuration-and-deployment]]
=== Basic Configuration and Deployment

Once all linstor-csi __Pod__s are up and running, we can provision volumes
using the usual Kubernetes workflows.

Configuring the behavior and properties of LINSTOR volumes deployed via Kubernetes
is accomplished via the use of __StorageClass__es. Here below is the simplest
practical _StorageClass_ that can be used to deploy volumes:

.linstor-basic-sc.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  # The name used to identify this StorageClass.
  name: linstor-basic-storage-class
  # The name used to match this StorageClass with a provisioner.
  # linstor.csi.linbit.com is the name that the LINSTOR CSI plugin uses to identify itself
provisioner: linstor.csi.linbit.com
parameters:
  # LINSTOR will provision volumes from the drbdpool storage pool configured
  # On the satellite nodes in the LINSTOR cluster specified in the plugin's deployment
  storagePool: "drbdpool"
----

We can create the _StorageClass_ with the following command:

----
kubectl create -f linstor-basic-sc.yaml
----

Now that our _StorageClass_ is created, we can now create a _PersistentVolumeClaim_
which can be used to provision volumes known both to Kubernetes and LINSTOR:

.my-first-linstor-volume-pvc.yaml
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-first-linstor-volume
  annotations:
    # This line matches the PersistentVolumeClaim with our StorageClass
    # and therefore our provisioner.
    volume.beta.kubernetes.io/storage-class: linstor-basic-storage-class
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

We can create the _PersistentVolumeClaim_ with the following command:

----
kubectl create -f my-first-linstor-volume-pvc.yaml
----

This will create a _PersistentVolumeClaim_ known to Kubernetes, which will have
a _PersistentVolume_ bound to it, additionally LINSTOR will now create this
volume according to the configuration defined in the `linstor-basic-storage-class`
_StorageClass_. The LINSTOR volume's name will be a UUID prefixed with `csi-`
This volume can be observed with the usual `linstor resource list`. Once that
volume is created, we can attach it to a pod. The following _Pod_ spec will spawn
a Fedora container with our volume attached that busy waits so it is not
unscheduled before we can interact with it:

.my-first-linstor-volume-pod.yaml
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fedora
  namespace: default
spec:
  containers:
  - name: fedora
    image: fedora
    command: [/bin/bash]
    args: ["-c", "while true; do sleep 10; done"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "my-first-linstor-volume"
----

We can create the _Pod_ with the following command:

----
kubectl create -f my-first-linstor-volume-pod.yaml
----

Running `kubectl describe pod fedora` can be used to confirm that _Pod_
scheduling and volume attachment succeeded.

To remove a volume, please ensure that no pod is using it and then delete the
_PersistentVolumeClaim_ via `kubectl`. For example, to remove the volume that we
just made, run the following two commands, noting that the _Pod_ must be
unscheduled before the _PersistentVolumeClaim_ will be removed:

----
kubectl delete pod fedora # unschedule the pod.

kubectl get pod -w # wait for pod to be unscheduled

kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.
----

[[s-kubernetes-snapshots]]
=== Snapshots

Creating <<s-linstor-snapshots, snapshots>> and creating new volumes from
snapshots is done via the use of __VolumeSnapshot__s, __VolumeSnapshotClass__es,
and __PVC__s. First, you'll need to create a _VolumeSnapshotClass_:

.my-first-linstor-snapshot-class.yaml
[source,yaml]
----
kind: VolumeSnapshotClass
apiVersion: snapshot.storage.k8s.io/v1alpha1
metadata:
  name: my-first-linstor-snapshot-class
  namespace: kube-system
snapshotter: io.drbd.linstor-csi
----

Create the _VolumeSnapshotClass_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot-class.yaml
----

Now we will create a volume snapshot for the volume that we created above. This
is done with a _VolumeSnapshot_:

.my-first-linstor-snapshot.yaml
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  name: my-first-linstor-snapshot
spec:
  snapshotClassName: my-first-linstor-snapshot-class
  source:
    name: my-first-linstor-volume
    kind: PersistentVolumeClaim
----

Create the _VolumeSnapshot_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot.yaml
----

Finally, we'll create a new volume from the snapshot with a _PVC_.

.my-first-linstor-volume-from-snapshot.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-first-linstor-volume-from-snapshot
spec:
  storageClassName: linstor-basic-storage-class
  dataSource:
    name: my-first-linstor-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

Create the _PVC_ with `kubectl`:

----
kubectl create -f my-first-linstor-volume-from-snapshot.yaml
----


[[s-kubernetes-volume-accessibility]]
=== Volume Accessibility
// This only covers DRBD volumes, section might change if linked docs are updated.
LINSTOR volumes are typically accessible both locally and
<<s-drbd_clients,over the network>>.

By default, the CSI plugin will attach volumes directly if the _Pod_ happens
to be scheduled on a _kubelet_ where its underlying storage is present. However,
_Pod_ scheduling does not currently take volume locality into account. The
<<s-kubernetes-replicasonsame,replicasOnSame>> parameter can be used to restrict
where the underlying storage may be provisioned, if locally attached volumes
are desired.

See <<s-kubernetes-localstoragepolicy,localStoragePolicy>> to see how this
default behavior can be modified.

[[s-kubernetes-advanced-configuration]]
=== Advanced Configuration

In general, all configuration for LINSTOR volumes in Kubernetes should be done
via the _StorageClass_ parameters, as seen with the _storagePool_ in the basic
example above. We'll give all the available options an in-depth treatment here.

[[s-kubernetes-nodelist]]
==== nodeList

`nodeList` is a list of nodes for volumes to be assigned to. This will assign
the volume to each node and it will be replicated among all of them. This
can also be used to select a single node by hostname, but it's more flexible to use
<<s-kubernetes-replicasonsame,replicasOnSame>> to select a single node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-autoplace,autoPlace>>.

TIP: This option determines on which LINSTOR nodes the underlying storage for volumes
will be provisioned and is orthogonal from which _kubelets_ these volumes will be
accessible.

Example: `nodeList: "node-a node-b node-c"`

Example: `nodeList: "node-a"`

[[s-kubernetes-autoplace]]
==== autoPlace

`autoPlace` is an integer that determines the amount of replicas a volume of
this _StorageClass_ will have.  For instance, `autoPlace: 3` will produce
volumes with three-way replication. If neither `autoPlace` nor `nodeList` are
set, volumes will be <<s-autoplace-linstor,automatically placed>> on one node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-nodelist,nodeList>>.

TIP: This option (and all options which affect autoplacement behavior) modifies the
number of LINSTOR nodes on which the underlying storage for volumes will be
provisioned and is orthogonal to which _kubelets_ those volumes will be accessible
from.

Example: `autoPlace: 2`

Default: `autoPlace: 1`


[[s-kubernetes-replicasonsame]]
==== replicasOnSame

// These should link to the linstor documentation about node properties, but those
// do not exist at the time of this commit.
`replicasOnSame` is a list of key=value pairs used as required autoplacement selection
labels when <<s-kubernetes-autoplace,autoplace>> is used to determine where to
provision storage. These labels correspond to LINSTOR node aux props. Please note both
the key and value names are user-defined and arbitrary. Let's explore this behavior
with examples assuming a LINSTOR cluster such that `node-a` is configured with the
following aux props `zone=z1` and `role=backups`, while `node-b` is configured with
only `zone=z1`.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1 role=backups"`,
then all volumes created from that _StorageClass_ will be provisioned on `node-a`,
since that is the only node with all of the correct key=value pairs in the LINSTOR
cluster. This is the most flexible way to select a single node for provisioning.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on either `node-a` or `node-b` as they both have
the `zone=z1` aux prop.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1 role=backups"`,
then provisioning will fail, as there are not two or more nodes that have
the appropriate aux props.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on both `node-a` and `node-b` as they both have
the `zone=z1` aux prop.

Example: `replicasOnSame: "zone=z1 role=backups"`

[[s-kubernetes-replicasondifferent]]
==== replicasOnDifferent

`replicasOnDifferent` is a list of key=value pairs to avoid as autoplacement
selection. It is the inverse of <<s-kubernetes-replicasonsame,replicasOnSame>>.

Example: `replicasOnDifferent: "no-csi-volumes=true"`

[[s-kubernetes-localstoragepolicy]]
==== localStoragePolicy

`localStoragePolicy` determines, via volume topology, which LINSTOR
__Satellite__s volumes should be assigned and from where Kubernetes will
access volumes. The behavior of each option is explained below in detail.

IMPORTANT: If you specify a <<s-kubernetes-nodelist,nodeList>>, volumes will
be created on those nodes, irrespective of the `localStoragePolicy`; however,
the accessibility reporting will still be as described.

IMPORTANT: You must set `volumeBindingMode: WaitForFirstConsumer` in the
_StorageClass_ and the LINSTOR __Satellite__s running on the __kubelet__s must
be able to support the diskfull placement of volumes as they are configured in
that _StorageClass_ for <<s-kubernetes-localstoragepolicy-required,required>>
or <<s-kubernetes-localstoragepolicy-preferred,preferred>> to work properly.

TIP: Use `topologyKey: "linbit.com/hostname"` rather than `topologyKey:
"kubernetes.io/hostname"` if you are setting `affinity` in your _Pod_ or
_StatefulSet_ specs.

Example: `localStoragePolicy: required`

[[s-kubernetes-localstoragepolicy-ignore]]
===== ignore (default)

When `localStoragePolicy` is set to `ignore`, regular autoplacement
occurs based on <<s-kubernetes-autoplace,autoplace>>,
<<s-kubernetes-replicasonsame,replicasOnSame>>, and
<<s-kubernetes-replicasonsame,replicasOnDifferent>>. Volume location will not
affect _Pod_ scheduling in Kubernetes and the volumes will be accessed over
the network if they're not local to the _kubelet_ where the _Pod_ was scheduled.

[[s-kubernetes-localstoragepolicy-required]]
===== required

When `localStoragePolicy` is set to `required`, Kubernetes will report a list
of places that it wants to schedule a _Pod_ in order of preference. The plugin
will attempt to provision the volume(s) according to that preference. The
number of volumes to be provisioned in total is based off of
<<s-kubernetes-autoplace,autoplace>>.

If all preferences have been attempted, but no volumes where successfully
assigned volume creation will fail.

In case of multiple replicas when all preferences have been attempted, and at
least one has succeeded, but there are still replicas remaining to be
provisioned, <<s-kubernetes-autoplace,autoplace>> behavior will apply for the
remaining volumes.

With this option set, Kubernetes will consider volumes that are not locally
present on a _kubelet_ to be unaccessible from that _kubelet_.

[[s-kubernetes-localstoragepolicy-preferred]]
===== preferred

When `localStoragePolicy` is set to `preferred`, volume placement behavior
will be the same as when it's set to
<<s-kubernetes-localstoragepolicy-required,required>> with the exception that
volume creation will not fail if no preference was able to be satisfied.
Volume accessibility will be the same as when set to
<<s-kubernetes-localstoragepolicy-ignore,ignore>>.

[[s-kubernetes-storagepool]]
==== storagePool

`storagePool` is the name of the LINSTOR <<s-storage_pools,storage pool>> that
will be used to provide storage to the newly-created volumes.

CAUTION: Only nodes configured with this same _storage pool_ with be considered
for <<s-kubernetes-autoplace,autoplacement>>. Likewise, for _StorageClasses_ using
<<s-kubernetes-nodelist,nodeList>> all nodes specified in that list must have this
_storage pool_ configured on them.

Example: `storagePool: my-storage-pool`

[[s-kubernetes-disklessstoragepool]]
==== disklessStoragePool

// This should link to the linstor section talking about diskless storage pools
// when that gets written.
`disklessStoragePool` is an optional parameter that only effects LINSTOR volumes
assigned disklessly to _kubelets_ i.e., as clients. If you have a custom
_diskless storage pool_ defined in LINSTOR, you'll specify that here.

Example: `disklessStoragePool: my-custom-diskless-pool`

[[s-kubernetes-encryption]]
==== encryption

`encryption` is an optional parameter that determines whether to encrypt
volumes. LINSTOR must be <<s-linstor-encrypted-Volumes,configured for encryption>>
for this to work properly.

Example: `encryption: "true"`

[[s-kubernetes-filesystem]]
==== filesystem

`filesystem` is an option parameter to specify the filesystem for non raw block
volumes. Currently supported options are `xfs` and `ext4`.

Example: `filesystem: "xfs"`

Default: `filesystem: "ext4"`

[[s-kubernetes-fsops]]
==== fsOpts
`fsOpts` is an optional parameter that passes options to the volume's
filesystem at creation time.

IMPORTANT: Please note these values are specific to your chosen
<<s-kubernetes-filesystem, filesystem>>.

Example: `fsOpts: "-b 2048"`

[[s-kubernetes-mountops]]
==== mountOpts
`mountOpts` is an optional parameter that passes options to the volume's
filesystem at mount time.

Example: `mountOpts: "sync,noatime"`
