== Common administrative tasks - LINSTOR

LINSTOR is a configuration management system for storage on Linux systems.
It manages LVM logical volumes and/or ZFS ZVOLs on a cluster of nodes. It
leverages DRBD for replication between different nodes and to provide
block storage devices to users and applications. It manages snapshots,
encryption and caching of HDD backed data in SSDs via bcache.

This chapter outlines typical administrative tasks encountered during
day-to-day operations. It does not cover troubleshooting tasks, these
are covered in detail in <<ch-troubleshooting>>.

=== Concepts and Terms

A linstor setup has exactly one active controller and multiple satellites.
The _linstor controller_ contains the database that holds all configuration
information for the whole cluster. It makes all decisions that need to have a
view of the whole cluster. The controller is typically deployed as a HA service
using Pacemaker and DRBD as it is a crucial part of the system.

The _linstor satellite_ runs on each node where linstor consumes local
storage or provides storage to services. It is stateless; it receives
all the information it needs from the controller. It runs programs
like `lvcreate` and `drbdadm`. It is like a node agent.

The _linstor client_ is a command line utility that you use to issue
commands to the system and to investigate the status of the system

=== Broader Context

While linstor might be used to make the management of DRBD more convenient
it is often integrated with software stacks higher up. Such integrations
exists for: Kubernetes. (In progress: OpenStack, OpenNebula, Proxmox).

The southbound drivers used by linstor are LVM, thinLVM, ZFS and thinZFS.
(Support for Swordfish is in progress.)

=== Packages

Linstor is packaged in just two packages in both the .rpm and the .dpkg variants:

. _linstor-client_ contains the command line client program. It only depends
  on python which is usually already installed.
. _linstor-server_ contains the _controller_ and the _satellite_. It provides
  systemd unit files for both services. It depends on a Java runtime environment
  (JRE) version 1.8 (headless) or higher. This might pull in about 100MB of dependencies.

[[s-linstor-init-cluster]]
=== Initializing your cluster
We assume that the following steps are accomplished on *all* cluster nodes:

. The DRBD9 kernel module is installed and loaded
. `drbd-utils` are installed
. `LVM` tools are installed
. `linstor server` and its dependencies are installed

Start the linstor-controller service:
----------------------------
# systemctl start linstor-controller
----------------------------

=== Using the linstor client
Whenever you run the linstor command line client it needs to know where your
linstor-server runs. If you do not specify it, it will try to reach a locally
running linstor-controller listening on IP `127.0.0.1` port `3376`.

----------------------------
# linstor list-nodes
----------------------------
should give show you an empty list and not an error message.

You can use the `linstor` command on any other machine, but then you need
to tell the client how to find the linstor server. As shown, this can be
specified as a command line option, an environment variable or in a global
file:

----------------------------
# linstor --controllers=alice list-nodes
# LS_CONTROLLERS=alice linstor list-nodes
# FIXME add info about /etc/file...
----------------------------
FIXME describe how to specify multiple controllers

=== Adding nodes to your cluster
The next step is to add nodes to your linstor cluster. You need to provide

. A node name which *must* match the output of `uname -n`
. The IP address of the node.

----------------------------
# linstor create-node bravo 10.43.70.3
----------------------------

When you use `linstor list-nodes` you will see that the new node
is marked as offline. Now start the linstor satellite on that node
with:
----------------------------
# systemctl start linstor-satellite
----------------------------
About 10 seconds later you will see the status in `linstor list-nodes`
becoming online. Of course the satellite process may be started before
the controller knows about the existence of the satellite node.

--

.Note
In case the node which hosts your controller should also contribute
storage to the linstor cluster, you have to add it as node and start
the linstor-satellite as well.
--

=== Storage pools

Storage pools group storage in the context of linstor. They
might span multiple nodes. For example one valid approach is to
group all SSDs into one storage pool and all HDDs into a second storage
pool.

On each host contributing to a storage pool you need to create
either a LVM VG or an ZFS zPool. The VGs contributing to one linstor
storage pool my have different VG names on the hosts, but do yourself
a favor and use the same VG name on all nodes.
(FIXME add wording about zPools)

----------------------------
# vgcreate vg_ssd /dev/nvme0n1 /dev/nvme1n1 [...]
# linstor create-storage-pool pool_ssd alpha lvm vg_ssd
# linstor create-storage-pool pool_ssd bravo lvm vg_ssd
----------------------------

--

.Note
The listed commands create a storage pool definition implicitly.
You can see that by using `linstor list-storage-pool-definitions`.
Creating storage pool definitions explicitly is possible but
not necessary.
--
==== A storage pool per backend device

In clusters where you have only one kind of storage and the capability
to hot-repair storage devices you may choose a model where you create
one storage pool per physical backing device. The advantage of this
model is to confine failure domains to a single storage device.


[[s-linstor-set-config]]
=== Cluster configuration
FIXME

==== Available storage plugins

indexterm:[linstor, storage plugins]

LINSTOR has four supported storage plugins as of this writing:

  * Thick LVM

  * Thin LVM with a single thin pool

  * Thin LVM with thin pools for each volume

  * Thick ZFS

  * Thin ZFS

FIXME

[[s-linstor-new-volume]]

=== Creating and deploying resources/volumes
In the following scenario we assume that the goal is to create a resource
'backups' with a size of '500 GB' that is replicated among 3 cluster nodes.

First, we create a new resource definition:

----------------------------
# linstor create-resource-definition backups
----------------------------

Second, we create a new volume definition within that resource definition:

----------------------------
# linstor create-volume-definition backups 500G
----------------------------

So far we have only created objects in linstor's database, not a single LV was
created on the storage nodes. Now you have to choice of delegating the
task of placement to linstor or doing it yourself

==== Autoplace

The value after autoplace tells linstor how many replicas you want to have.
The storage-pool option should be obvious.
----------------------------
# linstor create-resource backups --auto-place 3 --storage-pool pool_hdd
----------------------------
Maybe not so obvious is that you may omit the `--storage-pool` option, then
linstor may select different storage pools on different nodes!

==== Manual placement

With the `create-resource` command you may assign a resource definition
to named nodes explicitly.

----------------------------
# linstor create-resource backups alpha --storage-pool pool_hdd
# linstor create-resource backups bravo --storage-pool pool_hdd
# linstor create-resource backups charlie --storage-pool pool_hdd
----------------------------

==== DRBD clients
By using the `--diskless` option instead of `--storage-pool` you can
have a permanently diskless DRBD device on a node.

----------------------------
# linstor create-resource backups delta --diskless
----------------------------

==== Volumes of one resource to different Storage-Pools
IMPLEMENT
Linstor can do that, but it is not yet implemented in the client.

[[s-linstor-snapshots]]
=== Managing snapshots
IMPLEMENT

[[s-linstor-status]]
=== Checking the state of your cluster
`Linstor` provides various commands to check the state of your cluster.
These commands start with a 'list-' prefix and provide various filtering and
sorting options. The '--groupby' option can be used to group and sort the
output in multiple dimensions.

----------------------------
# linstor list-nodes
# linstor list-storage-pools --groupby Size
----------------------------

[[s-linstor-setupopts]]
=== Setting options for resources
IMPLEMENT

[[s-linstor-rebalance]]
=== Rebalancing data with LINSTOR
FIXME

[[s-linstor-getting-help]]
=== Getting help
WRITE MAN PAGE

A quick way to list available commands on the command line is to type
`linstor`.

Further information on subcommands (e.g., list-nodes) can be retrieved in
two ways:
----------------------------
# linstor list-nodes -h
# linstor help list-nodes
----------------------------

Using the 'help' subcommand is especially helpful when linstor is executed
in interactive mode (`linstor interactive`).

One of the most helpful features of linstor is its rich tab-completion,
which can be used to complete basically every object linstor knows about
(e.g., node names, IP addresses, resource names, ...).
In the following we show some possible completions, and their results:

----------------------------
# linstor create-node alpha 1<tab> # completes the IP address if hostname can be resolved
# linstor create-resource b<tab> c<tab> # linstor assign-resource backups charlie
----------------------------

If tab-completion does not work out of the box, please try to source the
appropriate file:

----------------------------
# source /etc/bash_completion.d/linstor # or
# source /usr/share/bash_completion/completions/linstor
----------------------------

