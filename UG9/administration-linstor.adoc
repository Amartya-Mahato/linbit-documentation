== Common administrative tasks - LINSTOR

LINSTOR is a configuration management system for storage on Linux systems.
It manages LVM logical volumes and/or ZFS ZVOLs on a cluster of nodes. It
leverages DRBD for replication between different nodes and to provide
block storage devices to users and applications. It manages snapshots,
encryption and caching of HDD backed data in SSDs via bcache.

This chapter outlines typical administrative tasks encountered during
day-to-day operations. It does not cover troubleshooting tasks, these
are covered in detail in <<ch-troubleshooting>>.

=== Concepts and Terms

A linstor setup has exactly one active controller and multiple satellites.
The _linstor controller_ contains the database that holds all configuration
information for the whole cluster. It makes all decisions that need to have a
view of the whole cluster. The controller is typically deployed as a HA service
using Pacemaker and DRBD as it is a crucial part of the system.

The _linstor satellite_ runs on each node where linstor consumes local
storage or provides storage to services. It is stateless; it receives
all the information it needs from the controller. It runs programs
like `lvcreate` and `drbdadm`. It is like a node agent.

The _linstor client_ is a command line utility that you use to issue
commands to the system and to investigate the status of the system

=== Broader Context

While linstor might be used to make the management of DRBD more convenient
it is often integrated with software stacks higher up. Such integrations
exists for: Kubernetes. (In progress: OpenStack, OpenNebula, Proxmox).

The southbound drivers used by linstor are LVM, thinLVM and ZFS.
(Support for Swordfish is in progress.)

=== Packages

Linstor is packaged in just two packages in both the .rpm and the .dpkg variants:

. _linstor-client_ contains the command line client program. It only depends
  on python which is usually already installed.
. _linstor-server_ contains the _controller_ and the _satellite_. It provides
  systemd unit files for both services. It depends on a Java runtime environment
  (JRE) version 1.8 (headless) or higher. This might pull in about 100MB of dependencies.

[[s-linstor-init-cluster]]
=== Initializing your cluster
We assume that the following steps are accomplished on *all* cluster nodes:

. The DRBD9 kernel module is installed and loaded
. `drbd-utils` are installed
. `LVM` tools are installed
. `linstor server` and its dependencies are installed

Start the linstor-controller service:
----------------------------
# systemctl start linstor-controller
----------------------------

=== Using the linstor client
Whenever you run the linstor command line client it needs to know where your
linstor-server runs. If you do not specify it, it will try to reach a locally
running linstor-controller listening on IP `127.0.0.1` port `3376`.

----------------------------
# linstor list-nodes
----------------------------
should give show you an empty list and not an error message.

You can use the `linstor` command on any other machine, but then you need
to tell the client how to find the linstor server. As shown, this can be
specified as a command line option, an environment variable or in a global
file:

----------------------------
# linstor --controllers=alice list-nodes
# LS_CONTROLLERS=alice linstor list-nodes
# FIXME add info about /etc/file...
----------------------------
FIXME describe how to specify multiple controllers

=== Adding nodes to your cluster
The next step is to add nodes to your linstor cluster. You need to provide

. A node name which *must* match the output of `uname -n`
. The IP address of the node.

----------------------------
# linstor create-node bravo 10.43.70.3
----------------------------

When you use `linstor list-nodes` you will see that the new node
is marked as offline. Now start the linstor satellite on that node
with:
----------------------------
# systemctl start linstor-satellite
----------------------------
About 10 seconds later you will see the status in `linstor list-nodes`
becoming online. Of course the satellite process may be started before
the controller knows about the existence of the satellite node.

NOTE: In case the node which hosts your controller should also contribute
storage to the linstor cluster, you have to add it as node and start
the linstor-satellite as well.

=== Storage pools

_Storage pools_ identify storage in the context of linstor.
To group storage pools from multiple nodes, simply use the same name
on each node.
For example one valid approach is to give all SSDs one name and
all HDDs another.

On each host contributing storage you need to create
either an LVM VG or a ZFS zPool. The VGs and zPools identified with one
linstor storage pool name may have different VG or zPool names on the
hosts, but do yourself a favor and use the same VG or zPool name on all
nodes.

----------------------------
# vgcreate vg_ssd /dev/nvme0n1 /dev/nvme1n1 [...]
----------------------------

These then need to be registered with linstor:

----------------------------
# linstor create-storage-pool pool_ssd alpha lvm vg_ssd
# linstor create-storage-pool pool_ssd bravo lvm vg_ssd
----------------------------

NOTE: The storage pool name and common metadata is referred to as a
_storage pool definition_.
The listed commands create a storage pool definition implicitly.
You can see that by using `linstor list-storage-pool-definitions`.
Creating storage pool definitions explicitly is possible but
not necessary.

==== A storage pool per backend device

In clusters where you have only one kind of storage and the capability
to hot-repair storage devices you may choose a model where you create
one storage pool per physical backing device. The advantage of this
model is to confine failure domains to a single storage device.


[[s-linstor-set-config]]
=== Cluster configuration
FIXME

==== Available storage plugins

indexterm:[linstor, storage plugins]

LINSTOR has three supported storage plugins as of this writing:

  * Thick LVM

  * Thin LVM with a single thin pool

  * ZFS

FIXME

[[s-linstor-new-volume]]

=== Creating and deploying resources/volumes
In the following scenario we assume that the goal is to create a resource
'backups' with a size of '500 GB' that is replicated among 3 cluster nodes.

First, we create a new resource definition:

----------------------------
# linstor create-resource-definition backups
----------------------------

Second, we create a new volume definition within that resource definition:

----------------------------
# linstor create-volume-definition backups 500G
----------------------------

So far we have only created objects in linstor's database, not a single LV was
created on the storage nodes. Now you have to choice of delegating the
task of placement to linstor or doing it yourself

==== Manual placement

With the `create-resource` command you may assign a resource definition
to named nodes explicitly.

----------------------------
# linstor create-resource backups alpha --storage-pool pool_hdd
# linstor create-resource backups bravo --storage-pool pool_hdd
# linstor create-resource backups charlie --storage-pool pool_hdd
----------------------------

==== Autoplace

The value after autoplace tells linstor how many replicas you want to have.
The storage-pool option should be obvious.
----------------------------
# linstor create-resource backups --auto-place 3 --storage-pool pool_hdd
----------------------------
Maybe not so obvious is that you may omit the `--storage-pool` option, then
linstor may select a storage pool on its own. The selection follows these rules:

  * Ignore all nodes and storage pools the current user has no access to
  * Ignore all diskless storage pools
  * Ignore all storage pools not having enough free space

From the remaining storage pools linstor currently chooses the one with the
most available free space.

==== DRBD clients
By using the `--diskless` option instead of `--storage-pool` you can
have a permanently diskless DRBD device on a node.

----------------------------
# linstor create-resource backups delta --diskless
----------------------------

==== Volumes of one resource to different Storage-Pools
This can be achieved by setting the `StorPoolName` property to the volume
definitions before the resource is deployed to the nodes:

----------------------------
# linstor create-resource-definition backups
# linstor create-volume-definition backups 500G
# linstor create-volume-definition backups 100G
# linstor set-volume-definition-property backups 0 StorPoolName pool_hdd
# linstor set-volume-definition-property backups 1 StorPoolName pool_ssd
# linstor create-resource backups alpha
# linstor create-resource backups bravo
# linstor create resource backups charlie
----------------------------

NOTE: Since the `create-volume-definition` command is used without the `--vlmnr` option
linstor assigned the volume nubers starting at 0. In the following two
lines the 0 and 1 refers to these automatically asigned volume numbers.

Here the 'create-resource' commands do not need a `--storage-pool` option.
In this case linstor uses a 'fallback' storage pool. Finding that storage pool
linstor queries the properties of the following objects in the following order:

  * Volume definition
  * Resource
  * Resource definition
  * Node

If none of those objects contain a `StorPoolName` property, the controller
falls back to a hardcoded 'DfltStorPool' string as a storage pool.

This also means that if you forgot to define a storage pool prior deploying a
resource, you will get an error message that linstor could not find the
storage pool named 'DfltStorPool'.

=== Managing Network Interface Cards

Linstor can deal with multiple network interface cards (NICs) in a machine,
they are called `netif` in linstor speak.

NOTE: When a satellite node is created a first `netif` gets created implicitly
with the name `default`. Using the `--interface-name` option of the `create-node`
command you can give it a different name.

Additional NICs are created like that:
----------------------------
# linstor create-netif alpha 100G_nic 192.168.43.221
# linstor create-netif alpha 10G_nic 192.168.43.231
----------------------------

NICs are identified by the IP address only, the name is arbitrary and is
*not* related to the interface name used by Linux. The NICs can be assigned
to storage pools so that whenever a resource is created in such a storage
pool, the DRBD traffic will be routed through the specified NIC.

----------------------------
# linstor set-storage-pool-property pool_hdd alpha PrefNic 10G_nic
# linstor set-storage-pool-property pool_ssd alpha PrefNic 100G_nic
----------------------------

FIXME describe how to route the controler <-> client communication through
a specific `netif`.

[[s-linstor-snapshots]]
=== Managing snapshots
IMPLEMENT

[[s-linstor-status]]
=== Checking the state of your cluster
`Linstor` provides various commands to check the state of your cluster.
These commands start with a 'list-' prefix and provide various filtering and
sorting options. The '--groupby' option can be used to group and sort the
output in multiple dimensions.

----------------------------
# linstor list-nodes
# linstor list-storage-pools --groupby Size
----------------------------

[[s-linstor-setupopts]]
=== Setting options for resources
IMPLEMENT

[[s-linstor-rebalance]]
=== Rebalancing data with LINSTOR
FIXME

[[s-linstor-getting-help]]
=== Getting help
WRITE MAN PAGE

A quick way to list available commands on the command line is to type
`linstor`.

Further information on subcommands (e.g., list-nodes) can be retrieved in
two ways:
----------------------------
# linstor list-nodes -h
# linstor help list-nodes
----------------------------

Using the 'help' subcommand is especially helpful when linstor is executed
in interactive mode (`linstor interactive`).

One of the most helpful features of linstor is its rich tab-completion,
which can be used to complete basically every object linstor knows about
(e.g., node names, IP addresses, resource names, ...).
In the following we show some possible completions, and their results:

----------------------------
# linstor create-node alpha 1<tab> # completes the IP address if hostname can be resolved
# linstor create-resource b<tab> c<tab> # linstor assign-resource backups charlie
----------------------------

If tab-completion does not work out of the box, please try to source the
appropriate file:

----------------------------
# source /etc/bash_completion.d/linstor # or
# source /usr/share/bash_completion/completions/linstor
----------------------------

