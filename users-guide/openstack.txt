[[ch-openstack]]
== DRBD volumes in Openstack

indexterm:[Openstack]indexterm:[Cinder]indexterm:[Nova]In this chapter you will
learn how to use DRBD in Openstack for persistent, replicated, high-performance
block storage.

[[s-openstack-overview]]
=== Openstack Overview

Openstack itself consists of a big range of individual services; the two that are mostly concerned with DRBD are Cinder and Nova. *Cinder* is the block storage service, while *Nova* is the compute node service that's responsible to make the volumes available for the VMs.

DRBD storage volumes can be accessed in two ways: using the iSCSI protocol (for maximum compatibility), and using DRBD client functionality (being submitted to Openstack).  For a discussion of these two modes and their differences please see <<s-openstack-transport-protocol>>.

[[s-openstack-install]]
=== DRBD for Openstack Installation

The __drbdmanage__ driver is upstream in Openstack since the Liberty release. It is used (mostly) in the ++c-vol++ service, so you'll need drbdmanage and DRBD 9 installed on the node(s) running that.


Depending on the specific Openstack variant being used there are a few differences for paths, user names, etc.:

.Distribution dependent settings
[format="csv",separator=";",options="header"]
|============================================
what   ;   rdostack   ;   devstack
Cinder/DRBDmanage driver file location; ++/usr/lib/python2.6/site-packages/cinder/volume/drivers/drbdmanagedrv.py++ ; ++/opt/stack/cinder/cinder/volume/drivers/drbdmanagedrv.py++
Cinder configuration file ; ++/usr/share/cinder/cinder-dist.conf++ ; ++/etc/cinder/cinder.conf++
Admin access data, for sourcing into shell ; ++/etc/nagios/keystonerc_admin++ ; ++~stack/devstack/accrc/admin/admin++
User used for running ++c-vol++ service ; ++cinder++ ; ++stack ++
|============================================


The generalized installations steps are these:

  * In the ++cinder.conf++ you'll need something like that; the ++volume_driver++ consists of the class name (last part), and the file path:
+
--

-------
[DEFAULT]
enabled_backends=drbdmanage

[drbd-1]
volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageIscsiDriver
volume_backend_name=DRBD-Managed
drbdmanage_redundancy=1
-------

Please see also <<s-openstack-transport-protocol>> for choosing between iSCSI 
and DRBD transport modes, and <<s-openstack-addtl-conf,other configuration 
settings>>.
--

  * Register the backend: (you might need to fetch the authentication environment variables via ++source __<admin access data>__++)
+
--

-------
# cinder type-create drbd-1
-------
--

  * Allow the user to access the "__org.drbd.drbdmanaged__" service on DBus. For that you need to extend the file ++/etc/dbus-1/system.d/org.drbd.drbdmanaged.conf++ by an additional stanza like this (replace __USER__ by the username as per the table above):
+
--

-------
<policy user="USER">
  <allow own="org.drbd.drbdmanaged"/>
  <allow send_interface="org.drbd.drbdmanaged"/>
  <allow send_destination="org.drbd.drbdmanaged"/>
</policy>
-------
--


That's it; after a restart of the ++c-vol++ service you should be able to create your DRBD volumes.


[[s-openstack-addtl-conf]]
==== Additional Configuration

The ++drbdmanage++ backend configuration in ++cinder.conf++ can contain a few 
additional settings that modify the exact behaviour.

  * indexterm:[redundancy in Cinder]++drbdmanage_redundancy = 2++ eg. would 
    declare that each volume needs to have 2 storage locations, ie. be 
    replicated once. This means that two times the storage will be used, and 
    that the reported free space <<s-openstack-free-space,looks limited>>.
	+
	You can request more 
    than two copies of the data; the limit is given by DRBD 9 and the number of 
    storage hosts you have defined.

  * ++drbdmanage_devs_on_controller = True++: By default each volume will get 
    a DRBD client mapped on the Cinder controller node; apart from being used 
    for iSCSI exports, this might prove helpful for debugging, too.

  * indexterm:[iSCSI, in Cinder]indexterm:[Openstack Cinder iSCSI transport]In 
    case you need to choose a different iSCSI backend, you can provide an 
    additional configuration to set it, like
+
--

-------
iscsi_helper=lioadm
-------
--

These configuration settings can be different from one backend to another.



[[s-openstack-transport-protocol]]
=== Choosing the Transport Protocol

There are two main ways to run DRBD with Cinder:

  * accessing storage via <<s-openstack-iscsi,iSCSI exports>>, and
 
  * using <<s-openstack-drbd,the DRBD protocol>> on the wire.

These are not exclusive; you can define multiple backends, have some of them 
use iSCSI, and others the DRBD protocol.


[[s-openstack-iscsi]]
==== iSCSI Transport

The default way to export Cinder volumes is via iSCSI. This brings the 
advantage of maximum compatibility - iSCSI can be used with every hypervisor, 
be it VMWare, Xen, HyperV, or KVM.

The drawback is that all data has to be sent to a Cinder node, to be processed 
by an (userspace) iSCSI daemon; that means that the data needs to pass the 
kernel/userspace border, and these transitions will cost some performance.

TODO: performance comparision


[[s-openstack-drbd]]
==== DRBD Transport

The alternative is to get the data to the VMs by using DRBD as the transport 
protocol. This means that DRBD 9footnote:[The kernel module and userspace, and currently
the DRBDmanage daemon too; but please see the note at <<s-openstack-drbd-external-NOTE>>.]
needs to be installed on the Nova nodes too, and so restricts them 
to Linux with KVM at the moment.

One advantage of that solution is that the storage access requests of the VMs can be sent via 
the DRBD kernel module to the storage nodes, which can then directly access the 
allocated LVs; this means no Kernel/Userspace transitions on the data path, and 
consequently better performance. Combined with RDMA capable hardware you should 
get about the same performance as with VMs accessing a FC backend directly.

Another advantage is that you will be implicitly benefitting from the HA background 
of DRBD: using multiple storage nodes, possibly available over different network connections, 
means redundancy and avoiding a single point of failure.


[[s-openstack-drbd-external-NOTE]]
.NOTE
=================
Currently, you'll need to have the hypervisor nodes be part of the DRBDmanage cluster.

When DRBDmanage becomes able to process "__external nodes__", the requirements
on the hypervisor nodes will shrink to DRBD 9 kernel module and -userspace only.
=================


[[s-openstack-conf-transport-protocol]]
==== Configuring the Transport Protocol

In the storage stanzas in ++cinder.conf++ you can define the volume driver to use; 
you can use different drivers for different backend configurations, ie. you can 
define a 2-way-redundancy iSCSI backend, a 2-way-redundancy DRBD backend, and 
a 3-way DRBD backend at the same time. Horizonfootnote:[The Openstack GUI] 
should offer these storage backends at volume creation time.

The available configuration items for the two drivers are

	* for iSCSI:
+
--

    volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageIscsiDriver
--

and

    * for DRBD:
+
--

    volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageDrbdDriver

--


The old class name "DrbdManageDriver" is being kept for the time because of 
compatibility reasons; it's just an alias to the iSCSI driver.


[[s-openstack-notes]]
=== Some further notes


[[s-openstack-free-space]]
==== Free space reporting

The free space that the cinder driver reports is fetched from DRBDmanage, using 
the defined <<s-openstack-addtl-conf,+drbdmanage_redundancy+>> setting.

This will return the size for the single largest volume that can be created 
with this replication count; so, with 10 storage nodes each having 1TiB free 
space, the value returned for a redundancy count of three will be 1TiB, and 
allocating such a volume will not change the free space value, as there are 
three more nodes with that much free space available. For storage nodes with 
20GiB, 15GiB, 10GiB, and 5GiB space available, the free space for ++drbdmanage_redundancy++ 
being 3 will be 10GiB, and 15GiB for 2.

This issue is further muddled by thin LVM pools (one or multiple, 
depending on storage backend in DRBDmanage), and snapshots taken from Cinder 
volumes.

For further information, please see the Openstack Specs about Thin Provisioning 
- there's the 
<<https://blueprints.launchpad.net/cinder/+spec/over-subscription-in-thin-provisioning, blueprint>>
and the 
<<https://github.com/openstack/cinder-specs/blob/master/specs/kilo/over-subscription-in-thin-provisioning.rst,specification text>>.


// TODO: some performance numbers
