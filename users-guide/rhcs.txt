[[ch-rhcs]]
== Integrating DRBD with Red Hat Cluster Suite

indexterm:[Red Hat Cluster Suite]This chapter describes using DRBD as
replicated storage for Red Hat Cluster Suite high availability
clusters.

NOTE: This guide deals primarily with Red Hat Cluster Suite as found in
Red Hat Enterprise Linux (RHEL 5). If you are deploying DRBD on
earlier versions such as RHEL 4, configuration details and semantics
may vary.

[[s-rhcs-primer]]
=== Red Hat Cluster Suite primer

[[s-rhcs-openais]]
==== OpenAIS and CMAN

indexterm:[OpenAIS]The http://www.saforum.org[Service Availability
Forum]is an industry consortium with the purpose of developing high
availability interface definitions and software specifications. The
Application Interface Specification ( ) is one of these
specifications, and OpenAIS is an open source AIS implementation
maintained by a team staffed (primarily) by Red Hat employees. OpenAIS
serves as Red Hat Cluster Suite's principal cluster communications
infrastructure.

Specifically, Red Hat Cluster Suite makes use of the Totem group
communication algorithm for reliable group messaging among cluster
members.

Red Hat Cluster Suite in Red Hat Enterprise Linux (RHEL) version 5
adds an abstraction and convenience interface layer above OpenAIS
named +cman+. +cman+ also serves as a compatibility layer to RHEL 4,
in which +cman+ behaved similarly, albeit without utilizing OpenAIS.

[[s-rhcs-ccs]]
==== CCS

The Cluster Configuration System (CCS) and its associated daemon,
+ccsd+, maintains and updates the cluster configuration. Management
applications utilize +ccsd+ and the CCS libraries to query and update
cluster configuration items.

[[s-rhcs-fencing]]
==== Fencing

Red Hat Cluster Suite, originally designed primarily for shared
storage clusters, relies on node fencing to prevent concurrent,
uncoordinated access to shared resources. The Red Hat Cluster Suite
fencing infrastructure relies on the fencing daemon +fenced+, and
fencing agents implemented as shell scripts.

Even though DRBD-based clusters utilize no shared storage resources
and thus fencing is not strictly required from DRBD's standpoint, Red
Hat Cluster Suite still requires fencing even in DRBD-based
configurations.

[[s-rhcs-rgmanager]]
==== The Resource Group Manager

The resource group manager ( +rgmanager+, alternatively +clurgmgr+) is
akin to the Cluster Resource Manager in Heartbeat. It serves as the
cluster management suite's primary interface with the applications it
is configured to manage.

[[s-rhcs-resources]]
===== Red Hat Cluster Suite resources

indexterm:[Red Hat Cluster Suite]A single highly available
application, filesystem, IP address and the like is referred to as a
_resource_ in Red Hat Cluster Suite terminology.

Where resources depend on each other -- such as, for example, an NFS
export depending on a filesystem being mounted -- they form a
_resource tree_, a form of nesting resources inside another. Resources
in inner levels of nesting may inherit parameters from resources in
outer nesting levels. The concept of resource trees is absent in
Heartbeat.

[[s-rhcs-services]]
===== Red Hat Cluster Suite services

indexterm:[Red Hat Cluster Suite]Where resources form a co-dependent
collection, that collection is called a _service_. This is different
from Heartbeat, where such a collection is referred to as a _resource
group_.

[[s-rhcs-resource-agents]]
===== rgmanager resource agents

The resource agents invoked by +rgmanager+ are similar to those used by
the Heartbeat CRM, in the sense that they utilize the same shell-based
API as defined in the Open Cluster Framework ( ), although Heartbeat
utilizes some extensions not defined in the framework. Thus in theory,
the resource agents are largely interchangeable between Red Hat
Cluster Suite and Heartbeat â€” in practive however, the two cluster
management suites use different resource agents even for similar or
identical tasks.

Red Hat Cluster Suite resource agents install into the
+/usr/share/cluster+ directory. Unlike Heartbeat OCF resource agents
which are by convention self-contained, some RHCS resource agents are
split into a +.sh+ file containing the actual shell code, and a
+.metadata+ file containing XML resource agent metadata.

Starting with version 8.3, DRBD includes a Red Hat Cluster Suite
resource agent. It installs into the customary directory as
+drbd.sh+ and +drbd.metadata+.

[[s-rhcs-config]]
=== Red Hat Cluster Suite configuration

This section outlines the configuration steps necessary to get Red Hat
Cluster Suite running. Preparing your cluster configuration is fairly
straightforward; all a DRBD-based RHCS cluster requires are two
participating nodes (referred to as _Cluster Members_ in Red Hat's
documentation) and a fencing device.

NOTE: For more information about configuring Red Hat clusters, see
http://www.redhat.com/docs/manuals/csgfs/[Red Hat's documentation on
the Red Hat Cluster Suite and GFS.]


[[s-rhcs-cluster-conf]]
==== The +cluster.conf+ file

RHEL clusters keep their configuration in a single configuration file,
indexterm:[Red Hat Cluster Suite]indexterm:[cluster.conf (RHCS
configuration file)]+/etc/cluster/cluster.conf+. You may manage your
cluster configuration in the following ways:

.Editing the configuration file directly
This is the most straightforward method. It has no prerequisites other
than having a text editor available.

.Using the `system-config-cluster` GUI
This is a GUI application written in Python using Glade. It requires
the availability of an X display (either directly on a server console,
or tunneled via SSH).

.Using the Conga web-based management infrastructure
The Conga infrastructure consists of a node agent ( `ricci`)
communicating with the local cluster manager, cluster resource
manager, and cluster LVM daemon, and an administration web application
( `luci`) which may be used to configure the cluster infrastructure
using a simple web browser.


[[s-rhcs-failover-clusters]]
=== Using DRBD in RHCS fail-over clusters

NOTE: This section deals exclusively with setting up DRBD for RHCS
fail over clusters not involving GFS. For GFS (and GFS2)
configuration, please see <<ch-gfs>>.

This section, like the <<s-heartbeat-crm-config,corresponding section in the
chapter on Heartbeat clusters>>, assumes you are about to configure a
highly available MySQL database with the following configuration
parameters:

* The DRBD resources to be used as your database storage area is named
  +mysql+, and it manages the device +/dev/drbd0+.

* The DRBD device holds an ext3 filesystem which is to be mounted to
  +/var/lib/mysql+ (the default MySQL data directory).

* The MySQL database is to utilize that filesystem, and listen on a
  dedicated cluster IP address, 192.168.42.1.


[[s-rhcs-example-cluster-conf]]
==== Setting up your cluster configuration

To configure your highly available MySQL database, create or modify
your +/etc/cluster/cluster.conf+ file to contain the following
configuration items.

To do that, open +/etc/cluster/cluster.conf+ with your preferred text
editing application. Then, include the following items in your
resource configuration:

[source,xml]
----------------------------
<rm>
  <resources />
  <service autostart="1" name="mysql">
    <drbd name="drbd-mysql" resource="mysql">
      <fs device="/dev/drbd/by-res/mysql"
          mountpoint="/var/lib/mysql"
          fstype="ext3"
          name="mysql"
          options="noatime"/>
    </drbd>
    <ip address="10.9.9.180" monitor_link="1"/>
    <mysql config_file="/etc/my.cnf"
           listen_address="10.9.9.180"
           name="mysqld"/>
  </service>
</rm>
----------------------------

Nesting resource references inside one another in +<service/>+ is the
Red Hat Cluster way of expressing resource dependencies.

Be sure to increment the +config_version+ attribute, found on the root
+<cluster>+ element, after you have completed your
configuration. Then, issue the following commands to commit your
changes to the running cluster configuration:

----------------------------
ccs_tool update /etc/cluster/cluster.conf
cman_tool version -r version
----------------------------

In the second command, be sure to replace _version_ with the new
cluster configuration version number.

NOTE: Both the +system-config-cluster+ GUI configuration utility and
the Conga web based cluster management infrastructure will complain
about your cluster configuration after including the +drbd+ resource
agent in your +cluster.conf+ file. This is due to the design of the
Python cluster management wrappers provided by these two applications
which does not expect third party extensions to the cluster
infrastructure.

Thus, when you utilize the +drbd+ resource agent in cluster
configurations, it is not recommended to utilize
+system-config-cluster+ nor Conga for cluster configuration
purposes. Using either of these tools to only monitor the cluster's
status, however, is expected to work fine.
