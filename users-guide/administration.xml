
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-admin">
  <title>Common administrative tasks</title>
  <para>This chapter outlines typical administrative tasks encountered
    during day-to-day operations. It does not cover troubleshooting
    tasks, these are covered in detail in <xref
    linkend="ch-troubleshooting"/>.</para>
  <section id="s-check-status">
    <title>Checking DRBD status</title>
    <section id="s-proc-drbd">
      <title>Status information in
      <filename>/proc/drbd</filename></title>
      <para><indexterm>
	  <primary>/proc/drbd</primary>
      </indexterm><filename>/proc/drbd</filename> is a virtual file
	displaying real-time status information about all DRBD
	resources currently configured. You may interrogate this
	file's contents using this command:
	<literallayout><userinput>cat /proc/drbd</userinput>
	  <computeroutput>version: 8.2.4 (api:88/proto:86-88)
GIT-hash: fc00c6e00a1b6039bfcebe37afa3e7e28dbd92fa build by buildsystem@linbit, 2008-01-11 12:44:36
 0: cs:Connected st:Secondary/Secondary ds:UpToDate/UpToDate C r---
    ns:524288 nr:524288 dw:524288 dr:524288 al:0 bm:64 lo:0 pe:0 ua:0 ap:0
        resync: used:0/31 hits:524224 misses:64 starving:0 dirty:0 changed:64
        act_log: used:0/257 hits:0 misses:0 starving:0 dirty:0 changed:0</computeroutput></literallayout>
      </para>
      <para>The first line, prefixed with <code>version:</code>, shows
      the DRBD version used on your system. The second line contains
      information about this specific build.</para>
      <para>The other four lines in this example form a block that is
      repeated for every DRBD device configured, prefixed by the
	device minor number. In this case, this is <code>0</code>,
	corresponding to the device
      <filename>/dev/drbd0</filename>.</para>
      <para>The resource-specific output from
	<filename>/proc/drbd</filename> contains various pieces of
	information about the resource:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>cs</code> (connection state)</title>
	      <indexterm>
		<primary>connection state</primary>
	      </indexterm>
	      <para>Status of the network connection. See <xref
	      linkend="s-connection-states"/> for details about the
	      various connection states.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>st</code> (states)</title>
	      <indexterm>
		<primary>resource</primary>
		<secondary>role</secondary>
	      </indexterm>
	      <para>Roles of the nodes. The role of the local node is
		displayed first, followed by the role of the partner
		node shown after the slash. See <xref
	      linkend="s-resource-states"/> for details about the
		possible resource roles.
		<note>
		  <para>For historical reasons, both
		    <filename>/proc/drbd</filename> and
		    <command>drbdadm</command> use the ambiguous term
		    <quote>state</quote> when referring to resource
		    roles. That usage is about to fade away;
		    <quote>role</quote> is the preferred term.</para>
		</note>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ds</code> (disk states)</title>
	      <indexterm>
		<primary>disk state</primary>
	      </indexterm>
	      <para>State of the hard disks. Prior to the slash the
		state of the local node is displayed, after the slash
		the state of the hard disk of the partner node is
		shown. See <xref
	      linkend="s-disk-states"/> for details about the various
		connection states.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ns</code> (network send)</title>
	      <para> Volume of net data sent to the partner via the
		network connection; in Kibyte.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>nr</code> (network receive)</title>
	      <para> Volume of net data received by the partner via
		the network connection; in Kibyte.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>dw</code> (disk write)</title>
	      <para>Net data written on local hard disk; in
		Kibyte.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>dr</code> (disk read)</title>
	      <para>Net data read from local hard disk; in Kibyte.
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>al</code> (activity log)</title>
	      <para>Number of updates of the AL area of the meta
		data.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>bm</code> (bit map)</title>
	      <para> Number of updates of the bitmap area of the meta
		data. </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>lo</code> (local count)</title>
	      <para>Number of open requests to the local I/O sub-system
		issued by DRBD. </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>pe</code> (pending)</title>
	      <para>Number of requests sent to the partner, but that
		have not yet been answered by the latter.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ua</code> (unacknowledged)</title>
	      <para>Number of requests received by the partner via the
		network connection, but that have not yet been
		answered. </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ap</code> (application pending)</title>
	      <para>Number of block I/O requests forwarded to DRBD, but
		not yet answered by DRBD.</para> 
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-connection-states">
      <title>Connection states</title>
      <para><indexterm>
	<primary>connection state</primary>
      </indexterm>
	A resource's connection state can be observed either by
	monitoring <filename>/proc/drbd</filename>, or by issuing the
	<command>drbdadm cstate</command> command:
      <literallayout><userinput>drbdadm cstate <replaceable>resource</replaceable></userinput>
<computeroutput>Connected</computeroutput></literallayout></para>
      <para>A resource may have one of the following connection
	states:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>StandAlone</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>StandAlone</secondary>
	      </indexterm>
	      <para>No network configuration available. The resource
		has not yet been connected, or has been
		administratively disconnected (using <command>drbdadm
		  disconnect</command>), or has dropped its connection
		due to failed authentication or split brain.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Disconnecting</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>Disconnecting</secondary>
	      </indexterm>
	      <para> Temporary state during disconnection. The next
		state is StandAlone.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Unconnected</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>Unconnected</secondary>
	      </indexterm>
	      <para> Temporary state, prior to a connection attempt.
		Possible next states: WFConnection and
		WFReportParams.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Timeout</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>Timeout</secondary>
	      </indexterm>
	      <para>Temporary state following a timeout in the
		communication with the peer. Next state:
		Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>BrokenPipe</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>BrokenPipe</secondary>
	      </indexterm>
	      <para>Temporary state after the connection to the peer
		was lost. Next state: Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>NetworkFailure</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>NetworkFailure</secondary>
	      </indexterm>
	      <para>Temporary state after the connection to the
		partner was lost. Next state: Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ProtocolError</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>ProtocolError</secondary>
	      </indexterm>
	      <para>Temporary state after the connection to the
		partner was lost. Next state: Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>TearDown</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>TearDown</secondary>
	      </indexterm>
	      <para>Temporary state. The peer is closing the
		connection. Next state: Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFConnection</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFConnection</secondary>
	      </indexterm>
	      <para>This node is waiting until the peer node becomes
		visible on the network. </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFReportParams</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFReportParams</secondary>
	      </indexterm>
	      <para>TCP connection has been established, this node
		waits for the first network packet from the
		peer.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Connected</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>Connected</secondary>
	      </indexterm>
	      <para>A DRBD connection has been established, data
		mirroring is now active. This is the normal
		state.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>StartingSyncS</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>StartingSyncS</secondary>
	      </indexterm>
	      <para>Full synchronization, initiated by the
		administrator, is just starting. The next possible
		states are: SyncSource or PausedSyncS.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>StartingSyncT</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>StartingSyncT</secondary>
	      </indexterm>
	      <para>Full synchronization, initiated by the
		administrator, is just starting. Next state:
		WFSyncUUID.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFBitMapS</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFBitMapS</secondary>
	      </indexterm>
	      <para>Partial resnchronisation is just starting. Next
		possible states: SyncSource or PausedSyncS.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFBitMapT</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFBitMapT</secondary>
	      </indexterm>
	      <para>Partial resnchronisation is just starting. Next
		possible state: WFSyncUUID.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFSyncUUID</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFSyncUUID</secondary>
	      </indexterm>
	      <para>Synchronization is about to begin. Next possible
		states: SyncTarget or PausedSyncT.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>SyncSource</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>SyncSource</secondary>
	      </indexterm>
	      <para>Synchronization is currently running, with the
		local node being the source of
		synchronization.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>SyncTarget</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>SyncTarget</secondary>
	      </indexterm>
	      <para>Synchronization is currently running, with the
		local node being the target of
		synchronization.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>PausedSyncS</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>PausedSyncS</secondary>
	      </indexterm>
	      <para>The local node is the source of an ongoing
		synchronization, but synchronization is currently
		paused. This may be due to a dependency on the
		completion of another synchronization process, or
		due to synchronization having been manually
		interrupted by <command>drbdadm
		  pause-sync</command>.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>PausedSyncT</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>PausedSyncT</secondary>
	      </indexterm>
	      <para>The local node is the target of an ongoing
		synchronization, but synchronization is currently
		paused. This may be due to a dependency on the
		completion of another synchronization process, or
		due to synchronization having been manually
		interrupted by <command>drbdadm
		  pause-sync</command>.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>VerifyS</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>VerifyS</secondary>
	      </indexterm>
	      <para>On-line device verification is currently running,
		with the local node being the source of
		verification.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>VerifyT</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>VerifyT</secondary>
	      </indexterm>
	      <para>On-line device verification is currently running,
		with the local node being the target of
		verification.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-resource-states">
      <title>Resource roles</title>
      <para><indexterm>
	  <primary>resource</primary>
	  <secondary>role</secondary></indexterm>A resource's role can
	be observed either by monitoring
	<filename>/proc/drbd</filename>, or by issuing the
	<indexterm>
	  <primary>drbdadm</primary>
	  <secondary>state</secondary>
	</indexterm> <command>drbdadm state</command> command:
	<literallayout><userinput>drbdadm state <replaceable>resource</replaceable></userinput>
<computeroutput>Primary/Secondary</computeroutput></literallayout> 
	The local resource role is always displayed first, the remote
	resource role last.</para>
      <para>You may see one of the following resource roles:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>Primary</code></title>
	      <para>The resource is currently in the primary role, and
	      may be read from and written to. This role only occurs
		on one of the two nodes, unless <link
	      linkend="s-dual-primary-mode">dual-primary node</link>
	      is enabled.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Secondary</code></title>
	      <para>The resource is currently in the secondary
	      role. It normally receives updates from its peer (unless
	      running in disconnected mode), but may neither be read
	      from nor written to. This role may occur on one node or
	      both nodes.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Unknown</code></title>
	      <para>The resource's role is currently unknown. The
	      local resource role never has this status. It is only
	      displayed for the peer's resource role, and only in
	      disconnected mode.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-disk-states">
      <title>Disk states</title>
      <para>A resource's disk state can be observed either by
	monitoring <filename>/proc/drbd</filename>, or by issuing the
	<command>drbdadm dstate</command> command:
      <literallayout><userinput>drbdadm dstate <replaceable>resource</replaceable></userinput>
<computeroutput>UpToDate/UpToDate</computeroutput></literallayout>
      The local disk state is always displayed first, the remote disk
      state last.</para>
      <para>
	Both the local and the remote disk state may be one of the
	following:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>Diskless</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Diskless</secondary>
	      </indexterm>
	      <para>No local block device has been assigned to the
		DRBD driver. This may mean that the resource has never
		attached to its backing device, that it has been
		manually detached using <command>drbdadm
		  detach</command>, or that it automatically detached
		after a lower-level I/O error.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Attaching</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Attaching</secondary>
	      </indexterm>
	      <para> Transient state during which meta data are
		read.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Failed</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Failed</secondary>
	      </indexterm>
	      <para>Transient state following an I/O failure report by
		the local block device. Next state: Diskless.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Negotiating</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Negotiating</secondary>
	      </indexterm>
	      <para>Transient state when an Attach is carried out on
		an already-connected DRBD device.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Inconsistent</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Inconsistent</secondary>
	      </indexterm>
	      <para>The data is inconsistent. This status occurs
		immediately upon creation of a new resource, on both
		nodes (before the initial full sync). Also, this
		status is found in one node (the synchronization
		target) during synchronization.
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Outdated</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Outdated</secondary>
	      </indexterm>
	      <para>Resource data is consistent, but <link
		  linkend="s-outdate">outdated</link>.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>DUnknown</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>DUnknown</secondary>
	      </indexterm>
	      <para>This state is used for the peer disk if no network
		connection is available.</para> 
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Consistent</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Consistent</secondary>
	      </indexterm>
	      <para>Consistent data of a node without connection. When
		the connection is established, it is decided whether
		the data are UpToDate or Outdated.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>UpToDate</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>UpToDate</secondary>
	      </indexterm>
	      <para>Consistent, up-to-date state of the data. This is
		the normal state.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
  </section>
  <section id="s-enable-disable">
    <title>Enabling and disabling resources</title>
    <section id="s-enable-resource">
      <title>Enabling resources</title>
      <para><indexterm>
	<primary>resource</primary>
	<secondary>enabling</secondary>
      </indexterm>
	Normally, all resources configured in
	<filename>/etc/drbd.conf</filename> are automatically enabled
	upon system startup by the <filename>/etc/init.d/drbd</filename>
	init script. If you choose to disable this startup script (as
	may be required by some applications), you may enable specific
	resources by issuing the commands
      <literallayout><userinput>drbdadm attach <replaceable>resource</replaceable></userinput>
<userinput>drbdadm connect <replaceable>resource</replaceable></userinput></literallayout>
      or the shorthand version of the above,
      <literallayout><userinput>drbdadm up <replaceable>resource</replaceable></userinput></literallayout>
	</para>
      <para>As always, you may use the keyword <code>all</code>
	instead of a specific resource name if you want to enable all
	resources configured in <filename>/etc/drbd.conf</filename> at
	once.
      </para>
    </section>
    <section id="s-disable-resource">
      <title>Disabling resources</title>
      <para><indexterm>
	<primary>resource</primary>
	<secondary>disabling</secondary>
      </indexterm>
	You may temporarily disable specific
	resources by issuing the commands
      <literallayout><userinput>drbdadm disconnect <replaceable>resource</replaceable></userinput>
<userinput>drbdadm detach <replaceable>resource</replaceable></userinput></literallayout>
      or the shorthand version of the above,
      <literallayout><userinput>drbdadm down <replaceable>resource</replaceable></userinput></literallayout>
	<note>
	  <para>There is, in fact, a slight syntactical difference
	    between these two methods. While <command>drbdadm
	      down</command> implies a preceding <link
	      linkend="s-switch-resource-roles">resource
	      demotion</link>, <command>drbdadm
	      disconnect/detach</command> does not. So while you can
	    run <command>drbdadm down</command> on a resource that is
	    currently in the primary role, <command>drbdadm
	      disconnect/detach</command> in the same situation will
	    be refused by DRBD's internal state engine.</para>
	</note>
      </para>
      <para>Here, too, you may use the keyword <code>all</code> in
	place of a resource name if you wish to temporarily disable
	all resources listed in <filename>/etc/drbd.conf</filename> at
	once.
      </para>
    </section>
  </section>
  <section id="s-reconfigure">
    <title>Reconfiguring resources</title>
    <para><indexterm>
	<primary>resource</primary>
	<secondary>reconfiguring</secondary>
      </indexterm>
      DRBD allows you to reconfigure resources while they are
      operational. To that end,
      <itemizedlist>
	<listitem>
	  <para>make any necessary changes to the resource
	    configuration in
	    <filename>/etc/drbd.conf</filename>,</para>
	</listitem>
	<listitem>
	  <para>synchronize your <filename>/etc/drbd.conf</filename>
	    file between both nodes,</para>
	</listitem>
	<listitem>
	  <para>issue the <indexterm>
	      <primary>drbdadm</primary>
	      <secondary>adjust</secondary>
	    </indexterm> <command>drbdadm adjust
	      <replaceable>resource</replaceable></command> command on
	    both nodes.</para>
	</listitem>
      </itemizedlist>
    </para>
    <para><command>drbdadm adjust</command> then hands off to
      <command>drbdsetup</command> to make the necessary adjustments
      to the configuration. As always, you are able to review the
      pending <command>drbdsetup</command> invocations by running
      <command>drbdadm</command> with the <option>-d</option>
      (dry-run) option.</para>
    <note>
      <para>When making changes to the <code>common</code> section in
	<filename>/etc/drbd.conf</filename>, you can adjust the
	configuration for all resources in one run, by issuing
	<command>drbdadm adjust all</command>.</para>
    </note>
  </section>
  <section id="s-switch-resource-roles">
    <title>Promoting and demoting resources</title>
    <para><indexterm>
      <primary>resource</primary>
      <secondary>promotion</secondary>
    </indexterm><indexterm>
      <primary>resource</primary>
      <secondary>demotion</secondary>
    </indexterm>Manually switching a <link
	linkend="s-resource-roles">resource's role</link> from
      secondary to primary (promotion) or vice versa (demotion) is
      done using the following commands:
      <literallayout><userinput>drbdadm primary <replaceable>resource</replaceable></userinput>
<userinput>drbdadm secondary <replaceable>resource</replaceable></userinput></literallayout>
    </para>
      <para>
      In <link linkend="s-single-primary-mode">single-primary
	mode</link> (DRBD's default), any resource can be in the
      primary role on only one node at any given time while the <link
	linkend="s-connection-states">connection state</link> is
      <code>Connected</code>. Thus, issuing <command>drbdadm primary
	<replaceable>resource</replaceable></command> on one node
      while <replaceable>resource</replaceable> is still in the
      primary role on the peer will result in an error.</para>
    <para>A resource configured to allow <link
	linkend="s-dual-primary-mode">dual-primary mode</link> can be
	switched to the primary role on both nodes.</para>
  </section>
  <section id="s-enable-dual-primary">
    <title>Enabling dual-primary mode</title>
    <para>This feature is available in DRBD 8.0 and later.</para>
    <para><indexterm>
      <primary>dual-primary mode</primary>
    </indexterm>
      To enable dual-primary mode, add
	the <option>allow-two-primaries</option> option to the
	<code>net</code> section of your resource
	configuration:
	<programlisting>resource <replaceable>resource</replaceable>
  net { 
    allow-two-primaries; 
  }
  ...
}</programlisting></para>
    <para>
      When a resource is configured to support dual-primary mode, it
      may also be desirable to automatically switch the resource into
      the primary role upon system (or DRBD) startup. To do this, add
      the <option>become-primary-on</option> option, available in DRBD
      8.2.0 and above, to the <code>startup</code> section of your
      resource configuration:
      <programlisting>resource <replaceable>resource</replaceable>
  startup { 
    become-primary-on both;
  }
  ...
}</programlisting></para>
    <para>After you have made these changes to
      <filename>/etc/drbd.conf</filename>, do not forget to
      synchronize the configuration between nodes. Then, you can use
      <command>drbdadm adjust
      <replaceable>resource</replaceable></command> (on both nodes),
      and afterwards, <command>drbdadm primary
      <replaceable>resource</replaceable></command> (again, on both
      nodes).</para>
  </section>
  <section id="s-use-online-verify">
    <title>Using on-line device verification</title>
    <para>This feature is available in DRBD 8.2.5 and later.</para>
    <section id="s-online-verify-enable">
      <title>Enabling on-line verification</title>
      <para><indexterm>
	<primary>on-line device verification</primary>
	<secondary>enabling</secondary>
      </indexterm>
	<link linkend="s-online-verify">On-line device
	  verification</link> is not enabled for resources by default.
	To enable it, add the following lines to your resource
	configuration in <filename>/etc/drbd.conf</filename>:
	<programlisting>resource <replaceable>resource</replaceable>
  syncer { 
    verify-alg <replaceable>algorithm</replaceable>;
  }
  ...
}</programlisting> 
	<replaceable>algorithm</replaceable>
	may be any message digest algorithm supported by the kernel
	crypto API in your system's kernel configuration. Normally,
	you should be able to choose at least from <code>sha1</code>,
	<code>md5</code>, and <code>crc32c</code>.</para>
      <para>If you make this change to an existing resource, as
	always, synchronize your <filename>drbd.conf</filename> to the
	peer. and run <command>drbdadm adjust
	  <replaceable>resource</replaceable></command> on both
	nodes.</para>
    </section>
    <section id="s-online-verify-invoke">
      <title>Invoking on-line verification</title>
       <para><indexterm>
	<primary>on-line device verification</primary>
	<secondary>invoking</secondary>
      </indexterm>After you have enabled on-line verification, you will be
	able to initiate a verification run using the following
	command: 
	<literallayout><userinput>drbdadm verify <replaceable>resource</replaceable></userinput></literallayout> 
	When you do so, DRBD starts an online verification run for
	<replaceable>resource</replaceable>, and if it detects any
	blocks not in sync, will mark those blocks as such and write a
	message to the kernel log. Any applications using the device
	at that time can continue to do so unimpeded, and you may also
	<link linkend="s-switch-resource-roles">switch resource
	  roles</link> at will.</para>
      <para>If out-of-sync blocks were detected during the
	verification run, you may resynchronize them using the
	following commands after verification has completed:
	<literallayout><userinput>drbdadm disconnect <replaceable>resource</replaceable></userinput>
<userinput>drbdadm connect <replaceable>resource</replaceable></userinput></literallayout>
      </para>
    </section>
    <section id="s-online-verify-automate">
      <title>Automating on-line verification</title>
      <para><indexterm>
	<primary>on-line device verification</primary>
	<secondary>automating</secondary>
      </indexterm>Most users will want to automate on-line device
	verification. This can be easily accomplished. Create a file
	with the following contents, named
	<filename>/etc/cron.d/drbd-verify</filename> on
	<emphasis>one</emphasis> of your nodes:
      <programlisting>42 0 * * 0    root    /sbin/drbdadm verify <replaceable>resource</replaceable></programlisting>
	This will have <code>cron</code> invoke a device verification
      every Sunday at 42 minutes past midnight.</para>
      <para>If you have enabled on-line verification for all your
	resources (for example, by adding <code>verify-alg
	  <replaceable>algorithm</replaceable></code> to the
	  <code>common</code> section in
	<filename>/etc/drbd.conf</filename>), you may also use:
      <programlisting>42 0 * * 0    root    /sbin/drbdadm verify all</programlisting>
      </para>
    </section>
  </section>
  <section id="s-configure-syncer-rate">
    <title>Configuring the rate of synchronization</title>
     <para><indexterm>
      <primary>synchronization</primary>
      <secondary>configuring rate</secondary>
    </indexterm>Normally, one tries to ensure that background
      synchronization (which makes the data on the synchronization
      target temporarily inconsistent) completes as quickly as
      possible. However, it is also necessary to keep background
      synchronization from hogging all bandwidth otherwise
      available for foreground replication, which would be detrimental
      to application performance. Thus, you must configure the
      sychronization bandwidth to match your hardware &mdash; which
      you may do in a permanent fashion or on-the-fly.</para>
    <important>
      <para>It does not make sense to set a synchronization rate
	that is higher than the maximum write throughput on your
	secondary node. You must not expect your secondary node to
	miraculously be able to write faster than its I/O subsystem
	allows, just because it happens to be the target of an ongoing
	device synchronization.</para>
      <para>Likewise, and for the same reasons, it does not make sense
	to set a synchronization rate that is higher than the
	bandwidth available on the replication network.</para>
    </important>
    <section id="s-configure-syncer-rate-permanent">
      <title>Permanent syncer rate configuration</title>
      <para>The maximum bandwidth a resource uses for background
	re-synchronization is permanently configured using the
	<option>rate</option> option for a resource. This must be
	included in the resource configuration's
	<option>syncer</option> section in
	<filename>/etc/drbd.conf</filename>:
	<programlisting>resource <replaceable>resource</replaceable>
  syncer { 
    rate 40M;
    ...
  }
  ...
}</programlisting></para>
      <para>Note that the rate setting is given in
	<emphasis>bytes</emphasis>, not <emphasis>bits</emphasis> per
	second. </para>
      <tip>
	<para>A good rule of thumb for this value is to use about 30%
	  of the available replication bandwidth. Thus, if you had an
	  I/O subsystem capable of sustaining write throughput of
	  180MB/s, and a Gigabit Ethernet network capable of
	  sustaining 110 MB/s network throughput (the network being
	  the bottleneck), you would calculate:
	  <equation id="eq-syncer-rate-example1">
	    <title>Syncer rate example, 110MB/s effective available
	      bandwidth</title>
	    <alt>\[110\cdot0.3=33\]</alt>
	    <graphic fileref="syncer-rate-example1"/>
	  </equation> Thus, the recommended value for the
	  <option>rate</option> option would be
	  <code>33M</code>.</para>
	<para>By contrast, if you had an I/O subsystem with a maximum
	  throughput of 80MB/s and a Gigabit Ethernet connection (the
	  I/O subsystem being the bottleneck), you would calculate:
	  <equation id="eq-syncer-rate-example2">
	    <title>Syncer rate example, 80MB/s effective available
	      bandwidth</title>
	    <alt>\[80\cdot0.3=24\]</alt>
	    <graphic fileref="syncer-rate-example2"/>
	  </equation> In this case, the recommended value for the
	  <option>rate</option> option would be
	  <code>24M</code>.
	</para>
      </tip>
    </section>
    <section id="s-configure-syncer-rate-temporary">
      <title>Temporary syncer rate configuration</title>
      <para>It is sometimes desirable to temporarily adjust the syncer
	rate. For example, you might want to speed up background
	re-synchronization after having performed scheduled
	maintenance on one of your cluster nodes. Or, you might want
	to throttle background re-synchronization if it happens to
	occur at a time when your application is extremely busy with
	write operations, and you want to make sure that a large
	portion of the existing bandwidth is available to
	replication.</para>
      <para>For example, in order to make most bandwidth of a Gigabit
	Ethernet link available to re-synchronization, issue the
	following command:
	<literallayout><userinput>drbdsetup <filename>/dev/drbd<replaceable>num</replaceable></filename> syncer -r 110M</userinput></literallayout> 
	As always, replace <replaceable>num</replaceable> with the
	device minor number of your DRBD device. You need to issue
	this command on only one of your nodes.</para>
      <para>To revert this temporary setting and re-enable the syncer
	rate set in <filename>/etc/drbd.conf</filename>, issue this
	command:
	<literallayout><userinput>drbdadm adjust <replaceable>resource</replaceable></userinput></literallayout>
      </para>
    </section>
  </section>
  <section id="s-configure-io-error-behavior">
    <title>Configuring I/O error handling strategies</title>
     <para><indexterm>
	<primary>I/O errors</primary>
    </indexterm>
    <indexterm>
	<primary>drbd.conf</primary>
	<secondary>on-io-error</secondary>
    </indexterm>DRBD's <link linkend="s-handling-disk-errors">strategy
	for handling lower-level I/O errors</link> is determined by
      the <option>on-io-error</option> option, included in the
      resource <code>disk</code> configuration in
      <filename>/etc/drbd.conf</filename>:
      <programlisting>resource <replaceable>resource</replaceable> {
  disk {
    on-io-error <replaceable>strategy</replaceable>;
    ...
  }
  ...
}</programlisting>
      You may, of course, set this in the <code>common</code> section
    too, if you want to define a global I/O error handling policy for
    all resources.</para>
    <para><replaceable>strategy</replaceable> may be one of the
    following options:
      <itemizedlist>
	<listitem>
	  <formalpara>
	    <title><option>detach</option></title>
	    <para>This is the recommended option. On the occurrence
	      of a lower-level I/O error, the node drops its backing
	      device, and continues in diskless mode.</para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title><option>pass_on</option></title>
	    <para>This causes DRBD to report the I/O error to the
	      upper layers. On the primary node, it is reported it to
	      the mounted file system. On the secondary node, it is
	      ignored (because the secondary has no upper layer to
	      report to). This is the default for historical reasons,
	      but is no longer recommended for most new installations
	      &mdash; except if you have a very compelling reason to
	      use this strategy, instead of
	      <option>detach</option>.</para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title><option>call-local-io-error</option></title>
	    <para>Invokes the command defined as the local I/O error
	      handler. This requires that a corresponding
	      <option>local-io-error</option> command invocation is
	      defined in the resource's <code>handlers</code> section.
	      It is entirely left to the administrator's discretion to
	      implement I/O error handling using the command (or
	      script) invoked by <option>local-io-error</option>.
	      <note>
		<para>Early DRBD versions (prior to 8.0) included
		  another option, <option>panic</option>, which would
		  forcibly remove the node from the cluster by way of
		  a kernel panic, whenever a local I/O error occurred.
		  While that option is no longer available, the same
		  behavior may be mimicked via the
		  <option>local-io-error</option>/
		  <option>call-local-io-error</option> interface. You
		  should do so only if you fully understand the
		  implications of such behavior.</para>
	      </note>
	    </para>
	  </formalpara>
	</listitem>
      </itemizedlist>
    </para>
    <para>You may reconfigure a running resource's I/O error handling
      strategy by following the following process: 
      <itemizedlist>
	<listitem>
	  <para>Edit the resource configuration in
	    <filename>/etc/drbd.conf</filename>.</para>
	</listitem>
	<listitem>
	  <para>Copy the configuration to the peer node.</para>
	</listitem>
	<listitem>
	  <para>Issue
	    <command>drbdadm adjust <replaceable>resource</replaceable></command> 
	    on the node where the resource is currently in the
	    secondary role.</para>
	</listitem>
	<listitem>
	  <para>Switch resource roles (if your system is managed by a
	    cluster management application, this would involve a
	    manual switch-over of cluster resources).</para>
	</listitem>
	<listitem>
	  <para>Again, issue
	    <command>drbdadm adjust <replaceable>resource</replaceable></command> 
	    on the node where the resource is currently in the
	    secondary role (this is <emphasis>not</emphasis> the same
	    node as the one where you issued the command
	    earlier).</para>
	</listitem>
	<listitem>
	  <para>Switch resource roles (or cluster node roles) back, if
	    desired.</para>
	</listitem>
      </itemizedlist>
    </para>
  </section>
  <section id="s-resizing">
    <title>Resizing resources</title>
      <section id="s-growing-online">
	<title>Growing on-line</title>
	<para><indexterm>
	  <primary>resource</primary>
	  <secondary>growing</secondary>
	  <tertiary>on-line</tertiary>
	</indexterm>If the backing block devices can be grown while in
	operation (online), it is also possible to increase the size
	of a DRBD device based on these devices during operation. To
	do so, two criteria must be fulfilled:
	<orderedlist>
	  <listitem>
	    <para>The affected resource's backing device must be one
	      managed by a logical volume management subsystem, such
	      as LVM or EVMS.</para>
	  </listitem>
	  <listitem>
	    <para>The resource must currently be in the
	      <code>Connected</code> connection state.</para>
	  </listitem>
	</orderedlist>
      </para>
	<para>Having grown the backing block devices on both nodes,
	  ensure that only one node is in primary state. Then enter on
	  one node:</para>
	<programlisting>drbdadm resize <replaceable>resource</replaceable></programlisting>
	<para>This triggers a synchronization of the new section.
	  The synchronization is done from the primary node to the
	  secondary node.</para>
      </section>
      <section id="s-growing-offline">
	<title>Growing off-line</title>
	<para><indexterm>
	  <primary>resource</primary>
	  <secondary>growing</secondary>
	  <tertiary>off-line</tertiary>
	</indexterm>When the backing block devices on both nodes are grown
	  while DRBD is inactive, the new size is recognized
	  automatically. No administrative intervention is
	  necessary.</para>
	<para>The resulting DRBD device will have the new size after
	  the next activation of DRBD on both nodes and a successful
	  establishment of a network connection.</para>
      </section>
      <section id="s-shrinking-online">
	<title>Shrinking on-line</title>
      <para><indexterm>
	  <primary>resource</primary>
	  <secondary>shrinking</secondary>
	  <tertiary>on-line</tertiary>
	</indexterm>Before shrinking a DRBD device, you
	<emphasis>must</emphasis> shrink that the layers above DRBD,
	i.e. usually the file system. Since DRBD cannot ask the file
	system how much space it actually uses, you have to be careful
	in order not to cause data loss. <note>
	  <para>Whether or not the <emphasis>filesystem</emphasis> can
	    be shrunk on-line depends on the filesystem being used.
	    Most filesystems do not support on-line shrinking. XFS
	    does not support shrinking at all.</para>
	</note>
	</para>
	<para>When internal meta data are used, make sure to consider
	  the space required by the meta data. The size communicated
	  to <command>drbdadm resize</command> is the net size for the
	  file system. In the case of internal meta data, the gross
	  size required by DRBD is higher (see also <xref
	    linkend="s-meta-data-size"/>).</para>
	<para>To shrink DRBD on-line, issue the following command
	  <emphasis>after</emphasis> you have shrunk the file system
	  residing on top of it:</para>
	<programlisting>drbdadm -- --size=<replaceable>new-size</replaceable> resize <replaceable>resource</replaceable></programlisting>
	<para>You may use the usual multiplier suffixes for
	  <replaceable>new-size</replaceable> (K, M, G etc.). After
	  you have shrunk DRBD, you may also shrink the containing
	  block device (if it supports shrinking).</para>
      </section>
      <section id="s-shrinking-offline">
	<title>Shrinking off-line</title>
	<para><indexterm>
	  <primary>resource</primary>
	  <secondary>shrinking</secondary>
	  <tertiary>off-line</tertiary>
	</indexterm>If you were to shrink a backing block device while DRBD
	  is inactive, DRBD would refuse to attach to this block
	  device during the next attach attempt, since it is now too
	  small (in case external meta data are used), or it would be
	  unable to find its meta data (in case internal meta data are
	  used). To work around these issues, use this procedure (if
	  you cannot use <link linkend="s-shrinking-online">on-line
	    shrinking</link>):</para>
	<warning>
	  <para>This is an advanced procedure. Use at your own
	  discretion.</para>
	</warning>
	<orderedlist>
	  <listitem>
	    <para>Shrink the file system from one node, while DRBD is
	      still configured.</para>
	  </listitem>
	  <listitem>
	    <para>Unconfigure your DRBD resource:</para>
	    <programlisting>drbdadm down <replaceable>resource</replaceable></programlisting>
	  </listitem>
	  <listitem>
	    <para>Save the meta data in a text file prior to
	      shrinking: <programlisting>drbdadm dump-md <replaceable>resource</replaceable> &gt; <filename>/tmp/metadata</filename></programlisting></para>
	    <para>You must do this on both nodes, using a separate
	      dump file for every node. <emphasis>Do not</emphasis>
	      dump the meta data on one node, and simply copy the dump
	      file to the peer. This will not work.</para>
	  </listitem>
	  <listitem>
	    <para>Shrink the backing block device on both
	      nodes.</para>
	  </listitem>
	  <listitem>
	    <para>Adjust the size information
	      (<code>la-size-sect</code>) in the file
	      <filename>/tmp/metadata</filename> accordingly, on both
	      nodes. Remember that <code>la-size-sect</code> must be
	      specified in sectors.</para>
	  </listitem>
	  <listitem>
	    <para><emphasis>Only if you are using internal
		metadata</emphasis> (which at this time have probably
	      been lost due to the shrinking process), re-initialize
	      the metadata area:</para>
	    <programlisting>drbdadm create-md <replaceable>resource</replaceable></programlisting>
	  </listitem>
	  <listitem>
	    <para>Re-import the corrected meta data, on both
	      nodes:
	    <literallayout><userinput>drbdmeta_cmd=$(drbdadm -d	dump-md test-disk)</userinput>
<userinput>${drbdmeta_cmd/dump-md/restore-md} /tmp/metadata</userinput> 
  <computeroutput> Valid meta-data in place, overwrite? [need to type 'yes' to confirm]</computeroutput> <userinput>yes</userinput>
  <computeroutput> Successfully restored meta data</computeroutput></literallayout>
	    <note>
	      <para>This example uses <code>bash</code> parameter
		substitution. It may or may not work in other shells.
		Check your <envar>SHELL</envar> environment variable
		if you are unsure which shell you are currently
		using.</para>
	    </note></para>
	  </listitem>
	  <listitem>
	    <para>Re-enable your DRBD resource:</para>
	    <programlisting>drbdadm up <replaceable>resource</replaceable></programlisting>
	  </listitem>
	</orderedlist>
      </section>
    </section>
  <section id="s-disable-flushes" status="draft">
    <title>Disabling backing device flushes</title>
    <para>
      <caution>
	<para>You should only disable device flushes when running DRBD
	  on devices with a battery-backed write cache (BBWC). Most
	  storage controllers allow to automatically disable the write
	  cache when the battery is depleted, switching to
	  write-through mode when the battery dies. It is strongly
	  recommended to enable such a feature.</para>
	<para>Disabling DRBD's flushes when running without BBWC, or
	  on BBWC with a depleted battery, is <emphasis>likely to
	  cause data loss</emphasis> and should not be attempted.</para>
      </caution>
    </para>
    <para>DRBD allows you to enable and disable <link
	linkend="s-disk-flush-support">backing device flushes</link>
      separately for the replicated data set and DRBD's own meta data.
      Both of these options are enabled by default. If you wish to
      disable either (or both), you would set this in the
      <code>disk</code> section for the DRBD configuration file,
      <filename>/etc/drbd.conf</filename>.</para>
    <para>To disable disk flushes for the replicated data set, include
      the following line in your configuration:
      <programlisting>resource <replaceable>resource</replaceable>
  disk {
    no-disk-flushes;
    ...
  }
  ...
}</programlisting></para>
    <para>To disable disk flushes on DRBD's meta data, include the
      following line:
      <programlisting>resource <replaceable>resource</replaceable>
  disk {
    no-md-flushes;
    ...
  }
  ...
}</programlisting></para>
    <para>After you have modified your resource configuration (and
      synchronized your <filename>/etc/drbd.conf</filename> between
      nodes, of course), you may enable these settings by issuing
      these commands on both nodes:
      <literallayout>drbdadm down <replaceable>resource</replaceable>
drbdadm up <replaceable>resource</replaceable></literallayout></para>
  </section>
  <section id="s-split-brain-notification">
    <title>Configuring split brain notification</title>
    <para>DRBD invokes the <code>split-brain</code> handler, if
      configured, at any time split brain is detected. To configure
      this handler, add the following item to your resource
      configuration:
      <programlisting>resource <replaceable>resource</replaceable>
  handlers {
    split-brain <replaceable>handler</replaceable>;
    ...
  }
  ...
}</programlisting>
      <replaceable>handler</replaceable> may be any executable
      present on the system.</para>
    <para>Since DRBD version 8.2.6, the DRBD distribution contains a
      split brain handler script that installs as
      <filename>/usr/lib/drbd/notify-split-brain.sh</filename>. It
      simply sends a notification e-mail message to
      a specified address. To configure the handler to send a message
      to <code>root@localhost</code> (which is expected to be an email
      address that forwards the notification to a real system
      administrator), configure the <code>split-brain handler</code> as
      follows:
      <programlisting>resource <replaceable>resource</replaceable>
  handlers {
    split-brain "/usr/lib/drbd/notify-split-brain.sh root";
    ...
  }
  ...
}</programlisting>
    </para>
    <para>After you have made this modfication on a running resource
    (and synchronized the configuration file between nodes), no
    additional intervention is needed to enable the handler. DRBD will
    simply invoke the newly-configured handler on the next occurrence
    of split brain.</para>
  </section>
  <section id="s-automatic-split-brain-recovery-configuration">
    <title>Configuring automatic split brain recovery policies</title>
    <para>In order to be able to enable and configure DRBD's automatic
      split brain recovery policies, you must understand that DRBD
      offers several configuration options for this purpose. DRBD
      applies its split brain recovery procedures based on the number
      of nodes in the Primary role at the time the split brain is
      detected. To that end, DRBD examines the following keywords, all
      found in the resource's <code>net</code> configuration section:
      <itemizedlist>
	<listitem>
	  <formalpara>
	    <title><code>after-sb-0pri</code></title>
	    <para>Split brain has just been detected, but at this time
	      the resource is not in the Primary role on any host. For
	      this option, DRBD understands the following keywords:
		<itemizedlist>
		<listitem>
		  <formalpara>
		    <title><code>disconnect</code></title>
		    <para>Do not recover automatically, simply invoke
		      the <code>split-brain</code> handler script (if
		      configured), drop the connection and continue in
		      disconnected mode.</para>
		  </formalpara>
		</listitem>
		<listitem>
		  <formalpara>
		    <title><code>discard-younger-primary</code></title>
		    <para>Discard and roll back the modifications made
		      on the host which assumed the Primary role
		      last.</para>
		  </formalpara>
		</listitem>
		<listitem>
		  <formalpara>
		    <title><code>discard-least-changes</code></title>
		    <para>Discard and roll back the modifications on
		      the host where fewer changes occurred.</para>
		  </formalpara>
		</listitem>
		<listitem>
		  <formalpara>
		    <title><code>discard-zero-changes</code></title>
		    <para>If there is any host on which no changes
		      occurred at all, simply apply all modifications
		      made on the other and continue.</para>
		  </formalpara>
		</listitem>
		</itemizedlist>
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title><code>after-sb-1pri</code></title>
	    <para>Split brain has just been detected, and at this time
	      the resource is in the Primary role on one host. For
	      this option, DRBD understands the following keywords:
		<itemizedlist>
		<listitem>
		  <formalpara>
		    <title><code>disconnect</code></title>
		    <para>As with <code>after-sb-0pri</code>, simply
		      invoke the <code>split-brain</code> handler
		      script (if configured), drop the connection and
		      continue in disconnected mode.</para>
		  </formalpara>
		</listitem>
		<listitem>
		  <formalpara>
		    <title><code>consensus</code></title>
		    <para>Apply the same recovery policies as
		      specified in <code>after-sb-0pri</code>. If a
		      split brain victim can be selected after
		      applying these policies, automatically resolve.
		      Otherwise, behave exactly as if
		      <code>disconnect</code> were specified.</para>
		  </formalpara>
		</listitem>
		<listitem>
		  <formalpara>
		    <title><code>call-pri-lost-after-sb</code></title>
		    <para>Apply the recovery policies as specified in
		      <code>after-sb-0pri</code>. If a split brain
		      victim can be selected after applying these
		      policies, invoke the
		      <code>pri-lost-after-sb</code> handler on the
		      victim node. This handler must be configured in
		      the <code>handlers</code> section and is
		      expected to forcibly remove the node from the
		      cluster.</para>
		  </formalpara>
		</listitem>
		<listitem>
		  <formalpara>
		    <title><code>discard-secondary</code></title>
		    <para>Whichever host is currently in the Secondary
		      role, make that host the split brain
		      victim.</para>
		  </formalpara>
		</listitem>
		</itemizedlist>
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title><code>after-sb-2pri</code></title>
	    <para>Split brain has just been detected, and at this time
	      the resource is in the Primary role on both hosts. This
	      option accepts the same keywords as
	      <code>after-sb-1pri</code> except, of course,
	      <code>discard-secondary</code>.</para>
	  </formalpara>
	</listitem>
	</itemizedlist>
    </para>
    <note>
      <para>DRBD understands additional keywords for these three
	options, which have been omitted here because they are very
	rarely used. Refer to <xref linkend="re-drbdconf"/> for
	details on split brain recovery keywords not discussed
	here.</para>
    </note>
  </section>
</chapter>
