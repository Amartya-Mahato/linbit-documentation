<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-admin">
  <title>Common administrative tasks</title>
  <para>This chapter outlines typical administrative tasks encountered
    during day-to-day operations. It does not cover troubleshooting
    tasks, these are covered in detail in <xref
    linkend="ch-troubleshooting"/>.</para>
  <section id="s-check-status">
    <title>Checking DRBD status</title>
    <section id="s-drbd-overview">
      <title>Retrieving status with <filename>drbd-overview</filename></title>
      <para>The most convenient way to look at DRBD's status is the
        <indexterm><primary>drbd-overview</primary></indexterm><filename>drbd-overview</filename>
	utility. It is available since DRBD-8.0.15 and 8.3.0. In 8.3.0
	it is installed as <filename>drbd-overview.pl</filename>.
	<literallayout><userinput>drbd-overview</userinput>
<computeroutput>  0:home                 Connected Primary/Secondary   UpToDate/UpToDate C r--- /home        xfs  200G 158G 43G  79%
  1:data                 Connected Primary/Secondary   UpToDate/UpToDate C r--- /mnt/ha1     ext3 9.9G 618M 8.8G 7%
  2:nfs-root             Connected Primary/Secondary   UpToDate/UpToDate C r--- /mnt/netboot ext3 79G  57G  19G  76%
</computeroutput></literallayout>
      </para>
    </section>
    <section id="s-proc-drbd">
      <title>Status information in
      <filename>/proc/drbd</filename></title>
      <para><indexterm>
	  <primary>/proc/drbd</primary>
      </indexterm><filename>/proc/drbd</filename> is a virtual file
	displaying real-time status information about all DRBD
	resources currently configured. You may interrogate this
	file's contents using this command:
	<literallayout><userinput>cat /proc/drbd</userinput>
<computeroutput>version: 8.3.0 (api:88/proto:86-89)
GIT-hash: 9ba8b93e24d842f0dd3fb1f9b90e8348ddb95829 build by buildsystem@linbit, 2008-12-18 16:02:26
 0: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r---
    ns:0 nr:8 dw:8 dr:0 al:0 bm:2 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0
 1: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r---
    ns:0 nr:12 dw:12 dr:0 al:0 bm:1 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0
 2: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r---
    ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0
</computeroutput></literallayout>
      </para>
      <para>The first line, prefixed with <code>version:</code>, shows
      the DRBD version used on your system. The second line contains
      information about this specific build.</para>
      <para>The other four lines in this example form a block that is
      repeated for every DRBD device configured, prefixed by the
	device minor number. In this case, this is <code>0</code>,
	corresponding to the device
      <filename>/dev/drbd0</filename>.</para>
      <para>The resource-specific output from
	<filename>/proc/drbd</filename> contains various pieces of
	information about the resource:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>cs</code> (connection state)</title>
	      <indexterm>
		<primary>connection state</primary>
	      </indexterm>
	      <para>Status of the network connection. See <xref
	      linkend="s-connection-states"/> for details about the
	      various connection states.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ro</code> (roles)</title>
	      <indexterm>
		<primary>resource</primary>
		<secondary>role</secondary>
	      </indexterm>
	      <para>Roles of the nodes. The role of the local node is
		displayed first, followed by the role of the partner
		node shown after the slash. See <xref
	      linkend="s-roles"/> for details about the
		possible resource roles.
		<note>
		  <para>Prior to DRBD 8.3,
		    <filename>/proc/drbd</filename> used the
		    <code>st</code> field (referring to the ambiguous
		    term <quote>state</quote>) when referring to
		    resource roles.</para>
		</note>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ds</code> (disk states)</title>
	      <indexterm>
		<primary>disk state</primary>
	      </indexterm>
	      <para>State of the hard disks. Prior to the slash the
		state of the local node is displayed, after the slash
		the state of the hard disk of the partner node is
		shown. See <xref
	      linkend="s-disk-states"/> for details about the various
		disk states.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ns</code> (network send)</title>
	      <para> Volume of net data sent to the partner via the
		network connection; in Kibyte.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>nr</code> (network receive)</title>
	      <para> Volume of net data received by the partner via
		the network connection; in Kibyte.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>dw</code> (disk write)</title>
	      <para>Net data written on local hard disk; in
		Kibyte.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>dr</code> (disk read)</title>
	      <para>Net data read from local hard disk; in Kibyte.
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>al</code> (activity log)</title>
	      <para>Number of updates of the activity log area of the meta
		data.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>bm</code> (bit map)</title>
	      <para> Number of updates of the bitmap area of the meta
		data. </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>lo</code> (local count)</title>
	      <para>Number of open requests to the local I/O sub-system
		issued by DRBD. </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>pe</code> (pending)</title>
	      <para>Number of requests sent to the partner, but that
		have not yet been answered by the latter.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ua</code> (unacknowledged)</title>
	      <para>Number of requests received by the partner via the
		network connection, but that have not yet been
		answered. </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ap</code> (application pending)</title>
	      <para>Number of block I/O requests forwarded to DRBD, but
		not yet answered by DRBD.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ep</code> (epochs)</title>
	      <para>Number of epoch objects. Usually 1. Might increase
		under I/O load when using either the
		<code>barrier</code> or the <code>none</code> write
		ordering method. Since 8.2.7.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>wo</code> (write order)</title>
	      <para>Currently used write ordering method:
		<code>b</code> (barrier), <code>f</code> (flush),
		<code>d</code> (drain) or <code>n</code> (none). Since
		8.2.7.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>oos</code> (out of sync)</title>
	      <para>Amount of storage currently out of sync; in
		Kibibytes. Since 8.2.6.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-connection-states">
      <title>Connection states</title>
      <para><indexterm>
	<primary>connection state</primary>
      </indexterm>
	A resource's connection state can be observed either by
	monitoring <filename>/proc/drbd</filename>, or by issuing the
	<command>drbdadm cstate</command> command:
      <literallayout><userinput>drbdadm cstate <replaceable>resource</replaceable></userinput>
<computeroutput>Connected</computeroutput></literallayout></para>
      <para>A resource may have one of the following connection
	states:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>StandAlone</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>StandAlone</secondary>
	      </indexterm>
	      <para>No network configuration available. The resource
		has not yet been connected, or has been
		administratively disconnected (using <command>drbdadm
		  disconnect</command>), or has dropped its connection
		due to failed authentication or split brain.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Disconnecting</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>Disconnecting</secondary>
	      </indexterm>
	      <para> Temporary state during disconnection. The next
		state is StandAlone.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Unconnected</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>Unconnected</secondary>
	      </indexterm>
	      <para> Temporary state, prior to a connection attempt.
		Possible next states: WFConnection and
		WFReportParams.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Timeout</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>Timeout</secondary>
	      </indexterm>
	      <para>Temporary state following a timeout in the
		communication with the peer. Next state:
		Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>BrokenPipe</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>BrokenPipe</secondary>
	      </indexterm>
	      <para>Temporary state after the connection to the peer
		was lost. Next state: Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>NetworkFailure</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>NetworkFailure</secondary>
	      </indexterm>
	      <para>Temporary state after the connection to the
		partner was lost. Next state: Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>ProtocolError</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>ProtocolError</secondary>
	      </indexterm>
	      <para>Temporary state after the connection to the
		partner was lost. Next state: Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>TearDown</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>TearDown</secondary>
	      </indexterm>
	      <para>Temporary state. The peer is closing the
		connection. Next state: Unconnected.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFConnection</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFConnection</secondary>
	      </indexterm>
	      <para>This node is waiting until the peer node becomes
		visible on the network. </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFReportParams</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFReportParams</secondary>
	      </indexterm>
	      <para>TCP connection has been established, this node
		waits for the first network packet from the
		peer.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Connected</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>Connected</secondary>
	      </indexterm>
	      <para>A DRBD connection has been established, data
		mirroring is now active. This is the normal
		state.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>StartingSyncS</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>StartingSyncS</secondary>
	      </indexterm>
	      <para>Full synchronization, initiated by the
		administrator, is just starting. The next possible
		states are: SyncSource or PausedSyncS.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>StartingSyncT</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>StartingSyncT</secondary>
	      </indexterm>
	      <para>Full synchronization, initiated by the
		administrator, is just starting. Next state:
		WFSyncUUID.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFBitMapS</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFBitMapS</secondary>
	      </indexterm>
	      <para>Partial synchronization is just starting. Next
		possible states: SyncSource or PausedSyncS.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFBitMapT</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFBitMapT</secondary>
	      </indexterm>
	      <para>Partial synchronization is just starting. Next
		possible state: WFSyncUUID.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>WFSyncUUID</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>WFSyncUUID</secondary>
	      </indexterm>
	      <para>Synchronization is about to begin. Next possible
		states: SyncTarget or PausedSyncT.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>SyncSource</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>SyncSource</secondary>
	      </indexterm>
	      <para>Synchronization is currently running, with the
		local node being the source of
		synchronization.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>SyncTarget</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>SyncTarget</secondary>
	      </indexterm>
	      <para>Synchronization is currently running, with the
		local node being the target of
		synchronization.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>PausedSyncS</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>PausedSyncS</secondary>
	      </indexterm>
	      <para>The local node is the source of an ongoing
		synchronization, but synchronization is currently
		paused. This may be due to a dependency on the
		completion of another synchronization process, or
		due to synchronization having been manually
		interrupted by <command>drbdadm
		  pause-sync</command>.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>PausedSyncT</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>PausedSyncT</secondary>
	      </indexterm>
	      <para>The local node is the target of an ongoing
		synchronization, but synchronization is currently
		paused. This may be due to a dependency on the
		completion of another synchronization process, or
		due to synchronization having been manually
		interrupted by <command>drbdadm
		  pause-sync</command>.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>VerifyS</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>VerifyS</secondary>
	      </indexterm>
	      <para>On-line device verification is currently running,
		with the local node being the source of
		verification.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>VerifyT</code></title>
	      <indexterm>
		<primary>connection state</primary>
		<secondary>VerifyT</secondary>
	      </indexterm>
	      <para>On-line device verification is currently running,
		with the local node being the target of
		verification.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-roles">
      <title>Resource roles</title>
      <para><indexterm>
	  <primary>resource</primary>
	  <secondary>role</secondary></indexterm>A resource's role can
	be observed either by monitoring
	<filename>/proc/drbd</filename>, or by issuing the
	<indexterm>
	  <primary>drbdadm</primary>
	  <secondary>role</secondary>
	</indexterm> <command>drbdadm role</command> command:
	<literallayout><userinput>drbdadm role <replaceable>resource</replaceable></userinput>
<computeroutput>Primary/Secondary</computeroutput></literallayout>
	The local resource role is always displayed first, the remote
	resource role last.
	<note>
	  <para>Prior to DRBD 8.3, the <command>drbdadm
	      state</command> command provided the same information.
	    Since <quote>state</quote> is an ambigious term, DRBD uses
	    <quote>role</quote> in its stead from version 8.3.0
	    forward. <command>drbdadm state</command> is also still
	    available, albeit only for compatibility reasons. You
	    should use <command>drbdadm role</command>.</para>
	</note>
      </para>
      <para>You may see one of the following resource roles:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>Primary</code></title>
	      <para>The resource is currently in the primary role, and
	      may be read from and written to. This role only occurs
		on one of the two nodes, unless <link
	      linkend="s-dual-primary-mode">dual-primary node</link>
	      is enabled.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Secondary</code></title>
	      <para>The resource is currently in the secondary
	      role. It normally receives updates from its peer (unless
	      running in disconnected mode), but may neither be read
	      from nor written to. This role may occur on one node or
	      both nodes.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Unknown</code></title>
	      <para>The resource's role is currently unknown. The
	      local resource role never has this status. It is only
	      displayed for the peer's resource role, and only in
	      disconnected mode.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-disk-states">
      <title>Disk states</title>
      <para>A resource's disk state can be observed either by
	monitoring <filename>/proc/drbd</filename>, or by issuing the
	<command>drbdadm dstate</command> command:
      <literallayout><userinput>drbdadm dstate <replaceable>resource</replaceable></userinput>
<computeroutput>UpToDate/UpToDate</computeroutput></literallayout>
      The local disk state is always displayed first, the remote disk
      state last.</para>
      <para>
	Both the local and the remote disk state may be one of the
	following:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>Diskless</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Diskless</secondary>
	      </indexterm>
	      <para>No local block device has been assigned to the
		DRBD driver. This may mean that the resource has never
		attached to its backing device, that it has been
		manually detached using <command>drbdadm
		  detach</command>, or that it automatically detached
		after a lower-level I/O error.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Attaching</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Attaching</secondary>
	      </indexterm>
	      <para>Transient state while reading meta data.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Failed</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Failed</secondary>
	      </indexterm>
	      <para>Transient state following an I/O failure report by
		the local block device. Next state: Diskless.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Negotiating</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Negotiating</secondary>
	      </indexterm>
	      <para>Transient state when an Attach is carried out on
		an already-connected DRBD device.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Inconsistent</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Inconsistent</secondary>
	      </indexterm>
	      <para>The data is inconsistent. This status occurs
		immediately upon creation of a new resource, on both
		nodes (before the initial full sync). Also, this
		status is found in one node (the synchronization
		target) during synchronization.
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Outdated</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Outdated</secondary>
	      </indexterm>
	      <para>Resource data is consistent, but <link
		  linkend="s-outdate">outdated</link>.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>DUnknown</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>DUnknown</secondary>
	      </indexterm>
	      <para>This state is used for the peer disk if no network
		connection is available.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>Consistent</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>Consistent</secondary>
	      </indexterm>
	      <para>Consistent data of a node without connection. When
		the connection is established, it is decided whether
		the data are UpToDate or Outdated.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>UpToDate</code></title>
	      <indexterm>
		<primary>disk state</primary>
		<secondary>UpToDate</secondary>
	      </indexterm>
	      <para>Consistent, up-to-date state of the data. This is
		the normal state.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
  </section>
  <section id="s-enable-disable">
    <title>Enabling and disabling resources</title>
    <section id="s-enable-resource">
      <title>Enabling resources</title>
      <para><indexterm>
	<primary>resource</primary>
	<secondary>enabling</secondary>
      </indexterm>
	Normally, all resources configured in
	<filename>/etc/drbd.conf</filename> are automatically enabled
	upon system startup by the <filename>/etc/init.d/drbd</filename>
	init script. If you choose to disable this startup script (as
	may be required by some applications), you may enable specific
	resources by issuing the commands
      <literallayout><userinput>drbdadm attach <replaceable>resource</replaceable></userinput>
<userinput>drbdadm syncer <replaceable>resource</replaceable></userinput>
<userinput>drbdadm connect <replaceable>resource</replaceable></userinput></literallayout>
      or the shorthand version of the three commands above,
      <literallayout><userinput>drbdadm up <replaceable>resource</replaceable></userinput></literallayout>
	</para>
      <para>As always, you may use the keyword <code>all</code>
	instead of a specific resource name if you want to enable all
	resources configured in <filename>/etc/drbd.conf</filename> at
	once.
      </para>
    </section>
    <section id="s-disable-resource">
      <title>Disabling resources</title>
      <para><indexterm>
	<primary>resource</primary>
	<secondary>disabling</secondary>
      </indexterm>
	You may temporarily disable specific
	resources by issuing the commands
      <literallayout><userinput>drbdadm disconnect <replaceable>resource</replaceable></userinput>
<userinput>drbdadm detach <replaceable>resource</replaceable></userinput></literallayout>
      or the shorthand version of the above,
      <literallayout><userinput>drbdadm down <replaceable>resource</replaceable></userinput></literallayout>
	<note>
	  <para>There is, in fact, a slight syntactical difference
	    between these two methods. While <command>drbdadm
	      down</command> implies a preceding <link
	      linkend="s-switch-resource-roles">resource
	      demotion</link>, <command>drbdadm
	      disconnect/detach</command> does not. So while you can
	    run <command>drbdadm down</command> on a resource that is
	    currently in the primary role, <command>drbdadm
	      disconnect/detach</command> in the same situation will
	    be refused by DRBD's internal state engine.</para>
	</note>
      </para>
      <para>Here, too, you may use the keyword <code>all</code> in
	place of a resource name if you wish to temporarily disable
	all resources listed in <filename>/etc/drbd.conf</filename> at
	once.
      </para>
    </section>
  </section>
  <section id="s-reconfigure">
    <title>Reconfiguring resources</title>
    <para><indexterm>
	<primary>resource</primary>
	<secondary>reconfiguring</secondary>
      </indexterm>
      DRBD allows you to reconfigure resources while they are
      operational. To that end,
      <itemizedlist>
	<listitem>
	  <para>make any necessary changes to the resource
	    configuration in
	    <filename>/etc/drbd.conf</filename>,</para>
	</listitem>
	<listitem>
	  <para>synchronize your <filename>/etc/drbd.conf</filename>
	    file between both nodes,</para>
	</listitem>
	<listitem>
	  <para>issue the <indexterm>
	      <primary>drbdadm</primary>
	      <secondary>adjust</secondary>
	    </indexterm> <command>drbdadm adjust
	      <replaceable>resource</replaceable></command> command on
	    both nodes.</para>
	</listitem>
      </itemizedlist>
    </para>
    <para><command>drbdadm adjust</command> then hands off to
      <command>drbdsetup</command> to make the necessary adjustments
      to the configuration. As always, you are able to review the
      pending <command>drbdsetup</command> invocations by running
      <command>drbdadm</command> with the <option>-d</option>
      (dry-run) option.</para>
    <note>
      <para>When making changes to the <code>common</code> section in
	<filename>/etc/drbd.conf</filename>, you can adjust the
	configuration for all resources in one run, by issuing
	<command>drbdadm adjust all</command>.</para>
    </note>
  </section>
  <section id="s-switch-resource-roles">
    <title>Promoting and demoting resources</title>
    <para><indexterm>
      <primary>resource</primary>
      <secondary>promotion</secondary>
    </indexterm><indexterm>
      <primary>resource</primary>
      <secondary>demotion</secondary>
    </indexterm>Manually switching a <link
	linkend="s-resource-roles">resource's role</link> from
      secondary to primary (promotion) or vice versa (demotion) is
      done using the following commands:
      <literallayout><userinput>drbdadm primary <replaceable>resource</replaceable></userinput>
<userinput>drbdadm secondary <replaceable>resource</replaceable></userinput></literallayout>
    </para>
      <para>
      In <link linkend="s-single-primary-mode">single-primary
	mode</link> (DRBD's default), any resource can be in the
      primary role on only one node at any given time while the <link
	linkend="s-connection-states">connection state</link> is
      <code>Connected</code>. Thus, issuing <command>drbdadm primary
	<replaceable>resource</replaceable></command> on one node
      while <replaceable>resource</replaceable> is still in the
      primary role on the peer will result in an error.</para>
    <para>A resource configured to allow <link
	linkend="s-dual-primary-mode">dual-primary mode</link> can be
	switched to the primary role on both nodes.</para>
  </section>
  <section id="s-enable-dual-primary">
    <title>Enabling dual-primary mode</title>
    <para>This feature is available in DRBD 8.0 and later.</para>
    <para><indexterm>
      <primary>dual-primary mode</primary>
    </indexterm>
      To enable dual-primary mode, add
	the <option>allow-two-primaries</option> option to the
	<code>net</code> section of your resource
	configuration:
	<programlisting>resource <replaceable>resource</replaceable>
  net {
    allow-two-primaries;
  }
  ...
}</programlisting></para>
    <para>
      When a resource is configured to support dual-primary mode, it
      may also be desirable to automatically switch the resource into
      the primary role upon system (or DRBD) startup. To do this, add
      the <option>become-primary-on</option> option, available in DRBD
      8.2.0 and above, to the <code>startup</code> section of your
      resource configuration:
      <programlisting>resource <replaceable>resource</replaceable>
  startup {
    become-primary-on both;
  }
  ...
}</programlisting></para>
    <para>After you have made these changes to
      <filename>/etc/drbd.conf</filename>, do not forget to
      synchronize the configuration between nodes. Then, proceed as
      follows:</para>
    <itemizedlist>
      <listitem>
	<para>Run <userinput>drbdadm disconnect
	    <replaceable>resource</replaceable></userinput> on both nodes.</para>
      </listitem>
      <listitem>
	<para>Execute <userinput>drbdadm connect
	    <replaceable>resource</replaceable></userinput> on both nodes.</para>
      </listitem>
      <listitem>
	<para>Finally, you may now execute <userinput>drbdadm
	    primary <replaceable>resource</replaceable></userinput> on
	  both nodes, instead of on just one.</para>
      </listitem>
    </itemizedlist>
  </section>
  <section id="s-use-online-verify">
    <title>Using on-line device verification</title>
    <para>This feature is available in DRBD 8.2.5 and later.</para>
    <section id="s-online-verify-enable">
      <title>Enabling on-line verification</title>
      <para><indexterm>
	<primary>on-line device verification</primary>
	<secondary>enabling</secondary>
      </indexterm>
	<link linkend="s-online-verify">On-line device
	  verification</link> is not enabled for resources by default.
	To enable it, add the following lines to your resource
	configuration in <filename>/etc/drbd.conf</filename>:
	<programlisting>resource <replaceable>resource</replaceable>
  syncer {
    verify-alg <replaceable>algorithm</replaceable>;
  }
  ...
}</programlisting>
	<replaceable>algorithm</replaceable>
	may be any message digest algorithm supported by the kernel
	crypto API in your system's kernel configuration. Normally,
	you should be able to choose at least from <code>sha1</code>,
	<code>md5</code>, and <code>crc32c</code>.</para>
      <para>If you make this change to an existing resource, as
	always, synchronize your <filename>drbd.conf</filename> to the
	peer, and run <command>drbdadm adjust
	  <replaceable>resource</replaceable></command> on both
	nodes.</para>
    </section>
    <section id="s-online-verify-invoke">
      <title>Invoking on-line verification</title>
       <para><indexterm>
	<primary>on-line device verification</primary>
	<secondary>invoking</secondary>
      </indexterm>After you have enabled on-line verification, you will be
	able to initiate a verification run using the following
	command:
	<literallayout><userinput>drbdadm verify <replaceable>resource</replaceable></userinput></literallayout>
	When you do so, DRBD starts an online verification run for
	<replaceable>resource</replaceable>, and if it detects any
	blocks not in sync, will mark those blocks as such and write a
	message to the kernel log. Any applications using the device
	at that time can continue to do so unimpeded, and you may also
	<link linkend="s-switch-resource-roles">switch resource
	  roles</link> at will.</para>
      <para>If out-of-sync blocks were detected during the
	verification run, you may resynchronize them using the
	following commands after verification has completed:
	<literallayout><userinput>drbdadm disconnect <replaceable>resource</replaceable></userinput>
<userinput>drbdadm connect <replaceable>resource</replaceable></userinput></literallayout>
      </para>
    </section>
    <section id="s-online-verify-automate">
      <title>Automating on-line verification</title>
      <para><indexterm>
	<primary>on-line device verification</primary>
	<secondary>automating</secondary>
      </indexterm>Most users will want to automate on-line device
	verification. This can be easily accomplished. Create a file
	with the following contents, named
	<filename>/etc/cron.d/drbd-verify</filename> on
	<emphasis>one</emphasis> of your nodes:
      <programlisting>42 0 * * 0    root    /sbin/drbdadm verify <replaceable>resource</replaceable></programlisting>
	This will have <code>cron</code> invoke a device verification
      every Sunday at 42 minutes past midnight.</para>
      <para>If you have enabled on-line verification for all your
	resources (for example, by adding <code>verify-alg
	  <replaceable>algorithm</replaceable></code> to the
	  <code>common</code> section in
	<filename>/etc/drbd.conf</filename>), you may also use:
      <programlisting>42 0 * * 0    root    /sbin/drbdadm verify all</programlisting>
      </para>
    </section>
  </section>
  <section id="s-configure-syncer-rate">
    <title>Configuring the rate of synchronization</title>
     <para><indexterm>
      <primary>synchronization</primary>
      <secondary>configuring rate</secondary>
    </indexterm>Normally, one tries to ensure that background
      synchronization (which makes the data on the synchronization
      target temporarily inconsistent) completes as quickly as
      possible. However, it is also necessary to keep background
      synchronization from hogging all bandwidth otherwise
      available for foreground replication, which would be detrimental
      to application performance. Thus, you must configure the
      sychronization bandwidth to match your hardware &mdash; which
      you may do in a permanent fashion or on-the-fly.</para>
    <important>
      <para>It does not make sense to set a synchronization rate
	that is higher than the maximum write throughput on your
	secondary node. You must not expect your secondary node to
	miraculously be able to write faster than its I/O subsystem
	allows, just because it happens to be the target of an ongoing
	device synchronization.</para>
      <para>Likewise, and for the same reasons, it does not make sense
	to set a synchronization rate that is higher than the
	bandwidth available on the replication network.</para>
    </important>
    <section id="s-configure-syncer-rate-permanent">
      <title>Permanent syncer rate configuration</title>
      <para>The maximum bandwidth a resource uses for background
	re-synchronization is permanently configured using the
	<option>rate</option> option for a resource. This must be
	included in the resource configuration's
	<option>syncer</option> section in
	<filename>/etc/drbd.conf</filename>:
	<programlisting>resource <replaceable>resource</replaceable>
  syncer {
    rate 40M;
    ...
  }
  ...
}</programlisting></para>
      <para>Note that the rate setting is given in
	<emphasis>bytes</emphasis>, not <emphasis>bits</emphasis> per
	second. </para>
      <tip>
	<para>A good rule of thumb for this value is to use about 30%
	  of the available replication bandwidth. Thus, if you had an
	  I/O subsystem capable of sustaining write throughput of
	  180MB/s, and a Gigabit Ethernet network capable of
	  sustaining 110 MB/s network throughput (the network being
	  the bottleneck), you would calculate:
	  <equation id="eq-syncer-rate-example1">
	    <title>Syncer rate example, 110MB/s effective available
	      bandwidth</title>
	    <alt>\[110\cdot0.3=33\]</alt>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="syncer-rate-example1"/>
	      </imageobject>
	    </mediaobject>
	  </equation> Thus, the recommended value for the
	  <option>rate</option> option would be
	  <code>33M</code>.</para>
	<para>By contrast, if you had an I/O subsystem with a maximum
	  throughput of 80MB/s and a Gigabit Ethernet connection (the
	  I/O subsystem being the bottleneck), you would calculate:
	  <equation id="eq-syncer-rate-example2">
	    <title>Syncer rate example, 80MB/s effective available
	      bandwidth</title>
	    <alt>\[80\cdot0.3=24\]</alt>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="syncer-rate-example2"/>
	      </imageobject>
	    </mediaobject>
	  </equation> In this case, the recommended value for the
	  <option>rate</option> option would be
	  <code>24M</code>.
	</para>
      </tip>
    </section>
    <section id="s-configure-syncer-rate-temporary">
      <title>Temporary syncer rate configuration</title>
      <para>It is sometimes desirable to temporarily adjust the syncer
	rate. For example, you might want to speed up background
	re-synchronization after having performed scheduled
	maintenance on one of your cluster nodes. Or, you might want
	to throttle background re-synchronization if it happens to
	occur at a time when your application is extremely busy with
	write operations, and you want to make sure that a large
	portion of the existing bandwidth is available to
	replication.</para>
      <para>For example, in order to make most bandwidth of a Gigabit
	Ethernet link available to re-synchronization, issue the
	following command:
	<literallayout><userinput>drbdsetup <filename>/dev/drbd<replaceable>num</replaceable></filename> syncer -r 110M</userinput></literallayout>
	As always, replace <replaceable>num</replaceable> with the
	device minor number of your DRBD device. You need to issue
	this command on only one of your nodes.</para>
      <para>To revert this temporary setting and re-enable the syncer
	rate set in <filename>/etc/drbd.conf</filename>, issue this
	command:
	<literallayout><userinput>drbdadm adjust <replaceable>resource</replaceable></userinput></literallayout>
      </para>
    </section>
  </section>
  <section id="s-configure-checksum-sync">
    <title>Configuring checksum-based synchronization</title>
    <para><indexterm>
	<primary>checksum-based synchronization</primary>
	<secondary>enabling</secondary>
      </indexterm>
      <link linkend="p-checksum-sync">Checksum-based
      synchronization</link> is not enabled for resources by default.
      To enable it, add the following lines to your resource
      configuration in <filename>/etc/drbd.conf</filename>:
      <programlisting>resource <replaceable>resource</replaceable>
  syncer {
    csums-alg <replaceable>algorithm</replaceable>;
  }
  ...
}</programlisting>
      <replaceable>algorithm</replaceable>
      may be any message digest algorithm supported by the kernel
      crypto API in your system's kernel configuration. Normally,
      you should be able to choose at least from <code>sha1</code>,
      <code>md5</code>, and <code>crc32c</code>.</para>
    <para>If you make this change to an existing resource, as
      always, synchronize your <filename>drbd.conf</filename> to the
      peer, and run <command>drbdadm adjust
	<replaceable>resource</replaceable></command> on both
      nodes.</para>
  </section>
  <section id="s-configure-io-error-behavior">
    <title>Configuring I/O error handling strategies</title>
     <para><indexterm>
	<primary>I/O errors</primary>
    </indexterm>
    <indexterm>
	<primary>drbd.conf</primary>
	<secondary>on-io-error</secondary>
    </indexterm>DRBD's <link linkend="s-handling-disk-errors">strategy
	for handling lower-level I/O errors</link> is determined by
      the <option>on-io-error</option> option, included in the
      resource <code>disk</code> configuration in
      <filename>/etc/drbd.conf</filename>:
      <programlisting>resource <replaceable>resource</replaceable> {
  disk {
    on-io-error <replaceable>strategy</replaceable>;
    ...
  }
  ...
}</programlisting>
      You may, of course, set this in the <code>common</code> section
    too, if you want to define a global I/O error handling policy for
    all resources.</para>
    <para><replaceable>strategy</replaceable> may be one of the
    following options:
      <itemizedlist>
	<listitem>
	  <formalpara>
	    <title><option>detach</option></title>
	    <para>This is the recommended option. On the occurrence
	      of a lower-level I/O error, the node drops its backing
	      device, and continues in diskless mode.</para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title><option>pass_on</option></title>
	    <para>This causes DRBD to report the I/O error to the
	      upper layers. On the primary node, it is reported to
	      the mounted file system. On the secondary node, it is
	      ignored (because the secondary has no upper layer to
	      report to). This is the default for historical reasons,
	      but is no longer recommended for most new installations
	      &mdash; except if you have a very compelling reason to
	      use this strategy, instead of
	      <option>detach</option>.</para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title><option>call-local-io-error</option></title>
	    <para>Invokes the command defined as the local I/O error
	      handler. This requires that a corresponding
	      <option>local-io-error</option> command invocation is
	      defined in the resource's <code>handlers</code> section.
	      It is entirely left to the administrator's discretion to
	      implement I/O error handling using the command (or
	      script) invoked by <option>local-io-error</option>.
	      <note>
		<para>Early DRBD versions (prior to 8.0) included
		  another option, <option>panic</option>, which would
		  forcibly remove the node from the cluster by way of
		  a kernel panic, whenever a local I/O error occurred.
		  While that option is no longer available, the same
		  behavior may be mimicked via the
		  <option>local-io-error</option>/
		  <option>call-local-io-error</option> interface. You
		  should do so only if you fully understand the
		  implications of such behavior.</para>
	      </note>
	    </para>
	  </formalpara>
	</listitem>
      </itemizedlist>
    </para>
    <para>You may reconfigure a running resource's I/O error handling
      strategy by following this process:
      <itemizedlist>
	<listitem>
	  <para>Edit the resource configuration in
	    <filename>/etc/drbd.conf</filename>.</para>
	</listitem>
	<listitem>
	  <para>Copy the configuration to the peer node.</para>
	</listitem>
	<listitem>
	  <para>Issue
	    <command>drbdadm adjust <replaceable>resource</replaceable></command>
	    on both nodes.</para>
	</listitem>
      </itemizedlist>
      <note>
	<para>DRBD versions prior to 8.3.1 will incur a full resync
	  after running <code>drbdadm adjust</code> on a node that is
	  in the Primary role. On such systems, the affected resource
	  must be demoted prior to running <code>drbdadm adjust</code>
	  after its <code>disk</code> configuration section has been
	  changed.</para>
      </note>
    </para>
  </section>
  <section id="s-configure-integrity-check">
    <title>Configuring replication traffic integrity checking</title>
    <para><indexterm>
	<primary>replication traffic integrity checking</primary>
	<secondary>enabling</secondary>
      </indexterm>
      <link linkend="s-integrity-check">Replication traffic integrity
      checking</link> is not enabled for resources by default.
      To enable it, add the following lines to your resource
      configuration in <filename>/etc/drbd.conf</filename>:
      <programlisting>resource <replaceable>resource</replaceable>
  net {
    data-integrity-alg <replaceable>algorithm</replaceable>;
  }
  ...
}</programlisting>
      <replaceable>algorithm</replaceable>
      may be any message digest algorithm supported by the kernel
      crypto API in your system's kernel configuration. Normally,
      you should be able to choose at least from <code>sha1</code>,
      <code>md5</code>, and <code>crc32c</code>.</para>
    <para>If you make this change to an existing resource, as
      always, synchronize your <filename>drbd.conf</filename> to the
      peer, and run <command>drbdadm adjust
	<replaceable>resource</replaceable></command> on both
      nodes.</para>
  </section>
  <section id="s-resizing">
    <title>Resizing resources</title>
      <section id="s-growing-online">
	<title>Growing on-line</title>
	<para><indexterm>
	  <primary>resource</primary>
	  <secondary>growing</secondary>
	  <tertiary>on-line</tertiary>
	</indexterm>If the backing block devices can be grown while in
	operation (online), it is also possible to increase the size
	of a DRBD device based on these devices during operation. To
	do so, two criteria must be fulfilled:
	<orderedlist>
	  <listitem>
	    <para>The affected resource's backing device must be one
	      managed by a logical volume management subsystem, such
	      as LVM or EVMS.</para>
	  </listitem>
	  <listitem>
	    <para>The resource must currently be in the
	      <code>Connected</code> connection state.</para>
	  </listitem>
	</orderedlist>
      </para>
	<para>Having grown the backing block devices on both nodes,
	  ensure that only one node is in primary state. Then enter on
	  one node:</para>
	<programlisting>drbdadm resize <replaceable>resource</replaceable></programlisting>
	<para>This triggers a synchronization of the new section.
	  The synchronization is done from the primary node to the
	  secondary node.</para>
      </section>
      <section id="s-growing-offline">
	<title>Growing off-line</title>
	<para><indexterm>
	  <primary>resource</primary>
	  <secondary>growing</secondary>
	  <tertiary>off-line</tertiary>
	</indexterm>When the backing block devices on both nodes are
	grown while DRBD is inactive, and the DRBD resource is using
	<link linkend="s-external-meta-data">external meta
	  data</link>, then the new size is recognized automatically.
	No administrative intervention is necessary. The DRBD device
	will have the new size after the next activation of DRBD on
	both nodes and a successful establishment of a network
	connection.</para>
      <para>If however the DRBD resource is configured to use <link
	  linkend="s-internal-meta-data">internal meta data</link>,
	then this meta data must be moved to the end of the grown
	device before the new size becomes available. To do so,
	complete the following steps:</para>
	<warning>
	  <para>This is an advanced procedure. Use at your own
	  discretion.</para>
	</warning>
	<orderedlist>
	  <listitem>
	    <para>Unconfigure your DRBD resource:</para>
	    <programlisting>drbdadm down <replaceable>resource</replaceable></programlisting>
	  </listitem>
	  <listitem>
	    <para>Save the meta data in a text file prior to
	      shrinking: <programlisting>drbdadm dump-md <replaceable>resource</replaceable> &gt; <filename>/tmp/metadata</filename></programlisting></para>
	    <para>You must do this on both nodes, using a separate
	      dump file for every node. <emphasis>Do not</emphasis>
	      dump the meta data on one node, and simply copy the dump
	      file to the peer. This will not work.</para>
	  </listitem>
	  <listitem>
	    <para>Grow the backing block device on both
	      nodes.</para>
	  </listitem>
	  <listitem>
	    <para>Adjust the size information
	      (<code>la-size-sect</code>) in the file
	      <filename>/tmp/metadata</filename> accordingly, on both
	      nodes. Remember that <code>la-size-sect</code> must be
	      specified in sectors.</para>
	  </listitem>
	  <listitem>
	    <para>Re-initialize
	      the metadata area:</para>
	    <programlisting>drbdadm create-md <replaceable>resource</replaceable></programlisting>
	  </listitem>
	  <listitem>
	    <para>Re-import the corrected meta data, on both
	      nodes:
	    <literallayout><userinput>drbdmeta_cmd=$(drbdadm -d	dump-md test-disk)</userinput>
<userinput>${drbdmeta_cmd/dump-md/restore-md} /tmp/metadata</userinput>
<computeroutput> Valid meta-data in place, overwrite? [need to type 'yes' to confirm]</computeroutput> <userinput>yes</userinput>
<computeroutput> Successfully restored meta data</computeroutput></literallayout>
	    <note>
	      <para>This example uses <code>bash</code> parameter
		substitution. It may or may not work in other shells.
		Check your <envar>SHELL</envar> environment variable
		if you are unsure which shell you are currently
		using.</para>
	    </note></para>
	  </listitem>
	  <listitem>
	    <para>Re-enable your DRBD resource:</para>
	    <programlisting>drbdadm up <replaceable>resource</replaceable></programlisting>
	  </listitem>
	  <listitem>
	    <para>On one node, promote the DRBD resource:</para>
	    <programlisting>drbdadm primary <replaceable>resource</replaceable></programlisting>
	  </listitem>
	  <listitem>
	    <para>Finally, grow the file system so it fills the
	    extended size of the DRBD device.</para>
	  </listitem>
	</orderedlist>
      </section>
      <section id="s-shrinking-online">
	<title>Shrinking on-line</title>
      <para><indexterm>
	  <primary>resource</primary>
	  <secondary>shrinking</secondary>
	  <tertiary>on-line</tertiary>
	</indexterm>Before shrinking a DRBD device, you
	<emphasis>must</emphasis> shrink that the layers above DRBD,
	i.e. usually the file system. Since DRBD cannot ask the file
	system how much space it actually uses, you have to be careful
	in order not to cause data loss. <note>
	  <para>Whether or not the <emphasis>filesystem</emphasis> can
	    be shrunk on-line depends on the filesystem being used.
	    Most filesystems do not support on-line shrinking. XFS
	    does not support shrinking at all.</para>
	</note>
	</para>
	<para>When using internal meta data, make sure to consider
	  the space required by the meta data. The size communicated
	  to <command>drbdadm resize</command> is the net size for the
	  file system. In the case of internal meta data, the gross
	  size required by DRBD is higher (see also <xref
	    linkend="s-meta-data-size"/>).</para>
	<para>To shrink DRBD on-line, issue the following command
	  <emphasis>after</emphasis> you have shrunk the file system
	  residing on top of it:</para>
	<programlisting>drbdadm -- --size=<replaceable>new-size</replaceable> resize <replaceable>resource</replaceable></programlisting>
	<para>You may use the usual multiplier suffixes for
	  <replaceable>new-size</replaceable> (K, M, G etc.). After
	  you have shrunk DRBD, you may also shrink the containing
	  block device (if it supports shrinking).</para>
      </section>
      <section id="s-shrinking-offline">
	<title>Shrinking off-line</title>
	<para><indexterm>
	  <primary>resource</primary>
	  <secondary>shrinking</secondary>
	  <tertiary>off-line</tertiary>
	</indexterm>If you were to shrink a backing block device while DRBD
	  is inactive, DRBD would refuse to attach to this block
	  device during the next attach attempt, since it is now too
	  small (in case external meta data is used), or it would be
	  unable to find its meta data (in case internal meta data are
	  used). To work around these issues, use this procedure (if
	  you cannot use <link linkend="s-shrinking-online">on-line
	    shrinking</link>):</para>
	<warning>
	  <para>This is an advanced procedure. Use at your own
	  discretion.</para>
	</warning>
	<orderedlist>
	  <listitem>
	    <para>Shrink the file system from one node, while DRBD is
	      still configured.</para>
	  </listitem>
	  <listitem>
	    <para>Unconfigure your DRBD resource:</para>
	    <programlisting>drbdadm down <replaceable>resource</replaceable></programlisting>
	  </listitem>
	  <listitem>
	    <para>Save the meta data in a text file prior to
	      shrinking: <programlisting>drbdadm dump-md <replaceable>resource</replaceable> &gt; <filename>/tmp/metadata</filename></programlisting></para>
	    <para>You must do this on both nodes, using a separate
	      dump file for every node. <emphasis>Do not</emphasis>
	      dump the meta data on one node, and simply copy the dump
	      file to the peer. This will not work.</para>
	  </listitem>
	  <listitem>
	    <para>Shrink the backing block device on both
	      nodes.</para>
	  </listitem>
	  <listitem>
	    <para>Adjust the size information
	      (<code>la-size-sect</code>) in the file
	      <filename>/tmp/metadata</filename> accordingly, on both
	      nodes. Remember that <code>la-size-sect</code> must be
	      specified in sectors.</para>
	  </listitem>
	  <listitem>
	    <para><emphasis>Only if you are using internal
		metadata</emphasis> (which at this time have probably
	      been lost due to the shrinking process), re-initialize
	      the metadata area:</para>
	    <programlisting>drbdadm create-md <replaceable>resource</replaceable></programlisting>
	  </listitem>
	  <listitem>
	    <para>Re-import the corrected meta data, on both
	      nodes:
	    <literallayout><userinput>drbdmeta_cmd=$(drbdadm -d	dump-md test-disk)</userinput>
<userinput>${drbdmeta_cmd/dump-md/restore-md} /tmp/metadata</userinput>
  <computeroutput> Valid meta-data in place, overwrite? [need to type 'yes' to confirm]</computeroutput> <userinput>yes</userinput>
  <computeroutput> Successfully restored meta data</computeroutput></literallayout>
	    <note>
	      <para>This example uses <code>bash</code> parameter
		substitution. It may or may not work in other shells.
		Check your <envar>SHELL</envar> environment variable
		if you are unsure which shell you are currently
		using.</para>
	    </note></para>
	  </listitem>
	  <listitem>
	    <para>Re-enable your DRBD resource:</para>
	    <programlisting>drbdadm up <replaceable>resource</replaceable></programlisting>
	  </listitem>
	</orderedlist>
      </section>
    </section>
  <section id="s-disable-flushes">
    <title>Disabling backing device flushes</title>
    <para>
      <caution>
	<para>You should only disable device flushes when running DRBD
	  on devices with a battery-backed write cache (BBWC). Most
	  storage controllers allow to automatically disable the write
	  cache when the battery is depleted, switching to
	  write-through mode when the battery dies. It is strongly
	  recommended to enable such a feature.</para>
	<para>Disabling DRBD's flushes when running without BBWC, or
	  on BBWC with a depleted battery, is <emphasis>likely to
	  cause data loss</emphasis> and should not be attempted.</para>
      </caution>
    </para>
    <para>DRBD allows you to enable and disable <link
	linkend="s-disk-flush-support">backing device flushes</link>
      separately for the replicated data set and DRBD's own meta data.
      Both of these options are enabled by default. If you wish to
      disable either (or both), you would set this in the
      <code>disk</code> section for the DRBD configuration file,
      <filename>/etc/drbd.conf</filename>.</para>
    <para>To disable disk flushes for the replicated data set, include
      the following line in your configuration:
      <programlisting>resource <replaceable>resource</replaceable>
  disk {
    no-disk-flushes;
    ...
  }
  ...
}</programlisting></para>
    <para>To disable disk flushes on DRBD's meta data, include the
      following line:
      <programlisting>resource <replaceable>resource</replaceable>
  disk {
    no-md-flushes;
    ...
  }
  ...
}</programlisting></para>
    <para>After you have modified your resource configuration (and
      synchronized your <filename>/etc/drbd.conf</filename> between
      nodes, of course), you may enable these settings by issuing
      these commands on both nodes:
      <literallayout>drbdadm down <replaceable>resource</replaceable>
drbdadm up <replaceable>resource</replaceable></literallayout></para>
  </section>
  <section id="s-configure-split-brain-behavior">
    <title>Configuring split brain behavior</title>
    <section id="s-split-brain-notification">
      <title>Split brain notification</title>
      <para>DRBD invokes the <code>split-brain</code> handler, if
	configured, at any time split brain is detected. To configure
	this handler, add the following item to your resource
	configuration:
	<programlisting>resource <replaceable>resource</replaceable>
  handlers {
    split-brain <replaceable>handler</replaceable>;
    ...
  }
  ...
}</programlisting>
	<replaceable>handler</replaceable> may be any executable
	present on the system.</para>
      <para>Since DRBD version 8.2.6, the DRBD distribution contains a
	split brain handler script that installs as
	<filename>/usr/lib/drbd/notify-split-brain.sh</filename>. It
	simply sends a notification e-mail message to
	a specified address. To configure the handler to send a message
	to <code>root@localhost</code> (which is expected to be an email
	address that forwards the notification to a real system
	administrator), configure the <code>split-brain handler</code> as
	follows:
	<programlisting>resource <replaceable>resource</replaceable>
  handlers {
    split-brain "/usr/lib/drbd/notify-split-brain.sh root";
    ...
  }
  ...
}</programlisting>
      </para>
      <para>After you have made this modfication on a running resource
	(and synchronized the configuration file between nodes), no
	additional intervention is needed to enable the handler. DRBD will
	simply invoke the newly-configured handler on the next occurrence
	of split brain.</para>
    </section>
    <section id="s-automatic-split-brain-recovery-configuration">
      <title>Automatic split brain recovery policies</title>
      <para>In order to be able to enable and configure DRBD's automatic
	split brain recovery policies, you must understand that DRBD
	offers several configuration options for this purpose. DRBD
	applies its split brain recovery procedures based on the number
	of nodes in the Primary role at the time the split brain is
	detected. To that end, DRBD examines the following keywords, all
	found in the resource's <code>net</code> configuration
	section:
      </para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>after-sb-0pri</code></title>
	      <para>Split brain has just been detected, but at this time
		the resource is not in the Primary role on any host. For
		this option, DRBD understands the following keywords:
		<itemizedlist>
		  <listitem>
		    <formalpara>
		      <title><code>disconnect</code></title>
		      <para>Do not recover automatically, simply invoke
			the <code>split-brain</code> handler script (if
			configured), drop the connection and continue in
			disconnected mode.</para>
		    </formalpara>
		  </listitem>
		  <listitem>
		    <formalpara>
		      <title><code>discard-younger-primary</code></title>
		      <para>Discard and roll back the modifications made
			on the host which assumed the Primary role
			last.</para>
		    </formalpara>
		  </listitem>
		  <listitem>
		    <formalpara>
		      <title><code>discard-least-changes</code></title>
		      <para>Discard and roll back the modifications on
			the host where fewer changes occurred.</para>
		    </formalpara>
		  </listitem>
		  <listitem>
		    <formalpara>
		      <title><code>discard-zero-changes</code></title>
		      <para>If there is any host on which no changes
			occurred at all, simply apply all modifications
			made on the other and continue.</para>
		    </formalpara>
		  </listitem>
		</itemizedlist>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>after-sb-1pri</code></title>
	      <para>Split brain has just been detected, and at this time
		the resource is in the Primary role on one host. For
		this option, DRBD understands the following keywords:
		<itemizedlist>
		  <listitem>
		    <formalpara>
		      <title><code>disconnect</code></title>
		      <para>As with <code>after-sb-0pri</code>, simply
			invoke the <code>split-brain</code> handler
			script (if configured), drop the connection and
			continue in disconnected mode.</para>
		    </formalpara>
		  </listitem>
		  <listitem>
		    <formalpara>
		      <title><code>consensus</code></title>
		      <para>Apply the same recovery policies as
			specified in <code>after-sb-0pri</code>. If a
			split brain victim can be selected after
			applying these policies, automatically resolve.
			Otherwise, behave exactly as if
			<code>disconnect</code> were specified.</para>
		    </formalpara>
		  </listitem>
		  <listitem>
		    <formalpara>
		      <title><code>call-pri-lost-after-sb</code></title>
		      <para>Apply the recovery policies as specified in
			<code>after-sb-0pri</code>. If a split brain
			victim can be selected after applying these
			policies, invoke the
			<code>pri-lost-after-sb</code> handler on the
			victim node. This handler must be configured in
			the <code>handlers</code> section and is
			expected to forcibly remove the node from the
			cluster.</para>
		    </formalpara>
		  </listitem>
		  <listitem>
		    <formalpara>
		      <title><code>discard-secondary</code></title>
		      <para>Whichever host is currently in the Secondary
			role, make that host the split brain
			victim.</para>
		    </formalpara>
		  </listitem>
		</itemizedlist>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><code>after-sb-2pri</code></title>
	      <para>Split brain has just been detected, and at this
	      time the resource is in the Primary role on both hosts.
	      This option accepts the same keywords as
	      <code>after-sb-1pri</code> except
	      <code>discard-secondary</code> and
	      <code>consensus</code>.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
	<note>
	<para>DRBD understands additional keywords for these three
	  options, which have been omitted here because they are very
	  rarely used. Refer to <xref linkend="re-drbdconf"/> for
	  details on split brain recovery keywords not discussed
	  here.</para>
	</note>
      <para>For example, a resource which serves as the block device
      for a GFS or OCFS2 file system in dual-Primary mode may have
      its recovery policy defined as follows:
	<programlisting>resource <replaceable>resource</replaceable> {
  handlers {
    split-brain "/usr/lib/drbd/notify-split-brain.sh root"
    ...
  }
  net {
    after-sb-0pri discard-zero-changes;
    after-sb-1pri discard-secondary;
    after-sb-2pri disconnect;
    ...
  }
  ...
}
</programlisting></para>
    </section>
  </section>
  <section id="s-three-nodes">
    <title>Creating a three-node setup</title>
    <subtitle>Available in DRBD version 8.3.0 and above</subtitle>
    <para>A three-node setup involves one DRBD device
    <quote>stacked</quote> atop another.</para>
    <section id="s-stacking-considerations">
    <title>Device stacking considerations</title>
    <para> The following considerations apply to this type of setup:
      <itemizedlist>
	<listitem>
	  <para>The stacked device is the active one. Assume you have configured
	    one DRBD device <filename>/dev/drbd0</filename>, and the
	    stacked device atop it is <filename>/dev/drbd10</filename>,
	    then <filename>/dev/drbd10</filename> will be the device
	    that you mount and use.</para>
	</listitem>
	<listitem>
	  <para>Device meta data will be stored twice, on the
	      underlying DRBD device <emphasis>and</emphasis> the
	      stacked DRBD device. On the stacked device, you must
	      always use <link linkend="s-internal-meta-data">internal
		meta data</link>. This means that the effectively
	      available storage area on a stacked device is slightly
	      smaller, compared to an unstacked device.</para>
	</listitem>
	<listitem>
	  <para>To get the stacked upper level device running, the
	    underlying device must be in the primary role.</para>
	</listitem>
	<listitem>
	  <para>To be able to synchronize the backup node, the stacked
	    device on the active node must be up and in the primary
	    role.</para>
	</listitem>
      </itemizedlist>
    </para>
  </section>
    <section id="s-three-node-config">
      <title>Configuring a stacked resource</title>
      <para>In the following example, nodes are named
	<code>alice</code>, <code>bob</code>, and
	<code>charlie</code>, with <code>alice</code> and
	<code>bob</code> forming a two-node cluster, and
	<code>charlie</code> being the backup node.</para>
    <programlisting>resource r0 {
  protocol C;

  on alice {
    device     /dev/drbd0;
    disk       /dev/sda6;
    address    10.0.0.1:7788
    meta-disk internal;
  }

  on bob {
    device    /dev/drbd0;
    disk      /dev/sda6;
    address   10.0.0.2:7788
    meta-disk internal;
  }
}

resource r0-U {
  protocol A;

  stacked-on-top-of r0 {
    device     /dev/drbd10;
    address    192.168.42.1:7788;
  }

  on charlie {
    device     /dev/drbd10;
    disk       /dev/hda6;
    address    192.168.42.2:7788; # Public IP of the backup node
    meta-disk  internal;
  }
}</programlisting>
      <para>As with any <filename>drbd.conf</filename> configuration
	file, this must be distributed across all nodes in the cluster
	&mdash; in this case, three nodes. Notice the following extra
	keyword not found in an unstacked resource configuration:
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title><code>stacked-on-top-of</code></title>
	      <para>This option informs DRBD that the resource which
		contains it is a stacked resource. It replaces one of
		the <code>on</code> sections normally found in any
		resource configuration. Do not use
		<option>stacked-on-top-of</option> in an lower-level
		resource.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
      <note>
	<para>It is not a requirement to use <link
	    linkend="fp-protocol-a">Protocol A</link> for stacked
	  resources. You may select any of DRBD's replication
	  protocols depending on your application.</para>
      </note>
    </section>
    <section id="s-three-node-enable">
      <title>Enabling stacked resources</title>
      <para>To enable a stacked resource, you first enable its
	lower-level resource and promote it:</para>
      <literallayout><userinput>drbdadm&nbsp;up&nbsp;r0</userinput>
<userinput>drbdadm&nbsp;primary&nbsp;r0</userinput></literallayout>
      <para>As with unstacked resources, you must create DRBD meta
	data on the stacked resources. This is done using the
	following command:
	<literallayout><userinput>drbdadm&nbsp;--stacked&nbsp;create-md&nbsp;r0-U</userinput></literallayout></para>
      <para>Then, you may enable the stacked resource:</para>
      <literallayout><userinput>drbdadm&nbsp;--stacked&nbsp;up&nbsp;r0-U</userinput>
<userinput>drbdadm&nbsp;--stacked&nbsp;primary&nbsp;r0-U</userinput></literallayout>
      <para>After this, you may bring up the resource on the backup
	node, enabling three-node replication:
	<literallayout><userinput>drbdadm&nbsp;create-md&nbsp;r0-U</userinput>
<userinput>drbdadm&nbsp;up&nbsp;r0-U</userinput></literallayout></para>
      <para>In order to automate stacked resource management, you may
	integrate stacked resources in your cluster manager
	configuration. See <xref linkend="s-pacemaker-stacked-resources"/> for
	information on doing this in a cluster managed by the
	Pacemaker cluster management framework.</para>
    </section>
  </section>

  <section id="s-using-drbd-proxy">
    <title>Using DRBD Proxy</title>
    <section id="s-drbd-proxy-deployment-considerations">
      <title>DRBD Proxy deployment considerations</title>
      <para>The <link linkend="s-drbd-proxy">DRBD Proxy</link>
	processes can either be located directly on the machines where
	DRBD is set up, or they can be placed on distinct dedicated
	servers. A DRBD Proxy instance can server as proxy for
	multiple DRBD devices of multiple nodes.</para>
      <para>DRBD Proxy is completely transparent to
	DRBD. Typically you will expect a high number of data packets
	in flight, therefore the activity log should be reasonably
	large. A big transfer log causes longer re-sync runs after the
	crash of a primary node. Therefore it is recommended to enable
	DRBD's csums-alg setting.</para>
    </section>
    <section id="s-drbd-proxy-configuration">
      <title>Configuration</title>
      <para>DRBD Proxy is configured in DRBD's main configuration
	file. It is configured by an additional options section called
	<code>proxy</code> and additional <code>proxy on</code>
	sections within the host
	sections.</para>
      <para>Below is a DRBD configuration example for proxies running
	directly on the DRBD nodes:</para>
      <programlisting>resource r0 {
        protocol c;
        device     minor 0;
        disk       /dev/sdb1;
        flexible-meta-disk  /dev/sdb2;

	proxy {
	      compression on;
	      memlimit 100M;
	}

        on alice {
                address 127.0.0.1:7789;
                proxy on alice {
                        inside 127.0.0.1:7788;
                        outside 192.168.23.1:7788;
                }
        }

        on bob {
                address 127.0.0.1:7789;
                proxy on bob {
                        inside 127.0.0.1:7788;
                        outside 192.168.23.2:7788;
                }
        }
}</programlisting>
      <para>The <code>inside</code> IP address is used for
	communication between DRBD and the DRBD Proxy, whereas
	the <code>outside</code> IP address is used for communication
	between the proxies.</para>
    </section>
    <section id="s-drbd-proxy-controlling">
      <title>Controlling DRBD Proxy</title>
      <para><code>drbdadm</code> offers the <code>proxy-up</code> and
	<code>proxy-down</code> subcommands to configure or delete the
	connection to the local DRBD Proxy process of the named DRBD
	resource(s). These commands are used by the <code>start</code>
	and <code>stop</code> actions which
	<code>/etc/init.d/drbdproxy</code> implements.</para>
      <para>The DRBD Proxy has a low level configuration tool, called
	<code>drbd-proxy-ctl</code>. When called without any option it
	operates in interactive mode. The available commands are
	displayed by the 'help' command.</para>
      <programlisting>Help for drbd-proxy.
--------------------

add connection &lt;name&gt; &lt;ip-listen1&gt;:&lt;port&gt; &lt;ip-connect1&gt;:&lt;port&gt;
   &lt;ip-listen2&gt;:&lt;port&gt; &lt;ip-connect2&gt;:&lt;port&gt;
   Creates a communication path between two DRBD instances.

set memlimit &lt;name&gt; &lt;memlimit-in-bytes&gt;
   Sets memlimit for connection &lt;name&gt;

del connection &lt;name&gt;
   Deletes communication path named name.

show
   Shows currently configured communication paths.

show memusage
   Shows memory usage of each connection.

list [h]subconnections
   Shows currently established individual connections
   together with some stats. With h outputs bytes in human
   readable format.

list [h]connections
   Shows currently configured connections and their states
   With h outputs bytes in human readable format.

list details
   Shows currently established individual connections with
   counters for each DRBD packet type.

quit
   Exits the client program (closes control connection).

shutdown
   Shuts down the drbd-proxy program. Attention: this
   unconditionally terminates any DRBD connections running.</programlisting>
    </section>
  </section>
</chapter>
