<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-internals" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>DRBD Internals</title>
  <para>This chapter gives <emphasis>some</emphasis> background
    information about some of DRBD's internal algorithms and
    structures. It is intended for interested users wishing to gain a
    certain degree of background knowledge about DRBD. It does not
    dive into DRBD's inner workings deep enough to be a reference for
    DRBD developers. For that purpose, please refer to the papers
    listed in <xref
  linkend="s-publications"/>, and of course to the comments in the
    DRBD source code.</para>
  <section id="s-metadata">
    <title>DRBD meta data</title>
    <para><indexterm>
      <primary>meta data</primary>
    </indexterm>DRBD stores various pieces of information about the data it
      keeps in a dedicated area. This metadata includes:
	<itemizedlist>
	<listitem>
	  <para>the size of the DRBD device,</para>
	</listitem>
	<listitem>
	  <para>the Generation Identifier (<acronym>GI</acronym>,
	    described in detail in <xref
	      linkend="s-gi"/>),</para>
	</listitem>
	<listitem>
	  <para>the Activity Log (<acronym>AL</acronym>, described in
	    detail in <xref
	      linkend="s-activity-log"/>).</para>
	</listitem>
	<listitem>
	  <para>the quick-sync bitmap (described in detail in <xref
		  linkend="s-quick-sync-bitmap"/>),
	  </para>
	</listitem>
      </itemizedlist>
    </para>
    <para>This metadata may be stored <emphasis>internally</emphasis>
      and <emphasis>externally</emphasis>. Which method is used is
      configurable on a per-resource basis.</para>
    <section id="s-internal-meta-data">
      <title>Internal meta data</title>
      <para><indexterm>
	<primary>meta data</primary>
	<secondary>internal</secondary>
      </indexterm>Configuring a resource to use internal meta data means
	that DRBD stores its meta data on the same physical
	lower-level device as the actual production data. It does so
	by setting aside an area at the <emphasis>end</emphasis> of
	the device for the specific purpose of storing metadata.
      </para>
      <formalpara>
	<title>Advantage</title>
	<para>Since the meta data are inextricably linked with the
	  actual data, no special action is required from the
	  administrator in case of a hard disk failure. The meta data
	  are lost together with the actual data and are also restored
	  together.</para>
      </formalpara>
      <formalpara>
	<title>Disadvantage</title>
	<para>In case of the lower-level device being a single
	  physical hard disk (as opposed to a RAID set), internal meta
	  data may negatively affect write throughput. The performance
	  of write requests by the application may trigger an update
	  of the meta data in DRBD. If the meta data are stored on the
	  same magnetic disk of a hard disk, the write operation may
	  result in two additional movements of the write/read head of
	  the hard disk.
	</para>
      </formalpara>
      <caution>
	<para>If you are planning to use internal meta data in
	  conjuntion with an existing lower-level device that already
	  has data which you wish to preserve, you
	  <emphasis>must</emphasis> account for the space required by
	  DRBD's meta data. Otherwise, upon DRBD resource creation,
	  the newly created metadata would overwrite data at the end
	  of the lower-level device, potentially destroying existing
	  files in the process. To avoid that, you must do one of the
	  following things:
	    <itemizedlist>
	    <listitem>
	      <para>Enlarge your lower-level device. This is possible
		with any logical volume management facility (such as
		<indexterm>
		  <primary>LVM</primary>
		</indexterm>LVM or <indexterm>
		  <primary>EVMS</primary>
		</indexterm>EVMS) as long as you have free space
		available in the corresponding volume group or
		container. It may also be supported by hardware
		storage solutions.</para>
	    </listitem>
	    <listitem>
	      <para>Shrink your existing file system on your
		lower-level device. This may or may not be supported
		by your file system.</para>
	    </listitem>
	    <listitem>
	      <para>If neither of the two are possible, use <link
		  linkend="s-external-meta-data">external meta
		  data</link> instead.</para>
	    </listitem>
	    </itemizedlist>
	</para>
	<para>To estimate the amount by which you must enlarge your
	  lower-level device our shrink your file system, see <xref
	    linkend="s-meta-data-size"/>.</para>
      </caution>
    </section>
    <section id="s-external-meta-data">
      <title>External meta data</title>
      <para><indexterm>
	<primary>meta data</primary>
	<secondary>external</secondary>
      </indexterm>External meta data is simply stored on a separate,
	dedicated block device distinct from that which holds your
	production data.</para>
      <formalpara>
	<title>Advantage</title>
	<para>For some write operations, using external meta data
	  produces a somewhat improved latency behavior.</para>
      </formalpara>
      <formalpara>
	<title>Disadvantage</title>
	<para>Meta data are not inextricably linked with the actual
	  production data. This means that manual intervention is
	  required in the case of a hardware failure destroying just
	  the production data (but not DRBD meta data), to effect a
	  full data sync from the surviving node onto the subsequently
	  replaced disk.</para>
      </formalpara>
      <para>Use of external meta data is also the only viable option
	if <emphasis>all</emphasis> of the following apply:
	  <itemizedlist>
	  <listitem>
	    <para>You are using DRBD to duplicate an existing device
	      that already contains data you wish to preserve,
	      <emphasis>and</emphasis></para>
	  </listitem>
	  <listitem>
	    <para>that existing device does not support enlargement,
	      <emphasis>and</emphasis></para>
	  </listitem>
	  <listitem>
	    <para>the existing file system on the device does not
	      support shrinking.</para>
	  </listitem>
	  </itemizedlist>
      </para>
      <para>To estimate the required size of the block device
	dedicated to hold your device meta data, see <xref
	  linkend="s-meta-data-size"/>.</para>
    </section>
    <section id="s-meta-data-size">
      <title>Estimating meta data size</title>
      <para><indexterm>
	<primary>meta data</primary>
	<secondary>size</secondary>
      </indexterm>You may calculate the exact space requirements for DRBD's
	meta data using the following formula:</para>
      <equation id="eq-metadata-size-exact">
	<title>Calculating DRBD meta data size (exactly)</title>
	<alt>\[M_s=\left\lceil \frac{C_s}{2^{18}}\right\rceil \cdot8+72\]</alt>
	<graphic fileref="metadata-size-exact"/>
      </equation>
      <para><replaceable>C<subscript>s</subscript></replaceable> is
	the data device size in sectors.<note>
	  <para>You may retrieve the device size by issuing
	    <command>blockdev --getsz
	      <replaceable>device</replaceable></command>. However,
	    <command>echo $(( $(blockdev --getsize64
	      <replaceable>device</replaceable>) / 512))</command> is
	    more reliable, since it also works for block devices
	    larger than 2 TB.
	  </para>
	</note>The result,
	<replaceable>M<subscript>s</subscript></replaceable>, is also
	expressed in sectors. To convert to MB, divide by
	2048 (on all Linux platforms expect s390).</para>
      <para>In practice, you may use a reasonably good approximation,
	given below. Note that in this formula, the unit is megabytes,
	not sectors:</para>
      <equation id="eq-metadata-size-approx">
	<title>Estimating DRBD meta data size (approximately)</title>
	<alt>\[M_{MB}\approx\frac{C_{MB}}{32768}+1\]</alt>
	<graphic fileref="metadata-size-approx"/>
      </equation>
    </section>
  </section>
  <section id="s-gi">
    <title>Generation Identifiers</title>
    <para><indexterm>
	<primary>generation identifiers</primary>
      </indexterm>DRBD uses <emphasis>generation
	identifiers</emphasis> (<acronym>GI's</acronym>) to identify
      <quote>generations</quote> of replicated data. This is DRBD's
      internal mechanism used for
      <itemizedlist>
	<listitem>
	  <para>determining whether the two nodes are in fact members
	    of the same cluster (as opposed to two nodes that were
	    connected accidentally),</para>
	</listitem>
	<listitem>
	  <para>determining the direction of background
	    re-synchronization (if necessary),</para>
	</listitem>
	<listitem>
	  <para>determining whether full re-synchronization is
	    necessary or whether partial re-synchronization is
	    sufficient,</para>
	</listitem>
	<listitem>
	  <para><indexterm>
	      <primary>split brain</primary>
	      <secondary>detection</secondary>
	    </indexterm>identifying split brain.</para>
	</listitem>
      </itemizedlist>
    </para>
    <section id="s-data-generations">
      <title>Data generations</title>
      <para>DRBD marks the start of a new <emphasis>data
	  generation</emphasis> at each of the following occurrences:
      <itemizedlist>
	  <listitem>
	    <para>The initial device full sync,</para>
	  </listitem>
	  <listitem>
	    <para>a disconnected resource switching to the primary
	      role,</para>
	  </listitem>
	  <listitem>
	    <para>a resource in the primary role disconnecting.</para>
	  </listitem>
      </itemizedlist> Thus, we can summarize that whenever a resource
	is in the <code>Connected</code> connection state, and both
	nodes' disk state is <code>UpToDate</code>, the current data
	generation on both nodes is the same. The inverse is also
	true.</para>
      <para>
	Every new data generation is identified by a 8-byte,
	universally unique identifier (<acronym>UUID</acronym>).
      </para>
    </section>
    <section id="s-gi-tuple">
      <title>The generation identifier tuple</title>
      <para>DRBD keeps four pieces of information about current and
	historical data generations in the local resource meta data:
      <itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Current UUID</title>
	      <para>This is the generation identifier for the current
		data generation, as seen from the local node's
		perspective. When a resource is <code>Connected</code>
		and fully synchronized, the current UUID is identical
		between nodes.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><quote>Bitmap</quote> UUID</title>
	      <para>This is the UUID of the generation against which
		the on-disk sync bitmap is tracking changes. As the
		on-disk sync bitmap itself, this identifier is only
		relevant while in disconnected mode. If the resource
		is <code>Connected</code>, this UUID is always empty
		(zero).</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Two <quote>Historical</quote> UUID's</title>
	      <para>These are the identifiers of the two data
		generations preceding the current one.</para>
	    </formalpara>
	  </listitem>
      </itemizedlist>
      </para>
      <para>Collectively, these four items are referred to as the
	<emphasis>generation identifier tuple</emphasis>, or <quote>GI
	  tuple</quote> for short.</para>
    </section>
    <section id="s-gi-changes">
      <title>How generation identifiers change</title>
      <section id="s-gi-changes-newgen">
	<title>Start of a new data generation</title>
	<para>When a node loses connection to its peer (either by
	  network failure or manual intervention), DRBD modifies its
	  local generation identifiers in the following manner:
	  <figure id="f-gi-changes-newgen">
	    <title>GI tuple changes at start of a new data
	      generation</title>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="gi-changes-newgen"/>
	      </imageobject>
	      <caption>Note: only changes on primary node shown (on a
	      secondary node, no changes apply).</caption>
	    </mediaobject>
	  </figure>
	  <orderedlist>
	    <listitem>
	      <para>A new UUID is created for the new data generation.
		This becomes the new current UUID for the primary
		node.</para>
	    </listitem>
	    <listitem>
	      <para>The previous UUID now refers to the generation the
		bitmap is tracking changes against, so it becomes the
		new bitmap UUID for the primary node.</para>
	    </listitem>
	    <listitem>
	      <para>On the secondary node, the GI tuple remains
		unchanged.</para>
	    </listitem>
	  </orderedlist>
	</para>
      </section>
      <section id="s-gi-changes-syncstart">
	<title>Start of re-sychronization</title>
	<para>Upon the initiation of re-synchronization, DRBD performs
	  these modifications on the local generation identifiers:
	  <figure id="f-gi-changes-syncstart">
	    <title>GI tuple changes at start of
	      re-synchronization</title>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="gi-changes-syncstart"/>
	      </imageobject>
	      <caption>Note: only changes on synchronization source
	      shown.</caption>
	    </mediaobject>
	  </figure>
	  <orderedlist>
	    <listitem>
	      <para>The current UUID on the synchronization source
		remains unchanged.</para>
	    </listitem>
	    <listitem>
	      <para>The bitmap UUID on the synchronization source is
		rotated out to the first historical UUID.</para>
	    </listitem>
	    <listitem>
	      <para>A new bitmap UUID is generated on the
		synchronization source.</para>
	    </listitem>
	    <listitem>
	      <para>This UUID becomes the new current UUID on the
		synchronization target.</para>
	    </listitem>
	    <listitem>
	      <para>The bitmap and historical UUID's on the
		synchronization target remain unchanged.</para>
	    </listitem>
	  </orderedlist>
	</para>
      </section>
      <section id="s-gi-changes-synccomplete">
	<title>Completion of re-synchronization</title>
	<para>When re-synchronization concludes, the following changes
	  are performed:
	<figure id="f-gi-changes-synccomplete">
	    <title>GI tuple changes at completion of
	      re-synchronization</title>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="gi-changes-synccomplete"/>
	      </imageobject>
	      <caption>Note: only changes on synchronization source
	      shown.</caption>
	    </mediaobject>
	  </figure>
	  <orderedlist>
	    <listitem>
	      <para>The current UUID on the synchronization source
		remains unchanged.</para>
	    </listitem>
	    <listitem>
	      <para>The bitmap UUID on the synchronization source is
		rotated out to the first historical UUID, with that
		UUID moving to the second historical entry (any
		existing second historical entry is discarded).</para>
	    </listitem>
	    <listitem>
	      <para>The bitmap UUID on the synchronization source is
		then emptied (zeroed).</para>
	    </listitem>
	    <listitem>
	      <para>The synchronization target adopts the entire GI
		tuple from the synchronization source.</para>
	    </listitem>
	  </orderedlist>
	</para>
      </section>
    </section>
    <section id="s-gi-use">
      <title>How DRBD uses generation identifiers</title>
      <para>When a connection between nodes is established, the two
	nodes exchange their currently available generation
	identifiers, and proceed accordingly. A number of possible
	outcomes exist:
      <orderedlist>
	  <listitem>
	    <formalpara>
	      <title>Current UUID's empty on both nodes</title>
	      <para>The local node detects that both its current UUID
		and the peer's current UUID are empty. This is the
		normal occurrence for a freshly configured resource
		that has not had the initial full sync initiated. No
		synchronization takes place; it has to be started
		manually.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Current UUID's empty on one node</title>
	      <para>The local node detects that the peer's current
		UUID is empty, and its own is not. This is the normal
		case for a freshly configured resource on which the
		initial full sync has just been initiated, the local
		node having been selected as the initial
		synchronization source. DRBD now sets all bits in the
		on-disk sync bitmap (meaning it considers the entire
		device out-of-sync), and starts synchronizing as a
		synchronization source.</para>
	    </formalpara>
	    <para>If the opposite case (local current UUID empty,
	      peer's non-empty), DRBD performs the same steps, except
	      that the local node becomes the synchronization
	      target.</para>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Equal current UUID's</title>
	      <para>The local node detects that its current UUID and
		the peer's current UUID are non-empty and equal. This
		is the normal occurrence for a resource that went into
		disconnected mode at a time when it was in the
		secondary role, and was not promoted on either node
		while disconnected. No synchronization takes place, as
		none is necessary.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Bitmap UUID matches peer's current UUID</title>
	      <para>The local node detects that its bitmap UUID
		matches the peer's current UUID, and that the peer's
		bitmap UUID is empty. This is the normal and expected
		occurrence after a secondary node failure, with the
		local node being in the primary role. It means that
		the peer never became primary in the meantime and
		worked on the basis of the same data generation all
		along. DRBD now initiates a normal, background
		re-synchronization, with the local node becoming the
		synchronization source.</para>
	    </formalpara>
	    <para>If, conversely, the local node detects that
	      <emphasis>its</emphasis> bitmap UUID is empty, and that
	      the <emphasis>peer's</emphasis> bitmap matches the local
	      node's current UUID, then that is the normal and
	      expected occurrence after a failure of the local node.
	      Again, DRBD now initiates a normal, background
	      re-synchronization, with the local node becoming the
	      synchronization target.</para>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Current UUID matches peer's historical
		UUID</title>
	      <para>The local node detects that its current UUID
		matches one of the peer's historical UUID's. This
		implies that while the two data sets share a common
		ancestor, and the local node has the up-to-date data,
		the information kept in the local node's bitmap is
		outdated and not useable. Thus, a normal
		synchronization would be insufficient. DRBD now marks
		the entire device as out-of-sync and initiates a full
		background re-synchronization, with the local node
		becoming the synchronization source.
	      </para>
	    </formalpara>
	    <para>In the opposite case (one of the local node's
	      historical UUID matches the peer's current UUID), DRBD
	      performs the same steps, except that the local node
	      becomes the synchronization target.
	    </para>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Bitmap UUID's match, current UUID's do not</title>
	      <para>
		<indexterm>
		  <primary>split brain</primary>
		  <secondary>detection</secondary>
		</indexterm>
		The local node detects that its current UUID differs
	      from the peer's current UUID, and that the bitmap UUID's
	      match. This is split brain, but one where the data
	      generations have the same parent. This means that DRBD
	      invokes split brain auto-recovery strategies, if
	      configured. Otherwise, DRBD disconnects and waits for
	      manual split brain resolution.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Neither current nor bitmap UUID's match</title>
	      <para>The local node detects that its current UUID
		differs from the peer's current UUID, and that the
		bitmap UUID's <emphasis>do not</emphasis> match. This
		is split brain with unrelated ancestor generations,
		thus auto-recovery strategies, even if configured, are
		moot. DRBD disconnects and waits for manual split
		brain resolution.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>No UUID's match</title>
	      <para>Finally, in case DRBD fails to detect even a
		single matching element in the two nodes' GI tuples,
		it logs a warning about unrelated data and
		disconnects. This is DRBD's safeguard against
		accidental connection of two cluster nodes that have
		never heard of each other before.</para>
	    </formalpara>
	  </listitem>
	</orderedlist>
      </para>
    </section>
  </section>
  <section id="s-activity-log">
    <title>The Activity Log</title>
    <section id="s-al-purpose">
      <title>Purpose</title>
      <para><indexterm>
	<primary>Activity Log</primary>
	<secondary>purpose</secondary>
      </indexterm>
	During a write operation DRBD forwards the write operation
	to the local backing block device, but also sends the data
	block over the network. These two actions occur, for all
	practical purposes, simultaneously. Random timing behavior may
	cause a situation where the write operation has been
	completed, but the transmission via the network has not yet
	taken place. </para>
      <para>If, at this moment, the active node fails and fail-over is
	being initiated, the data block that did not make it to the
	peer prior to the crash has to be removed from the data set of
	the node during subsequent synchronisation. Otherwise, the
	crashed node would be "one write ahead" of the surviving node,
	which would violate the <phrase>all or nothing</phrase>
	principle of replicated storage. This is an issue that is not
	limited to DRBD, in fact, this issue exists in practically all
	replicated storage configurations. Many other storage
	solutions (just as DRBD itself, prior to version 0.7) thus
	require that after a failure of the active, that node must be
	fully synchronized anew after its recovery.</para>
      <para>DRBD's approach, since version 0.7, is a different one.
	The <emphasis>activity log</emphasis> (AL), stored in the meta
	data area, keeps track of those blocks that have
	<quote>recently</quote> been written to. Colloquially, these
	areas are referred to as <phrase>hot extents</phrase>.</para>
      <para>If a temporarily failed node that was in active mode at
	the time of failure is synchronized, only those hot extents
	highlighted in the AL need to be synchronized, rather than the
	full device. This drastically reduces synchronization time
	after an active node crash.</para>
    </section>
    <section id="s-active-extents">
      <title>Active extents</title>
      <para><indexterm>
	  <primary>Activity Log</primary>
	  <secondary>active extents</secondary>
      </indexterm>The activity log has a configurable parameter, the
	number of active extents. Every active extent adds 4MiB to the
	amount of data being retransmitted after a Primary crash. This
	parameter must be understood as a compromise between the
	following opposites:
      <itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Many active extents</title>
	      <para>Keeping a large activity log improves write
		throughput. Every time a new extent is activated, an
		old extent is reset to inactive. This transition
		requires a write operation to the meta data area. If
		the number of active extents is high, old active
		extents are swapped out fairly rarely, reducing meta
		data write operations and thereby improving
		performance.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Few active extents</title>
	      <para>Keeping a small activity log reduces
		synchronization time after active node failure and
		subsequent recovery.</para>
	    </formalpara>
	  </listitem>
      </itemizedlist></para>
    </section>
    <section id="s-suitable-al-size">
      <title>Selecting a suitable Activity Log size</title>
      <para><indexterm>
	  <primary>Activity Log</primary>
	  <secondary>size</secondary>
      </indexterm>The definition of the number of extents should be
	based on the desired synchronisation time at a given
	synchronization rate. The number of active extents can be
	calculated as follows:</para>
      <equation id="eq-al-extents">
	<title>Active extents calculation based on sync rate and
	  target sync time</title>
	<alt>\[E=\frac{R\cdot t_{sync}}4\]</alt>
	<graphic fileref="al-extents"/>
      </equation>
      <para><replaceable>R</replaceable> is the synchronization
	rate, given in MB/s.
	<replaceable>t<subscript>sync</subscript></replaceable> is the
	target synchronization time, in seconds.
	<replaceable>E</replaceable> is the resulting number of active
	extents.</para>
      <para>To provide an example, suppose our cluster has an I/O
	subsystem with a throughput rate of 90 MiByte/s that was
	configured to a synchronization rate of 30 MiByte/s
	(<replaceable>R</replaceable>=30), and we want to keep our
	target synchronization time at 4 minutes or 240 seconds
	(<replaceable>t<subscript>sync</subscript></replaceable>=240):</para>
      <equation id="eq-al-extents-example">
	<title>Active extents calculation based on sync rate and
	  target sync time (example)</title>
	<alt>\[E=\frac{30\cdot240}{4}=1800\approx1801\]</alt>
	<graphic fileref="al-extents-example"/>
      </equation>
      <para>The exact result is 1800, but since DRBD's hash function
	for the implementation of the AL works best if the number of
	extents is set to a prime number, we select 1801.</para>
    </section>
  </section>
  <section id="s-quick-sync-bitmap">
    <title>The quick-sync bitmap</title>
    <para><indexterm>
	<primary>quick-sync bitmap</primary>
    </indexterm><indexterm>
	<primary>bitmap (DRBD-specific concept)</primary>
	<see>quick-sync bitmap</see>
    </indexterm>The quick-sync bitmap is the internal data structure
      which DRBD uses, on a per-resource basis, to keep track of
      blocks being in sync (identical on both nodes) or out-of sync.
      It is only relevant when a resource is in disconnected
      mode.</para>
    <para>In the quick-sync bitmap, one bit represents a 4-KiB chunk
      of on-disk data. If the bit is cleared, it means that the
      corresponding block is still in sync with the peer node. That
      implies that the block has not been written to since the time of
      disconnection. Conversely, if the bit is set, it means that the
      block has been modified and needs to be re-synchronized whenever
      the connection becomes available again.</para>
    <para>As DRBD detects write I/O on a disconnected device, and
      hence starts setting bits in the quick-sync bitmap, it does so
      in RAM &mdash; thus avoiding expensive synchronous metadata I/O
      operations. Only when the corresponding blocks turn
      <phrase>cold</phrase> (that is, expire from the <link
	linkend="s-activity-log">Activity Log</link>), DRBD makes the
      appropriate modifications in an on-disk representation of the
      quick-sync bitmap. Likewise, if the resource happens to be
      manually shut down on the remaining node while disconnected,
      DRBD flushes the <emphasis>complete</emphasis> quick-sync bitmap
      out to persistent storage.</para>
    <para>When the peer node recovers or the connection is
      re-established, DRBD combines the bitmap information from both
      nodes to determine the <emphasis>total data set</emphasis> that
      it must re-synchronize. Simultaneously, DRBD <link
	linkend="s-gi-use">examines the generation identifiers</link>
      to determine the <emphasis>direction</emphasis> of
      synchronization.</para>
    <para>The node acting as the synchronization source then transmits
      the agreed-upon blocks to the peer node, clearing sync bits in
      the bitmap as the synchronization target acknowledges the
      modifications. If the re-synchronization is now interrupted (by
      another network outage, for example) and subsequently resumed it
      will continue where it left off &mdash; with any additional
      blocks modified in the meantime being added to the
      re-synchronization data set, of course.
      <note>
	<para>Re-synchronization may be also be paused and resumed
	  manually with the <command>drbdadm pause-sync</command>
	  and <command>drbdadm resume-sync</command> commands.
	  You should, however, not do so light-heartedly &mdash;
	  interrupting re-synchronization leaves your secondary node's
	  disk <code>Inconsistent</code> longer than necessary.</para>
      </note>
    </para>
  </section>
  <section id="s-outdate-peer">
    <title>The peer outdater interface</title>
    <para>DRBD has a defined interface for the mechanism that outdates
      the peer node in case of the replication link being
      interrupted. The <filename>drbd-peer-outdater</filename> helper,
    bundled with Heartbeat, is the reference implementation for this
    interface. However, you may easily implement your own peer
    outdater helper program.</para>
    <para>The outdate helper is invoked only in case
      <orderedlist>
	<listitem>
	  <para>an <option>outdate-peer</option> handler has been
	    defined in the resource's (or common)
	    <code>handlers</code> section,
	    <emphasis>and</emphasis></para>
	</listitem>
	<listitem>
	  <para>the <option>fencing</option> option for the resource
	    is set to either <code>resource-only</code> or
	    <code>resource-and-stonith</code>, <emphasis>and</emphasis></para>
	</listitem>
	<listitem>
	  <para>the replication link is interrupted long enough for
	  DRBD to detect a network failure.</para>
	</listitem>
      </orderedlist>
    </para>
    <para>The program or script specified as the
      <code>outdate-peer</code> handler, when it is invoked, has the
      <code>DRBD_RESOURCE</code> and <code>DRBD_PEER</code>
      environment variables available. They contain the name of the
      affected DRBD resource and the peer's hostname,
      respectively.</para>
    <para>Any peer outdater helper program (or script) must return one
    of the following exit codes:
      <table>
	<title>Peer outdater exit codes</title>
	<tgroup cols="2">
	  <colspec colnum="1" colname="exit-code" colwidth="10%"/>
	  <colspec colnum="2" colname="implication" colwidth="90%"/>
	  <thead>
	    <row>
	      <entry>Exit code</entry>
	      <entry>Implication</entry>
	    </row>
	  </thead>
	  <tbody>
	    <row>
	      <entry>3</entry>
	      <entry>Peer's disk state was already <code>Inconsistent</code>.</entry>
	    </row>
	    <row>
	      <entry>4</entry>
	      <entry>Peer's disk state was successfully set to
		<code>Outdated</code> (or was <code>Outdated</code> to
	      begin with).</entry>
	    </row>
	    <row>
	      <entry>5</entry>
	      <entry>Connection to the peer node failed, peer could not
	      be reached.</entry>
	    </row>
	    <row>
	      <entry>6</entry>
	      <entry>Peer refused to be outdated because the affected
	      resource was in the primary role.</entry>
	    </row>
	    <row>
	      <entry>7</entry>
	      <entry>Peer node was successfully fenced off the
		cluster. This should never occur unless
		<option>fencing</option> is set to
		<code>resource-and-stonith</code> for the affected
		resource.
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </table>
    </para>
  </section>
</chapter>
