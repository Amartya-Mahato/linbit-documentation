<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-internals" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>DRBD Internals</title>
  <section id="s-metadata">
    <title>DRBD meta data</title>
    <indexterm>
      <primary>meta data</primary>
    </indexterm>
    <para>DRBD stores various pieces of information about the data it
      keeps in a dedicated area. This metadata includes:
	<itemizedlist>
	<listitem>
	  <para>the size of the DRBD device,</para>
	</listitem>
	<listitem>
	  <para>the Generation Identifier (<acronym>GI</acronym>,
	    described in detail in <xref
	      linkend="s-gi"/>),</para>
	</listitem>
	<listitem>
	  <para>the Activity Log (<acronym>AL</acronym>, described in
	    detail in <xref
	      linkend="s-activity-log"/>).</para>
	</listitem>
	<listitem>
	  <para>the quick-sync bitmap (described in detail in <xref
		  linkend="s-quick-sync-bitmap"/>),
	  </para>
	</listitem>
      </itemizedlist>
    </para>
    <para>This metadata may be stored <emphasis>internally</emphasis>
      and <emphasis>externally</emphasis>. Which method is used is
      configurable on a per-resource basis.</para>
    <section id="s-internal-meta-data">
      <title>Internal meta data</title>
      <indexterm>
	<primary>meta data</primary>
	<secondary>internal</secondary>
      </indexterm>
      <para>Configuring a resource to use internal meta data means
	that DRBD stores its meta data on the same physical
	lower-level device as the actual production data. It does so
	by setting aside an area at the <emphasis>end</emphasis> of
	the device for the specific purpose of storing metadata.
      </para>
      <formalpara>
	<title>Advantage</title>
	<para>Since the meta data are inextricably linked with the
	  actual data, no special action is required from the
	  administrator in case of a hard disk failure. The meta data
	  are lost together with the actual data and are also restored
	  together.</para>
      </formalpara>
      <formalpara>
	<title>Disadvantage</title>
	<para>In case of the lower-level device being a single
	  physical hard disk (as opposed to a RAID set), internal meta
	  data may negatively affect write throughput. The performance
	  of write requests by the application may trigger an update
	  of the meta data in DRBD. If the meta data are stored on the
	  same magnetic disk of a hard disk, the write operation may
	  result in two additional movements of the write/read head of
	  the hard disk.
	</para>
      </formalpara>
      <caution>
	<para>If you are planning to use internal meta data in
	  conjuntion with an existing lower-level device that already
	  has data which you wish to preserve, you
	  <emphasis>must</emphasis> account for the space required by
	  DRBD's meta data. Otherwise, upon DRBD resource creation,
	  the newly created metadata would overwrite data at the end
	  of the lower-level device, potentially destroying existing
	  files in the process. To avoid that, you must do one of the
	  following things:
	    <itemizedlist>
	    <listitem>
	      <para>Enlarge your lower-level device. This is possible
		with any logical volume management facility (such as
		LVM or EVMS) as long as you have free space available
		in the corresponding volume group or container. It may
		also be supported by hardware storage
		solutions.</para>
	    </listitem>
	    <listitem>
	      <para>Shrink your existing file system on your
		lower-level device. This may or may not be supported
		By your file system.<footnote>
		  <para>The ext3 file system, for example, supports
		    offline file system shrinking. So does ReiserFS.
		    XFS does not.</para>
		  </footnote>
	      </para>
	    </listitem>
	    <listitem>
	      <para>If neither of the two are possible, use <link
		  linkend="s-external-meta-data">external meta
		  data</link> instead.</para>
	    </listitem>
	    </itemizedlist>
	</para>
	<para>To estimate the amount by which you must enlarge your
	  lower-level device our shrink your file system, see <xref
	    linkend="s-meta-data-size"/>.</para>
      </caution>
    </section>
    <section id="s-external-meta-data">
      <title>External meta data</title>
      <indexterm>
	<primary>meta data</primary>
	<secondary>external</secondary>
      </indexterm>
      <para>External meta data are simply stored on a separate,
	dedicated block device distinct from that which holds your
	production data.</para>
      <formalpara>
	<title>Advantage</title>
	<para>For some write operations, using external meta data
	  produces a somewhat improved latency behavior.</para>
      </formalpara>
      <formalpara>
	<title>Disadvantage</title>
	<para>Meta data are not inextricably linked with the actual
	  production data. This means that manual intervention is
	  required in the case of a hardware failure destroying just
	  the production data (but not DRBD meta data), to effect a
	  full data sync from the surviving node onto the subsequently
	  replaced disk.</para>
      </formalpara>
      <para>Use of external meta data is also the only viable option
	if <emphasis>all</emphasis> of the following apply:
	  <itemizedlist>
	  <listitem>
	    <para>You are using DRBD to duplicate an existing device
	      that already contains data you wish to preserve,
	      <emphasis>and</emphasis></para>
	  </listitem>
	  <listitem>
	    <para>that existing device does not support enlargement,
	      <emphasis>and</emphasis></para>
	  </listitem>
	  <listitem>
	    <para>the existing file system on the device does not
	      support shrinking.</para>
	  </listitem>
	  </itemizedlist>
      </para>
      <para>To estimate the required size of the block device
	dedicated to hold your device meta data, see <xref
	  linkend="s-meta-data-size"/>.</para>
    </section>
    <section id="s-meta-data-size">
      <title>Estimating meta data size</title>
      <indexterm>
	<primary>meta data</primary>
	<secondary>size</secondary>
      </indexterm>
      <para>You may calculate the exact space requirements for DRBD's
	meta data using the following formula:</para>
      <equation id="eq-metadata-size-exact">
	<title>Calculating DRBD meta data size (exactly)</title>
	<graphic fileref="metadata-size-exact"/>
      </equation>
      <para><replaceable>C<subscript>s</subscript></replaceable> is
	the data device size in sectors.<footnote>
	  <para>You may retrieve the device size by issuing
	    <command>blockdev --getsz
	      <replaceable>device</replaceable></command>. However,
	    <command>echo $(( $(blockdev --getsize64
	      <replaceable>device</replaceable>) / 512))</command> is
	    more reliable, since it also works for block devices
	    larger than 2 TB.
	  </para>
	</footnote>The result,
	<replaceable>M<subscript>s</subscript></replaceable>, is also
	expressed in sectors. To convert to MB, divide by
	2048.<footnote>
	  <para>The hard sector size on Linux (most architectures) is
	    512 bytes or .5 KB. The only known deviation from this
	    applies to the s390 architecture.</para>
	  </footnote>
      </para>
      <para>In practice, you may use a reasonably good approximation,
	given below. Note that in this formula, the unit is megabytes,
	not sectors:</para>
      <equation id="eq-metadata-size-approx">
	<title>Estimating DRBD meta data size (approximately)</title>
	<graphic fileref="metadata-size-approx"/>
      </equation>
    </section>
  </section>
  <section id="s-gi">
    <title>Generation Identifiers</title>
    <para>DRBD uses <emphasis>generation identifiers</emphasis>
      (<acronym>GI's</acronym>) to identify <quote>generations</quote>
      of replicated data. This is DRBD's internal mechanism used for
      <itemizedlist>
	<listitem>
	  <para>determining whether the two nodes are in fact members
	    of the same cluster (as opposed to two nodes that were
	    connected accidentally),</para>
	</listitem>
	<listitem>
	  <para>determining the direction of background
	    re-synchronization (if necessary),</para>
	</listitem>
	<listitem>
	  <para>determining whether full re-synchronization is
	    necessary or whether partial re-synchronization is
	    sufficient,</para>
	</listitem>
	<listitem>
	  <para>identifying split brain.</para>
	</listitem>
      </itemizedlist>
    </para>
    <section id="s-data-generations">
      <title>Data generations</title>
      <para>DRBD marks the start of a new <emphasis>data
	  generation</emphasis> at each of the following occurrences:
      <itemizedlist>
	  <listitem>
	    <para>The initial device full sync,</para>
	  </listitem>
	  <listitem>
	    <para>a disconnected resource switching to the primary
	      role,</para>
	  </listitem>
	  <listitem>
	    <para>a resource in the primary role disconnecting.</para>
	  </listitem>
      </itemizedlist> Thus, we can summarize that whenever a resource
	is in the <code>Connected</code> connection state, and both
	nodes' disk state is <code>UpToDate</code>, the current data
	generation on both nodes is the same. The inverse is also
	true.</para>
      <para>
	Every new data generation is identified by an 8-byte,
	universally unique identifier (<acronym>UUID</acronym>).
      </para>
    </section>
    <section id="s-gi-tuple">
      <title>The generation identifier tuple</title>
      <para>DRBD keeps four pieces of information about current and
	historical data generations in the local resource meta data:
      <itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Current UUID</title>
	      <para>This is the generation identifier for the current
		data generation, as seen from the local node's
		perspective. When a resource is <code>Connected</code>
		and fully synchronized, the current UUID is identical
		between nodes.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title><quote>Bitmap</quote> UUID</title>
	      <para>This is the UUID of the generation against which
		the on-disk sync bitmap is tracking changes. As the
		on-disk sync bitmap itself, this identifier is only
		relevant while in disconnected mode. If the resource
		is <code>Connected</code>, this UUID is always empty
		(zero).</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Two <quote>Historical</quote> UUID's</title>
	      <para>These are the identifiers of the two data
		generations preceding the current one.</para>
	    </formalpara>
	  </listitem>
      </itemizedlist>
      </para>
      <para>Collectively, these four items are referred to as the
	<emphasis>generation identifier tuple</emphasis>, or <quote>GI
	  tuple</quote> for short.</para>
    </section>
    <section>
      <title>How generation identifiers change</title>
      <section>
	<title>Start of a new data generation</title>
	<para>When a node loses connection to its peer (either by
	  network failure or manual intervention), DRBD modifies its
	  local generation identifiers in the following manner:
	  <figure>
	    <title>GI tuple changes at start of a new data
	      generation</title>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="gi-changes-newgen"/>
	      </imageobject>
	      <caption>Note: only changes on primary node shown (on a
	      secondary node, no changes apply).</caption>
	    </mediaobject>
	  </figure>
	  <orderedlist>
	    <listitem>
	      <para>A new UUID is created for the new data generation.
		This becomes the new current UUID for the primary
		node.</para>
	    </listitem>
	    <listitem>
	      <para>The previous UUID now refers to the generation the
		bitmap is tracking changes against, so it becomes the
		new bitmap UUID for the primary node.</para>
	    </listitem>
	    <listitem>
	      <para>On the secondary node, the GI tuple remains
		unchanged.</para>
	    </listitem>
	  </orderedlist>
	</para>
      </section>
      <section>
	<title>Start of re-sychronization</title>
	<para>Upon the initiation of re-synchronization, DRBD performs
	  these modifications on the local generation identifiers:
	  <figure>
	    <title>GI tuple changes at start of
	      re-synchronization</title>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="gi-changes-syncstart"/>
	      </imageobject>
	      <caption>Note: only changes on synchronization source
	      shown.</caption>
	    </mediaobject>
	  </figure>
	  <orderedlist>
	    <listitem>
	      <para>The current UUID on the synchronization source
		remains unchanged.</para>
	    </listitem>
	    <listitem>
	      <para>The bitmap UUID on the synchronization source is
		rotated out to the first historical UUID.</para>
	    </listitem>
	    <listitem>
	      <para>A new bitmap UUID is generated on the
		synchronization source.</para>
	    </listitem>
	    <listitem>
	      <para>This UUID becomes the new current UUID on the
		synchronization target.</para>
	    </listitem>
	    <listitem>
	      <para>The bitmap and historical UUID's on the
		synchronization target remain unchanged.</para>
	    </listitem>
	  </orderedlist>
	</para>
      </section>
      <section>
	<title>Completion of re-synchronization</title>
	<para>When re-synchronization concludes, the following changes
	  are performed:
	<figure>
	    <title>GI tuple changes at completion of
	      re-synchronization</title>
	    <mediaobject>
	      <imageobject>
		<imagedata fileref="gi-changes-synccomplete"/>
	      </imageobject>
	      <caption>Note: only changes on synchronization source
	      shown.</caption>
	    </mediaobject>
	  </figure>
	  <orderedlist>
	    <listitem>
	      <para>The current UUID on the synchronization source
		remains unchanged.</para>
	    </listitem>
	    <listitem>
	      <para>The bitmap UUID on the synchronization source is
		rotated out to the first historical UUID, with that
		UUID moving to the second historical entry (any
		existing second historical entry is discarded).</para>
	    </listitem>
	    <listitem>
	      <para>The bitmap UUID on the synchronization source is
		then emptied (zeroed).</para>
	    </listitem>
	    <listitem>
	      <para>The synchronization target adopts the entire GI
		tuple from the synchronization source.</para>
	    </listitem>
	  </orderedlist>
	</para>
      </section>
    </section>
    <section>
      <title>How DRBD uses generation identifiers</title>
      <para>When a connection between nodes is established, the two
	nodes exchange their currently available generation
	identifiers, and proceed accordingly. A number of possible
	outcomes exist:
      <orderedlist>
	  <listitem>
	    <formalpara>
	      <title>Current UUID's empty on both nodes</title>
	      <para>The local node detects that both its current UUID
		and the peer's current UUID are empty.<footnote>
		  <para>In fact (but this is a technicality), for
		    freshly created meta data the current,
		    <quote>empty</quote> UUID has a magic value of
		    <code>0000000000000004</code>.</para>
		</footnote> This is the
		normal occurrence for a freshly configured resource
		that has not had the initial full sync initiated. No
		synchronization takes place; it has to be started
		manually.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Current UUID's empty on one node</title>
	      <para>The local node detects that the peer's current
		UUID is empty, and its own is not. This is the normal
		case for a freshly configured resource on which the
		initial full sync has just been initiated, the local
		node having been selected as the initial
		synchronization source. DRBD now sets all bits in the
		on-disk sync bitmap (meaning it considers the entire
		device out-of-sync), and starts synchronizing as a
		synchronization source.</para>
	    </formalpara>
	    <para>If the opposite case (local current UUID empty,
	      peer's non-empty), DRBD performs the same steps, except
	      that the local node becomes the synchronization
	      target.</para>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Equal current UUID's</title>
	      <para>The local node detects that its current UUID and
		the peer's current UUID are non-empty and equal. This
		is the normal occurrence for a resource that went into
		disconnected mode at a time when it was in the
		secondary role, and was not promoted on either node
		while disconnected. No synchronization takes place, as
		none is necessary.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Bitmap UUID matches peer's current UUID</title>
	      <para>The local node detects that its bitmap UUID
		matches the peer's current UUID, and that the peer's
		bitmap UUID is empty. This is the normal and expected
		occurrence after a secondary node failure, with the
		local node being in the primary role. It means that
		the peer never became primary in the meantime and
		worked on the basis of the same data generation all
		along. DRBD now initiates a normal, background
		re-synchronization, with the local node becoming the
		synchronization source.</para>
	    </formalpara>
	    <para>If, conversely, the local node detects that
	      <emphasis>its</emphasis> bitmap UUID is empty, and that
	      the <emphasis>peer's</emphasis> bitmap matches the local
	      node's current UUID, then that is the normal and
	      expected occurrence after a failure of the local node.
	      Again, DRBD now initiates a normal, background
	      re-synchronization, with the local node becoming the
	      synchronization target.</para>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Current UUID matches peer's historical
		UUID</title>
	      <para>The local node detects that its current UUID
		matches one of the peer's historical UUID's. This
		implies that while the two data sets share a common
		ancestor, and the local node has the up-to-date data,
		the information kept in the local node's bitmap is
		outdated and not useable. Thus, a normal resync would
		be insufficient. DRBD now marks the entire device as
		out-of-sync and initiates a full background
		re-synchronization, with the local node becoming the
		synchronization source.
	      </para>
	    </formalpara>
	    <para>In the opposite case (one of the local node's
	      historical UUID matches the peer's current UUID), DRBD
	      performs the same steps, except that the local node
	      becomes the synchronization target.
	    </para>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Bitmap UUID's match, current UUID's do not</title>
	    <para>The local node detects that its current UUID differs
	      from the peer's current UUID, and that the bitmap UUID's
	      match. This is split brain, but one where the data
	      generations have the same parent. This means that DRBD
	      invokes split brain auto-recovery strategies, if
	      configured. Otherwise, DRBD disconnects and waits for
	      manual split brain resolution.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Neither current nor bitmap UUID's match</title>
	      <para>The local node detects that its current UUID
		differs from the peer's current UUID, and that the
		bitmap UUID's <emphasis>do not</emphasis> match. This
		is split brain with unrelated ancestor generations,
		thus auto-recovery strategies, even if configured, are
		moot. DRBD disconnects and waits for manual split
		brain resolution.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>No UUID's match</title>
	      <para>Finally, in case DRBD fails to detect even a
		single matching element in the two nodes' GI tuples,
		it logs a warning about unrelated data and
		disconnects. This is DRBD's safeguard against
		accidental connection of two cluster nodes that have
		never heard of each other before.</para>
	    </formalpara>
	  </listitem>
	</orderedlist>
      </para>
    </section>
  </section>
  <section id="s-activity-log">
    <title>The Activity Log</title>
    <indexterm>
      <primary>Activity Log</primary>
    </indexterm>
    <section id="s-al-purpose">
      <title>Purpose</title>
      <indexterm>
	<primary>Activity Log</primary>
	<secondary>purpose</secondary>
      </indexterm>
      <para>During a write operation DRBD forwards the write operation
	to the local backing block device, but also sends the data
	block over the network. These two actions occur, for all
	practical purposes, simultaneously. Random timing behavior may
	cause a situation where the write operation has been
	completed, but the transmission via the network has not yet
	taken place. </para>
      <para>If, at this moment, the active node fails and fail-over is
	being initiated, the data block that did not make it to the
	peer prior to the crash has to be removed from the data set of
	the node during subsequent resynchronisation. Otherwise, the
	crashed node would be "one write ahead" of the surviving node,
	which would violate the <phrase>all or nothing</phrase>
	principle of replicated storage.<footnote>
	  <para>This is an issue that is not limited to DRBD, in fact,
	    this issue exists in practically all replicated storage
	    configurations.</para></footnote> Many other storage
	solutions (just as DRBD itself, prior to version 0.7) thus
	require that after a failure of the active, that node must be
	fully synchronized anew after its recovery.</para>
      <para>DRBD's approach, since version 0.7, is a different one.
	The <emphasis>activity log</emphasis> (AL), stored in the meta
	data area, keeps track of those blocks that have "recently"
	been written to.<footnote>
	  <para>The description of what constitutes a "recent" write
	    in terms of DRBD's AL implementation is omitted here for
	    brevity.</para>
      </footnote> Colloquially, these areas are referred to as
	<phrase>hot extents</phrase>.</para>
      <para>If a temporarily failed node that was in active mode at
	the time of failure is resynchronized, only those hot extents
	highlighted in the AL need to be synchronized, rather than the
	full device. This drastically reduces resynchronization time
	after an active node crash.</para>
    </section>
    <section id="s-active-extents">
      <title>Active extents</title>
      <indexterm>
	<primary>Activity Log</primary>
	<secondary>active extents</secondary>
      </indexterm>
      <para>The activity log has a configurable parameter, the number
	of active extents. Every active extent adds 4MiB to the amount
	of data being retransmitted after a Primary crash. This
	parameter must be understood as a compromise between the
	following opposites:
      <itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Many active extents</title>
	      <para>Keeping a large activity log improves write
		throughput. Every time a new extent is activated, an
		old extent is reset to inactive. This transition
		requires a write operation to the meta data area. If
		the number of active extents is high, old active
		extents are swapped out fairly rarely, reducing meta
		data write operations and thereby improving
		performance.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Few active extents</title>
	      <para>Keeping a small activity log reduces
		resynchronization time after active node failure and
		subsequent recovery.</para>
	    </formalpara>
	  </listitem>
      </itemizedlist></para>
    </section>
    <section id="s-suitable-al-size">
      <title>Selecting a suitable AL size</title>
      <indexterm>
	<primary>Activity Log</primary>
	<secondary>size</secondary>
      </indexterm>
      <para> The definition of the number of extents should be based
	on the desired resynchronisation time at a given
	resynchronization rate. The number of active extents can be
	calculated as follows:</para>
      <equation id="eq-al-extents">
	<title>Active extents calculation based on resync rate and
	  target resync time</title>
	<graphic fileref="al-extents"/>
      </equation>
      <para><replaceable>R</replaceable> is the resynchronization
	rate, given in MB/s.
	<replaceable>t<subscript>sync</subscript></replaceable> is the
	target resynchronization time, in seconds.
	<replaceable>E</replaceable> is the resulting number of active
	extents.</para>
      <para>To provide an example, suppose our cluster has an I/O
	subsystem with a throughput rate of 90 MiByte/s that was
	configured to a resynchronisation rate of 30 MiByte/s
	(<replaceable>R</replaceable>=30), and we want to keep our
	target resynchronization time at 4 minutes or 240 seconds
	(<replaceable>t<subscript>sync</subscript></replaceable>=240):</para>
      <equation id="eq-al-extents-example">
	<title>Active extents calculation based on resync rate and
	  target resync time (example)</title>
	<graphic fileref="al-extents-example"/>
      </equation>
      <para>The exact result is 1800, but since DRBD's hash function
	for the implementation of the AL works best if the number of
	extents is set to a prime number, we select 1801.</para>
    </section>
  </section>
  <section id="s-quick-sync-bitmap">
    <title>The quick-sync bitmap</title>
    <para>The quick-sync bitmap is the internal data structure which
      DRBD uses, on a per-resource basis, to keep track of blocks
      being in sync (identical on both nodes) or out-of sync. It is
      only relevant when a resource is in disconnected mode.</para>
    <para>In the quick-sync bitmap, one bit represents a 4-KiB chunk
      of on-disk data. If the bit is cleared, it means that the
      corresponding block is still in sync with the peer node. That
      implies that the block has not been written to since the time of
      disconnection. Conversely, if the bit is set, it means that the
      block has been modified and needs to be re-synchronized whenever
      the connection becomes available again.</para>
    <para>As DRBD detects write I/O on a disconnected device, and
      hence starts setting bits in the quick-sync bitmap, it does so
      in RAM &mdash; thus avoiding expensive synchronous metadata I/O
      operations. Only when the corresponding blocks turn
      <phrase>cold</phrase> (that is, expire from the <link
	linkend="s-activity-log">Activity Log</link>), DRBD makes the
      appropriate modifications in an on-disk representation of the
      quick-sync bitmap. Likewise, if the resource happens to be
      manually shut down on the remaining node while disconnected,
      DRBD flushes the <emphasis>complete</emphasis> quick-sync bitmap
      out to persistent storage.</para>
    <para>When the peer node recovers or the connection is
      re-established, DRBD combines the in-memory and on-disk bitmap
      information to determine the total data set that it must
      re-synchronize. Then, it transmits the corresponding blocks to
      the peer node, clearing sync bits in the bitmap as the secondary
      node acknowledges the modifications. If the re-synchronization
      is now interrupted (by another network outage, for example) and
      subsequently resumed it will continue where it left off &mdash;
      with any additional blocks modified in the meantime being added
      to the re-synchronization data set, of course.
      <note>
	<para>Re-synchronization may be also be paused and resumed
	  manually with the <command>drbdadm&nbsp;pause-sync</command>
	  and <command>drbdadm&nbsp;resume-sync</command> commands.
	  You should, however, not do so light-heartedly &mdash;
	  interrupted re-synchronization leaves your secondary node's
	  disk <code>Inconsistent</code> longer than necessary.</para>
      </note>
    </para>
  </section>
</chapter>
