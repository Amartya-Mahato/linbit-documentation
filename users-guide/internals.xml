<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-internals" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>DRBD Internals</title>
  <section id="s-metadata">
    <title>DRBD meta data</title>
    <indexterm>
      <primary>meta data</primary>
    </indexterm>
    <para>DRBD stores various pieces of information about the data it
      keeps in a dedicated area. This metadata includes:
	<itemizedlist>
	<listitem>
	  <para>the size of the DRBD device,</para>
	</listitem>
	<listitem>
	  <para>the Generation Identifier
	    (<acronym>GI</acronym>),</para>
	</listitem>
	<listitem>
	  <para>the on-disk sync bitmap,<footnote>
	      <para>Described in detail in <xref
		  linkend="s-on-disk-bitmap"/>.</para>
	      </footnote>
	  </para>
	</listitem>
	<listitem>
	  <para>the Activity Log (<acronym>AL</acronym>).<footnote>
	      <para>Described in detail in <xref
		  linkend="s-activity-log"/>.</para>
	    </footnote></para>
	</listitem>
	</itemizedlist>
    </para>
    <para>This metadata may be stored <emphasis>internally</emphasis>
      and <emphasis>externally</emphasis>. Which method is used is
      configurable on a per-resource basis.</para>
    <section id="s-internal-meta-data">
      <title>Internal meta data</title>
      <indexterm>
	<primary>meta data</primary>
	<secondary>internal</secondary>
      </indexterm>
      <para>Configuring a resource to use internal meta data means
	that DRBD stores its meta data on the same physical
	lower-level device as the actual production data. It does so
	by setting aside an area at the <emphasis>end</emphasis> of
	the device for the specific purpose of storing metadata.
      </para>
      <formalpara>
	<title>Advantage</title>
	<para>Since the meta data are inextricably linked with the
	  actual data, no special action is required from the
	  administrator in case of a hard disk failure. The meta data
	  are lost together with the actual data and are also restored
	  together.</para>
      </formalpara>
      <formalpara>
	<title>Disadvantage</title>
	<para>In case of the lower-level device being a single
	  physical hard disk (as opposed to a RAID set), internal meta
	  data may negatively affect write throughput. The performance
	  of write requests by the application may trigger an update
	  of the meta data in DRBD. If the meta data are stored on the
	  same magnetic disk of a hard disk, the write operation may
	  result in two additional movements of the write/read head of
	  the hard disk.
	</para>
      </formalpara>
      <caution>
	<para>If you are planning to use internal meta data in
	  conjuntion with an existing lower-level device that already
	  has data which you wish to preserve, you
	  <emphasis>must</emphasis> account for the space required by
	  DRBD's meta data. Otherwise, upon DRBD resource creation,
	  the newly created metadata would overwrite data at the end
	  of the lower-level device, potentially destroying existing
	  files in the process. To avoid that, you must do one of the
	  following things:
	    <itemizedlist>
	    <listitem>
	      <para>Enlarge your lower-level device. This is possible
		with any logical volume management facility (such as
		LVM or EVMS) as long as you have free space available
		in the corresponding volume group or container. It may
		also be supported by hardware storage
		solutions.</para>
	    </listitem>
	    <listitem>
	      <para>Shrink your existing file system on your
		lower-level device. This may or may not be supported
		By your file system.<footnote>
		  <para>The ext3 file system, for example, supports
		    offline file system shrinking. So does ReiserFS.
		    XFS does not.</para>
		  </footnote>
	      </para>
	    </listitem>
	    <listitem>
	      <para>If neither of the two are possible, use <link
		  linkend="s-external-meta-data">external meta
		  data</link> instead.</para>
	    </listitem>
	    </itemizedlist>
	</para>
	<para>To estimate the amount by which you must enlarge your
	  lower-level device our shrink your file system, see <xref
	    linkend="s-meta-data-size"/>.</para>
      </caution>
    </section>
    <section id="s-external-meta-data">
      <title>External meta data</title>
      <indexterm>
	<primary>meta data</primary>
	<secondary>external</secondary>
      </indexterm>
      <para>External meta data are simply stored on a separate,
	dedicated block device distinct from that which holds your
	production data.</para>
      <formalpara>
	<title>Advantage</title>
	<para>For some write operations, using external meta data
	  produces a somewhat improved latency behavior.</para>
      </formalpara>
      <formalpara>
	<title>Disadvantage</title>
	<para>Meta data are not inextricably linked with the actual
	  production data. This means that manual intervention is
	  required in the case of a hardware failure destroying just
	  the production data (but not DRBD meta data), to effect a
	  full data sync from the surviving node onto the subsequently
	  replaced disk.</para>
      </formalpara>
      <para>Use of external meta data is also the only viable option
	if <emphasis>all</emphasis> of the following apply:
	  <itemizedlist>
	  <listitem>
	    <para>You are using DRBD to duplicate an existing device
	      that already contains data you wish to preserve,
	      <emphasis>and</emphasis></para>
	  </listitem>
	  <listitem>
	    <para>that existing device does not support enlargement,
	      <emphasis>and</emphasis></para>
	  </listitem>
	  <listitem>
	    <para>the existing file system on the device does not
	      support shrinking.</para>
	  </listitem>
	  </itemizedlist>
      </para>
      <para>To estimate the required size of the block device
	dedicated to hold your device meta data, see <xref
	  linkend="s-meta-data-size"/>.</para>
    </section>
    <section id="s-meta-data-size">
      <title>Estimating meta data size</title>
      <indexterm>
	<primary>meta data</primary>
	<secondary>size</secondary>
      </indexterm>
      <para>You may calculate the exact space requirements for DRBD's
	meta data using the following formula:</para>
      <equation id="eq-metadata-size-exact">
	<title>Calculating DRBD meta data size (exactly)</title>
	<graphic fileref="metadata-size-exact"/>
      </equation>
      <para><replaceable>C<subscript>s</subscript></replaceable> is
	the data device size in sectors.<footnote>
	  <para>You may retrieve the device size by issuing
	    <command>blockdev --getsz
	      <replaceable>device</replaceable></command>.</para>
	  <para>However, <command>echo $(( $(blockdev --getsize64
	      <replaceable>device</replaceable>) / 512))</command> is
	    more reliable, since it also works for block devices
	    larger than 2 TB.
	  </para>
	</footnote>The result,
	<replaceable>M<subscript>s</subscript></replaceable>, is also
	expressed in sectors. To convert to MB, divide by
	2048.<footnote>
	  <para>The hard sector size on Linux (most architectures) is
	    512 bytes or .5 KB. The only known deviation from this
	    applies to the s390 architecture.</para>
	  </footnote>
      </para>
      <para>In practice, you may use a reasonably good approximation,
	given below. Note that in this formula, the unit is megabytes,
	not sectors:</para>
      <equation id="eq-metadata-size-approx">
	<title>Estimating DRBD meta data size (approximately)</title>
	<graphic fileref="metadata-size-approx"/>
      </equation>
    </section>
  </section>
  <section id="s-on-disk-bitmap">
    <title>The on-disk Sync Bitmap</title>
    <xi:include href="todo.xml"/>
  </section>
  <section id="s-activity-log">
    <title>The Activity Log</title>
    <indexterm>
      <primary>Activity Log</primary>
    </indexterm>
    <section id="s-al-purpose">
      <title>Purpose</title>
      <indexterm>
	<primary>Activity Log</primary>
	<secondary>purpose</secondary>
      </indexterm>
      <para>During a write operation DRBD forwards the write operation
	to the local backing block device, but also sends the data
	block over the network. These two actions occur, for all
	practical purposes, simultaneously. Random timing behavior may
	cause a situation where the write operation has been
	completed, but the transmission via the network has not yet
	taken place. </para>
      <para>If, at this moment, the active node fails and fail-over is
	being initiated, the data block that did not make it to the
	peer prior to the crash has to be removed from the data set of
	the node during subsequent resynchronisation. Otherwise, the
	crashed node would be "one write ahead" of the surviving node,
	which would violate the <phrase>all or nothing</phrase>
	principle of replicated storage.<footnote>
	  <para>This is an issue that is not limited to DRBD, in fact,
	    this issue exists in practically all replicated storage
	    configurations.</para></footnote> Many other storage
	solutions (just as DRBD itself, prior to version 0.7) thus
	require that after a failure of the active, that node must be
	fully synchronized anew after its recovery.</para>
      <para>DRBD's approach, since version 0.7, is a different one.
	The <emphasis>activity log</emphasis> (AL), stored in the meta
	data area, keeps track of those blocks that have "recently"
	been written to.<footnote>
	  <para>The description of what constitutes a "recent" write
	    in terms of DRBD's AL implementation is omitted here for
	    brevity.</para>
      </footnote> Colloquially, these areas are referred to as
	<phrase>hot extents</phrase>.</para>
      <para>If a temporarily failed node that was in active mode at
	the time of failure is resynchronized, only those hot extents
	highlighted in the AL need to be synchronized, rather than the
	full device. This drastically reduces resynchronization time
	after an active node crash.</para>
    </section>
    <section id="s-active-extents">
      <title>Active extents</title>
      <indexterm>
	<primary>Activity Log</primary>
	<secondary>active extents</secondary>
      </indexterm>
      <para>The activity log has a configurable parameter, the number
	of active extents. Every active extent adds 4MiB to the amount
	of data being retransmitted after a Primary crash. This
	parameter must be understood as a compromise between the
	following opposites:
      <itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Many active extents</title>
	      <para>Keeping a large activity log improves write
		throughput. Every time a new extent is activated, an
		old extent is reset to inactive. This transition
		requires a write operation to the meta data area. If
		the number of active extents is high, old active
		extents are swapped out fairly rarely, reducing meta
		data write operations and thereby improving
		performance.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Few active extents</title>
	      <para>Keeping a small activity log reduces
		resynchronization time after active node failure and
		subsequent recovery.</para>
	    </formalpara>
	  </listitem>
      </itemizedlist></para>
    </section>
    <section id="s-suitable-al-size">
      <title>Selecting a suitable AL size</title>
      <indexterm>
	<primary>Activity Log</primary>
	<secondary>size</secondary>
      </indexterm>
      <para> The definition of the number of extents should be based
	on the desired resynchronisation time at a given
	resynchronization rate. The number of active extents can be
	calculated as follows:</para>
      <equation id="eq-al-extents">
	<title>Active extents calculation based on resync rate and
	  target resync time</title>
	<graphic fileref="al-extents"/>
      </equation>
      <para><replaceable>R</replaceable> is the resynchronization
	rate, given in MB/s.
	<replaceable>t<subscript>sync</subscript></replaceable> is the
	target resynchronization time, in seconds.
	<replaceable>E</replaceable> is the resulting number of active
	extents.</para>
      <para>To provide an example, suppose our cluster has an I/O
	subsystem with a throughput rate of 90 MiByte/s that was
	configured to a resynchronisation rate of 30 MiByte/s
	(<replaceable>R</replaceable>=30), and we want to keep our
	target resynchronization time at 4 minutes or 240 seconds
	(<replaceable>t<subscript>sync</subscript></replaceable>=240):</para>
      <equation id="eq-al-extents-example">
	<title>Active extents calculation based on resync rate and
	  target resync time (example)</title>
	<graphic fileref="al-extents-example"/>
      </equation>
      <para>The exact result is 1800, but since DRBD's hash function
	for the implementation of the AL works best if the number of
	extents is set to a prime number, we select 1801.</para>
    </section>
  </section>
</chapter>
