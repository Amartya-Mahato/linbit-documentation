<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-pacemaker">
  <title>Integrating DRBD with Pacemaker clusters</title>
  <indexterm>
    <primary>Pacemaker</primary>
  </indexterm>
  <para>Using DRBD in conjunction with the Pacemaker cluster
    stack is arguably DRBD's most frequently found use case. Pacemaker
    is also one of the applications that make DRBD extremely powerful
    in a wide variety of usage scenarios.</para>
  <important>
    <para>This chapter is relevant for Pacemaker versions 1.0.3 and
      above, and DRBD version 8.3.2 and above. It does not touch upon
      DRBD configuration in Pacemaker clusters of earlier
      versions.</para>
    <para>Pacemaker is the direct, logical successor to the
      Heartbeat 2 cluster stack, and as far as the cluster resource
      manager infrastructure is concerned, a direct continuation of
      the Heartbeat 2 codebase. Since the intial stable release of
      Pacemaker, Heartbeat 2 can be considered obsolete and
      Pacemaker should be used instead.</para>
    <para>For legacy configurations where the legacy Heartbeat 2
    cluster manager must still be used, see <xref
    linkend="ch-heartbeat"/>.</para>
  </important>
  <section id="s-pacemaker-primer">
    <title>Pacemaker primer</title>
    <para>Pacemaker is a sophisticated, feature-rich, and widely
    deployed cluster resource manager for the Linux platform. It comes
    with a rich set of documentation. In order to understand this
    chapter, reading the following documents is highly recommended:</para>
    <itemizedlist>
      <listitem><para><ulink
      url="http://www.clusterlabs.org/doc/Cluster_from_Scratch.pdf">Clusters
      From Scratch</ulink>, a step-by-step guide to configuring high
      availability clusters;</para></listitem>
      <listitem>
	<para><ulink
	url="http://www.clusterlabs.org/doc/crm_cli.html">CRM CLI
	(command line interface) tool</ulink>, a manual for the CRM
	shell, a simple and intuitive command line interface bundled
	with Pacemaker;</para>
      </listitem>
      <listitem>
	<para><ulink
	url="http://www.clusterlabs.org/doc/en-US/Pacemaker/1.0/html/Pacemaker_Explained/s-intro-pacemaker.html">Pacemaker
	Configuration Explained</ulink>, a reference document
	explaining the concept and design behind Pacemaker.</para>
      </listitem>
    </itemizedlist>
  </section>
  <section id="s-pacemaker-crm-drbd-backed-service">
	<title>Adding a DRBD-backed service to the cluster
	configuration</title>
	<para>This section explains how to enable a DRBD-backed
	  service in a Pacemaker cluster.</para>
	<note>
	  <para>If you are employing the DRBD OCF resource agent, it
	    is recommended that you defer DRBD startup, shutdown,
	    promotion, and demotion <emphasis>exclusively</emphasis>
	    to the OCF resource agent. That means that you should
	    disable the DRBD init script:</para>
	  <literallayout><userinput>chkconfig drbd off</userinput></literallayout>
	</note>
	<para>The <code>drbd</code> OCF resource agent provides
	  Master/Slave capability, allowing Pacemaker to start and
	  monitor the DRBD resource on multiple nodes and promoting
	  and demoting as needed. You must, however, understand that
	  the <code>drbd</code> RA disconnects and detaches all DRBD
	  resources it manages on Pacemaker shutdown, and also upon
	  enabling standby mode for a node.</para>
	<important>
	  <para>The OCF resource agent which ships with DRBD belongs
	    to the <code>linbit</code> provider, and hence installs as
	    <filename>/usr/lib/ocf/resource.d/linbit/drbd</filename>.
	    This resource agent was bundled with DRBD in version 8.3.2
	    as a beta feature, and became fully supported in
	    8.3.4.</para>
	  <para>There is a legacy resource agent that shipped with
	    Heartbeat 2, which uses the <code>heartbeat</code>
	    provider and installs into
	    <filename>/usr/lib/ocf/resource.d/heartbeat/drbd</filename>.
	    Using the legacy OCF RA is not recommended.</para>
	</important>
	<para>In order to enable a DRBD-backed configuration for a
	  MySQL database in a Pacemaker CRM cluster with the
	  <code>drbd</code> OCF resource agent, you must create both
	  the necessary resources, and Pacemaker constraints to ensure
	  your service only starts on a previously promoted DRBD
	  resource. You may do so using the <code>crm</code> shell, as
	  outlined in the following example:</para>
	  <literallayout><userinput>crm configure</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive drbd_mysql ocf:linbit:drbd \
                    params drbd_resource="mysql" \
                    op monitor interval="15s"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>ms ms_drbd_mysql drbd_mysql \
                    meta master-max="1" master-node-max="1" \
                         clone-max="2" clone-node-max="1" \
                         notify="true"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive fs_mysql ocf:heartbeat:Filesystem \
                    params device="/dev/drbd/by-res/mysql" directory="/var/lib/mysql" fstype="ext3"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive ip_mysql ocf:heartbeat:IPaddr2 \
                    params ip="10.9.42.1" nic="eth0"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive mysqld lsb:mysqld</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>group mysql fs_mysql ip_mysql mysqld</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>colocation mysql_on_drbd inf: mysql ms_drbd_mysql:Master</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>order mysql_after_drbd inf: ms_drbd_mysql:promote mysql:start</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>commit</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>exit</userinput>
<computeroutput>bye</computeroutput></literallayout>
	<para>After this, your configuration should be enabled.
	  Pacemaker now selects a node on which it promotes the DRBD
	  resource, and then starts the DRBD-backed resource group on
	  that same node.</para>
  </section>
  <section id="s-pacemaker-fencing">
    <title>Using resource-level fencing in Pacemaker clusters</title>
    <para>This section outlines the steps necessary to prevent
      Pacemaker from promoting a <code>drbd</code> Master/Slave
      resource when its DRBD replication link has been interrupted.
      This keeps Pacemaker from starting a service with outdated data
      and causing an unwanted <quote>time warp</quote> in the process.
      <important>
	<para>It is absolutely vital to configure at least two
	  independent <link
	    linkend="s-openais-communication-channels">OpenAIS
	    communication channels</link> for this functionality to
	  work correctly.</para>
	<para>Furthermore, as mentioned in <xref
	  linkend="s-pacemaker-crm-drbd-backed-service"/>, you should
	  make sure the DRBD init script is disabled.</para>
      </important>
    </para>
    <para>In order to enable resource-level fencing for Pacemaker, you
      will have to set two options in
      <filename>drbd.conf</filename>:
      <programlisting>resource <replaceable>resource</replaceable> {
  disk {
    fencing resource-only;
    ...
  }
  handlers {
    fence-peer "/usr/lib/drbd/crm-fence-peer.sh";
    after-resync-target "/usr/lib/drbd/crm-unfence-peer.sh";
    ...
  }
  ...
}</programlisting>
      Thus, if the DRBD replication link becomes disconnected, the
      <filename>crm-fence-peer.sh</filename> script contacts the
      cluster manager, determines the Pacemaker Master/Slave resource
      associated with this DRBD resource, and ensures that the
      Master/Slave resource no longer gets promoted on any node other
      than the currently active one. Conversely, when the connection
      is re-established and DRBD completes its synchronization
      process, then that constraint is removed and the cluster manager
      is free to promote the resource on any node again.
    </para>
  </section>
  <section id="s-pacemaker-stacked-resources">
    <title>Using stacked DRBD resources in Pacemaker clusters</title>
    <para>Stacked resources allow DRBD to be used for multi-level
    redundancy in multiple-node clusters, or to establish off-site
    disaster recovery capability. This section describes how to
    configure DRBD and Pacemaker in such configurations.</para>
    <section id="s-pacemaker-stacked-dr">
      <title>Adding off-site disaster recovery to Pacemaker
      clusters</title>
      <para>In this configuration scenario, we would deal with a
	two-node high availability cluster in one site, plus a
	separate node which would presumably be housed off-site. The
	third node acts as a disaster recovery node and is a
	standalone server. Consider the following illustration to
	describe the concept.</para>
      <figure>
	<title>DRBD resource stacking in Pacemaker clusters</title>
	<mediaobject>
	  <imageobject>
	    <imagedata
	    fileref="drbd-resource-stacking-pacemaker-3nodes"/>
	  </imageobject>
	  <caption>3-node resource stacking in a Pacemaker
	  cluster. Storage in light blue, connections to storage in
	  red. Primary role in orange, Secondary in gray. Direction of
	  replication indicated by arrows.</caption>
	</mediaobject>
      </figure>
      <para>In this example, <code>alice</code> and <code>bob</code>
	form a two-node Pacemaker cluster, whereas
      <code>charlie</code> is an off-site node not managed by
      Pacemaker.</para>
      <para>To create such a configuration, you would first
	configure and initialize DRBD resources as described in <xref
      linkend="s-three-nodes"/>. Then, configure Pacemaker with the
      following CRM configuration:</para>
      <programlisting>primitive drbd_r0 ocf:linbit:drbd \
	params drbd_resource="r0"

primitive drbd_r0-U ocf:linbit:drbd \
	params drbd_resource="r0-U" \

primitive ip_stacked ocf:heartbeat:IPaddr2 \
	params ip="192.168.42.1" nic="eth0"

ms ms_drbd_r0 res_drbd_r0 \
	meta master-max="1" master-node-max="1" \
        clone-max="2" clone-node-max="1" \
        notify="true" globally-unique="false"

ms ms_drbd_r0-U res_drbd_r0-U \
	meta master-max="1" clone-max="1" \
        clone-node-max="1" master-node-max="1" \
        notify="true" globally-unique="false"

colocation c_drbd_r0-U_on_drbd_r0 \
        inf: ms_drbd_r0-U ms_drbd_r0:Master

colocation c_drbd_r0-U_on_ip \
        inf: ms_drbd_r0-U res_ip_stacked

colocation c_ip_on_r0_master \
        inf: res_ip_stacked ms_drbd_r0:Master

order o_ip_before_r0-U \
        inf: res_ip_stacked ms_drbd_r0-U:start

order o_drbd_r0_before_r0-U \
        inf: ms_drbd_r0:promote ms_drbd_r0-U:start
</programlisting>
      <para>Assuming you created this configuration in a temporary
        file named <filename>/tmp/crm.txt</filename>, you may import
        it into the live cluster configuration with the following
        command:
	<literallayout><userinput>crm configure &lt; /tmp/crm.txt</userinput></literallayout>
      </para>
      <para>This configuration will ensure that the following actions
        occur in the correct order on the
        <code>alice</code>/<code>bob</code> cluster:</para>
      <orderedlist>
	<listitem>
	  <para>Pacemaker starts the DRBD resource <code>r0</code> on
	  both cluster nodes, and promotes one node to the Master
	  (DRBD Primary) role.</para>
	</listitem>
	<listitem>
	  <para>Pacemaker then starts the IP address 192.168.42.1,
	    which the stacked resource is to use for replication to
	    the third node. It does so on the node it has previously
	    promoted to the Master role for <code>r0</code> DRBD
	    resource.</para>
	</listitem>
	<listitem>
	  <para>On the node which now has the Primary role for
	    <code>r0</code> and also the replication IP address for
	    <code>r0-U</code>, Pacemaker now starts the
	    <code>r0-U</code> DRBD resource, which connects and
	    replicates to the off-site node.</para>
	</listitem>
	<listitem>
	  <para>Pacemaker then promotes the <code>r0-U</code> resource
	  to the Primary role too, so it can be used by an
	  application.</para>
	</listitem>
      </orderedlist>
      <para>Thus, this Pacemaker configuration ensures that there is
	not only full data redundancy between cluster nodes, but also
	to the third, off-site node.</para>
      <note>
	<para>This type of setup is usually deployed together with
	  <link linkend="s-drbd-proxy">DRBD Proxy</link>.</para>
      </note>
    </section>
    <section id="s-pacemaker-stacked-4way">
      <title>Using stacked resources to achieve 4-way redundancy in
      Pacemaker clusters</title>
      <para>In this configuration, a total of three DRBD resources
	(two unstacked, one stacked) are used to achieve 4-way storage
	redundancy. This means that of a 4-node cluster, up to three
	nodes can fail while still providing service
	availability.</para>
      <para>Consider the following illustration to explain the
	concept.</para>
      <figure>
	<title>DRBD resource stacking in Pacemaker clusters</title>
	<mediaobject>
	  <imageobject>
	    <imagedata
	    fileref="drbd-resource-stacking-pacemaker-4nodes"/>
	  </imageobject>
	  <caption>4-node resource stacking in a Pacemaker
	  cluster. Storage in light blue, connections to storage in
	  red. Primary role in orange, Secondary in gray. Direction of
	  replication indicated by arrows.</caption>
	</mediaobject>
      </figure>
      <para>In this example, <code>alice</code>, <code>bob</code>,
	<code>charlie</code>, and <code>daisy</code> form two two-node
	Pacemaker clusters. <code>alice</code> and <code>bob</code>
	form the cluster named <code>left</code> and replicate data
	using a DRBD resource between them, while <code>charlie</code>
	and <code>daisy</code> do the same with a separate DRBD
	resource, in a cluster named <code>right</code>. A third,
	stacked DRBD resource connects the two clusters.</para>
      <note>
	<para>Due to limitations in the Pacemaker cluster manager as
	  of Pacemaker version 1.0.5, it is not possible to create
	  this setup in a single four-node cluster without disabling
	  CIB validation, which is an advanced process not recommended
	  for general-purpose use. It is anticipated that this is
	  being addressed in future Pacemaker releases.</para>
      </note>
      <para>To create such a configuration, you would first configure
	and initialize DRBD resources as described in <xref
      linkend="s-three-nodes"/> (except that the remote half of the
	DRBD configuration is also stacked, not just the local
	cluster). Then, configure Pacemaker with the following CRM
	configuration, starting with the cluster
	<code>left</code>:</para>
      <programlisting>primitive drbd_left ocf:linbit:drbd \
	params drbd_resource="left"

primitive drbd_stacked ocf:linbit:drbd \
	params drbd_resource="stacked" \

primitive ip_stacked_left ocf:heartbeat:IPaddr2 \
	params ip="10.9.9.100" nic="eth0"

ms ms_drbd_left res_drbd_left \
	meta master-max="1" master-node-max="1" \
        clone-max="2" clone-node-max="1" \
        notify="true"

ms ms_drbd_stacked res_drbd_stacked \
	meta master-max="1" clone-max="1" \
        clone-node-max="1" master-node-max="1" \
        notify="true" target-role="Master"

colocation c_ip_on_left_master \
        inf: res_ip_stacked_left ms_drbd_left:Master

colocation c_drbd_stacked_on_ip_left \
        inf: ms_drbd_stacked res_ip_stacked_left

order o_ip_before_stacked_left \
        inf: res_ip_stacked_left ms_drbd_stacked_left:start

order o_drbd_left_before_stacked_left \
        inf: ms_drbd_left:promote ms_drbd_stacked_left:start
</programlisting>
      <para>Assuming you created this configuration in a temporary
        file named <filename>/tmp/crm.txt</filename>, you may import
        it into the live cluster configuration with the following
        command:
	<literallayout><userinput>crm configure &lt; /tmp/crm.txt</userinput></literallayout>
      </para>
      <para>After adding this configuration to the CIB, Pacemaker will
	execute the following actions:</para>
      <orderedlist>
	<listitem>
	  <para>Bring up the DRBD resource <code>left</code>
	    replicating between <code>alice</code> and
	    <code>bob</code> promoting the resource to the Master
	    role on one of these nodes.</para>
	</listitem>
	<listitem>
	  <para>Bring up the IP address 10.9.9.100 (on either
	    <code>alice</code> or <code>bob</code>, depending on which
	    of these holds the Master role for the resource
	    <code>left</code>).</para>
	</listitem>
	<listitem>
	  <para>Bring up the DRBD resource <code>stacked</code> on the
	    same node that holds the just-configured IP address.</para>
	</listitem>
	<listitem>
	  <para>Promote the stacked DRBD resource to the Primary
	    role.</para>
	</listitem>
      </orderedlist>
      <para>Now, proceed on the cluster <code>right</code> by creating
	the following configuration:</para>
      <programlisting>primitive drbd_right ocf:linbit:drbd \
	params drbd_resource="right"

primitive drbd_stacked ocf:linbit:drbd \
	params drbd_resource="stacked" \

primitive ip_stacked_right ocf:heartbeat:IPaddr2 \
	params ip="10.9.10.101" nic="eth0"

ms ms_drbd_right res_drbd_right \
	meta master-max="1" master-node-max="1" \
        clone-max="2" clone-node-max="1" \
        notify="true"

ms ms_drbd_stacked res_drbd_stacked \
	meta master-max="1" clone-max="1" \
        clone-node-max="1" master-node-max="1" \
        notify="true" target-role="Slave"

colocation c_drbd_stacked_on_ip_right \
        inf: ms_drbd_stacked res_ip_stacked_right

colocation c_ip_on_right_master \
        inf: res_ip_stacked_right ms_drbd_right:Master

order o_ip_before_stacked_right \
        inf: res_ip_stacked_right ms_drbd_stacked_right:start

order o_drbd_right_before_stacked_right \
        inf: ms_drbd_right:promote ms_drbd_stacked_right:start
</programlisting>
      <para>After adding this configuration to the CIB, Pacemaker will execute the following actions:</para>
      <orderedlist>
	<listitem>
	  <para>Bring up the DRBD resource <code>right</code>
	    replicating between <code>charlie</code> and
	    <code>daisy</code>, promoting the resource to the Master
	    role on one of these nodes.</para>
	</listitem>
	<listitem>
	  <para>Bring up the IP address 10.9.10.101 (on either
	    <code>charlie</code> or <code>daisy</code>, depending on
	    which of these holds the Master role for the resource
	    <code>right</code>).</para>
	</listitem>
	<listitem>
	  <para>Bring up the DRBD resource <code>stacked</code> on the
	    same node that holds the just-configured IP address.</para>
	</listitem>
	<listitem>
	  <para>Leave the stacked DRBD resource in the Secondary role
	    (due to <code>target-role="Slave"</code>).</para>
	</listitem>
      </orderedlist>
    </section>
  </section>
  <section id="s-pacemaker-floating-peers">
    <title>Configuring DRBD to replicate between two SAN-backed
      Pacemaker clusters</title>
    <para>This is a somewhat advanced setup usually employed in
      split-site configurations. It involves two separate Pacemaker
      clusters, where each cluster has access to a separate Storage
      Area Network (SAN). DRBD is then used to replicate data stored
      on that SAN, across an IP link between sites.</para>
    <para>Consider the following illustration to describe the concept.
      <figure>
	<title>Using DRBD to replicate between SAN-based
	clusters</title>
	<mediaobject>
	  <imageobject>
	    <imagedata
	    fileref="drbd-pacemaker-floating-peers"/>
	  </imageobject>
	  <caption>DRBD floating peer configuration in a Pacemaker
	    cluster. Storage in light blue, active connections to
	    storage in red, inactive connections to storage in red
	    dashed. Primary role in orange, Secondary in gray.
	    Direction of replication indicated by arrows.</caption>
	</mediaobject>
      </figure>
    </para>
    <para>Which of the individual nodes in each site currently acts as
      the DRBD peer is not explicitly defined &mdash; the DRBD peers
      <link linkend="s-floating-peers">are said to
	<emphasis>float</emphasis></link>; that is, DRBD binds to
      virtual IP addresses not tied to a specific physical
      machine.</para>
    <note>
      <para>This type of setup is usually deployed together with <link
	  linkend="s-drbd-proxy">DRBD Proxy</link> and/or <link
	  linkend="s-truck-based-replication">truck based
	  replication</link>.</para>
      <para>Since this type of setup deals with shared storage,
	configuring and testing STONITH is absolutely vital for it to
	work properly.</para>
    </note>
    <section id="s-pacemaker-floating-peers-drbd-config">
      <title>DRBD resource configuration</title>
      <para>To enable your DRBD resource to float, configure it in
	<filename>drbd.conf</filename> in the following
	fashion:</para>
    <programlisting>resource <replaceable>resource</replaceable> {
  ...
  device /dev/drbd0;
  disk /dev/sda1;
  meta-disk internal;
  floating 10.9.9.100:7788;
  floating 10.9.10.101:7788;
}</programlisting>
    <para>The <option>floating</option> keyword replaces the
	<option>on <replaceable>host</replaceable></option> sections
	normally found in the resource configuration. In this mode,
	DRBD identifies peers by IP address and TCP port, rather than
	by host name. It is important to note that the addresses
	specified must be virtual cluster IP addresses, rather than
	physical node IP addresses, for floating to function properly.
	As shown in the example, in split-site configurations the two
	floating addresses can be expected to belong to two separate
	IP networks &mdash; it is thus vital for routers and firewalls
	to properly allow DRBD replication traffic between the
	nodes.</para>
    </section>
    <section id="s-pacemaker-floating-peers-crm-config">
      <title>Pacemaker resource configuration</title>
      <para>A DRBD floating peers setup, in terms of Pacemaker
	configuration, involves the following items (in each of the
	two Pacemaker clusters involved):
	<itemizedlist>
	  <listitem>
	    <para>A virtual cluster IP address.</para>
	  </listitem>
	  <listitem>
	    <para>A master/slave DRBD resource (using the DRBD OCF
	      resource agent).</para>
	  </listitem>
	  <listitem>
	    <para>Pacemaker constraints ensuring that
	      resources are started on the correct nodes, and in the
	      correct order.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <para>To configure a resource named <code>mysql</code> in a
	floating peers configuration in a 2-node cluster, using the
	replication address <code>10.9.9.100</code>, configure
	Pacemaker with the following <code>crm</code> commands:</para>
      <literallayout><userinput>crm configure</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive ip_float_left ocf:heartbeat:IPaddr2 \
                    params ip=10.9.9.100</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive drbd_mysql ocf:linbit:drbd \
                    params drbd_resource=mysql</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>ms ms_drbd_mysql drbd_mysql \
                    meta master-max="1" master-node-max="1" \
                         clone-max="1" clone-node-max="1" \
                         notify="true" target-role="Master"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>order drbd_after_left inf: ip_float_left ms_drbd_mysql</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>colocation drbd_on_left inf: ms_drbd_mysql ip_float_left</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>commit</userinput>
<computeroutput>bye</computeroutput></literallayout>
      <para>After adding this configuration to the CIB, Pacemaker will execute the following actions:</para>
      <orderedlist>
	<listitem>
	  <para>Bring up the IP address 10.9.9.100 (on either
	    <code>alice</code> or <code>bob</code>).</para>
	</listitem>
	<listitem>
	  <para>Bring up the DRBD resource according to the IP
	    address configured.</para>
	</listitem>
	<listitem>
	  <para>Promote the DRBD resource to the Primary role.</para>
	</listitem>
      </orderedlist>
      <para>Then, in order to create the matching configuration in the
	other cluster, configure <emphasis>that</emphasis> Pacemaker
	instance with the following commands:</para>
      <literallayout><userinput>crm configure</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive ip_float_right ocf:heartbeat:IPaddr2 \
                    params ip=10.9.10.101</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive drbd_mysql ocf:linbit:drbd \
                    params drbd_resource=mysql</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>ms ms_drbd_mysql drbd_mysql \
                    meta master-max="1" master-node-max="1" \
                         clone-max="1" clone-node-max="1" \
                         notify="true" target-role="Slave"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>order drbd_after_right inf: ip_float_right ms_drbd_mysql</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>colocation drbd_on_right inf: ms_drbd_mysql ip_float_right</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>commit</userinput>
<computeroutput>bye</computeroutput></literallayout>
      <para>After adding this configuration to the CIB, Pacemaker will execute the following actions:</para>
      <orderedlist>
	<listitem>
	  <para>Bring up the IP address 10.9.10.101 (on either
	    <code>charlie</code> or <code>daisy</code>).</para>
	</listitem>
	<listitem>
	  <para>Bring up the DRBD resource according to the IP
	    address configured.</para>
	</listitem>
	<listitem>
	  <para>Leave the DRBD resource in the Secondary role (due to
	    <code>target-role="Slave"</code>).</para>
	</listitem>
      </orderedlist>
    </section>
    <section id="s-pacemaker-floating-peers-site-fail-over">
      <title>Site fail-over</title>
      <para>In split-site configurations, it may be necessary to
      transfer services from one site to another. This may be a
      consequence of a scheduled transition, or of a disastrous
      event. In case the transition is a normal, anticipated event,
      the recommended course of action is this:</para>
      <itemizedlist>
	<listitem>
	  <para>Connect to the cluster on the site about to relinquish
	    resources, and change the affected DRBD resource's
	    <code>target-role</code> attribute from
	    <code>Master</code> to <code>Slave</code>. This will shut
	    down any resources depending on the Primary role of the
	    DRBD resource, demote it, and continue to run, ready to
	    receive updates from a new Primary.</para>
	</listitem>
	<listitem>
	  <para>Connect to the cluster on the site about to take over
	    resources, and change the affected DRBD resource's
	    <code>target-role</code> attribute from <code>Slave</code>
	    to <code>Master</code>. This will promote the DRBD
	    resources, start any other Pacemaker resources
	    depending on the Primary role of the DRBD resource, and
	    replicate updates to the remote site.</para>
	</listitem>
	<listitem>
	  <para>To fail back, simply reverse the procedure.</para>
	</listitem>
      </itemizedlist>
      <para>In the event that of a catastrophic outage on the active
      site, it can be expected that the site is off line and no longer
      replicated to the backup site. In such an event:</para>
      <itemizedlist>
	<listitem>
	  <para>Connect to the cluster on the still-functioning site
	    resources, and change the affected DRBD resource's
	    <code>target-role</code> attribute from <code>Slave</code>
	    to <code>Master</code>. This will promote the DRBD
	    resources, and start any other Pacemaker resources
	    depending on the Primary role of the DRBD resource.</para>
	</listitem>
	<listitem>
	  <para>When the original site is restored or rebuilt, you may
	  connect the DRBD resources again, and subsequently fail back
	  using the reverse procedure.</para>
	</listitem>
      </itemizedlist>
    </section>
  </section>
</chapter>
