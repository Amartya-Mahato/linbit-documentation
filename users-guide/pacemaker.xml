<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-pacemaker" xmlns:xi="http://www.w3.org/2001/XInclude" status="draft">
  <title>Integrating DRBD with Pacemaker clusters</title>
  <indexterm>
    <primary>Pacemaker</primary>
  </indexterm>
  <para>Using DRBD in conjunction with the OpenAIS/Pacemaker cluster
    stack is arguably DRBD's most frequently found use case. Pacemaker
    is also one of the applications that make DRBD extremely powerful
    in a wide variety of usage scenarios. Hence, this is one of the
    more detailed chapters in this guide.</para>
  <para>This chapter describes using DRBD as replicated storage for
    Pacemaker High Availability clusters.</para>
  <important>
    <para>This chapter is relevant for Pacemaker versions 1.0.3 and
      above, and DRBD version 8.3.2 and above. It does not touch upon
      DRBD configuration in Pacemaker clusters of earlier
      versions.</para>
  </important>
  <note>
    <para>OpenAIS/Pacemaker is the direct, logical successor to the
      Heartbeat 2 cluster stack, and as far as the cluster resource
      manager infrastructure is concerned, a direct continuation of
      the Heartbeat 2 codebase. Since the intial stable release of
      OpenAIS/Pacemaker, Heartbeat 2 can be considered obsolete and
      Pacemaker should be used instead.</para>
    <para>For legacy configurations where Heartbeat must still be
      used, see <xref
	linkend="ch-heartbeat"/>.</para>
  </note>
  <section id="s-pacemaker-primer">
    <title>Pacemaker primer</title>
    <section id="s-pacemaker-cluster-manager">
      <title>The Pacemaker cluster manager</title>
      <para><indexterm>
	  <primary>Pacemaker</primary>
	  <secondary>cluster manager</secondary>
      </indexterm>Pacemaker's purpose as a cluster manager is to
	ensure that the cluster maintains its services to the clients,
	even if single machines of the cluster fail. Applications that
	may be managed by Pacemaker as cluster services include, for
	example,
	<itemizedlist>
	  <listitem>
	    <para>a web server such as Apache,</para>
	  </listitem>
	  <listitem>
	    <para>a database server such as MySQL, Oracle, or
	      PostgreSQL,</para>
	  </listitem>
	  <listitem>
	    <para>a file server such as NFS or Samba, and many
	      others.</para>
	  </listitem>
	</itemizedlist>In essence, any server application may be
	managed by Pacemaker as a cluster service.</para>
      <para>Services managed by Pacemaker are typically removed from
	the system startup configuration; rather than being started at
	boot time, the cluster manager starts and stops them as
	required by the cluster configuration and status. If a machine
	(a physical cluster node) fails while running a particular set
	of services, Pacemaker will start the failed services on
	another machine in the cluster. These operations performed by
	Pacemaker are commonly referred to as (automatic)
	<indexterm>
	  <primary>fail-over</primary>
	</indexterm>
	<emphasis>fail-over</emphasis>.</para>
      <para>Moving cluster services from one cluster node to
	another, by manual intervention, is commonly termed "manual
	fail-over". This being a slightly self-contradictory term, the
	in Pacemaker terminology such an action is referred to as a
	<indexterm>
	  <primary>migration</primary>
	</indexterm><indexterm>
	  <primary>cluster resource</primary>
	  <secondary>migration</secondary>
	  <see>migration</see>
	</indexterm>
	<emphasis>resource migration</emphasis>, or simply
      <emphasis>migration</emphasis> for short. We use that same
      terminology throughout this chapter.</para>
      <para>Pacemaker is also capable of automatically migrating
	resources back to a previously failed node, as soon as the
	latter recovers (if this is desired). This process is
	called
	<indexterm>
	  <primary>fail-back</primary>
	</indexterm><emphasis>fail-back</emphasis>.</para>
    </section>
    <section id="s-pacemaker-resources">
      <title>Pacemaker resources</title>
      <para><indexterm>
	  <primary>Pacemaker</primary>
	  <secondary>resources</secondary>
      </indexterm>
      <indexterm>
	  <primary>resource (Pacemaker)</primary>
      </indexterm>Usually, there will be certain requirements in order
	to be able to start a cluster service managed by Pacemaker on
	a node. Consider the example of a typical database-driven web
	application:
	<itemizedlist>
	  <listitem>
	    <para>Both the web server and the database server assume
	      that their designated <emphasis>IP addresses</emphasis>
	      are available (i.e. configured) on the node.</para>
	  </listitem>
	  <listitem>
	    <para>The database will require a <emphasis>file
		system</emphasis> to retrieve data files from.</para>
	  </listitem>
	  <listitem>
	    <para>That file system will require its underlying
	      <emphasis>block device</emphasis> to read from and write
	      to (this is where DRBD comes in, as we will see
	      later).</para>
	  </listitem>
	  <listitem>
	    <para>The web server will also depend on the database
	      being started, assuming it cannot serve dynamic content
	      without an available database.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <para>The services Pacemaker controls, and any additional
	requirements those services depend on, are referred to as
	<emphasis>resources</emphasis> in Pacemaker terminology. Where
	resources form a co-dependent collection, that collection is
	called a <emphasis>resource group</emphasis>.</para>
    </section>
    <section id="s-pacemaker-resource-agents">
      <title>Pacemaker resource agents</title>
      <para><indexterm>
	<primary>Pacemaker</primary>
	<secondary>resource agent</secondary>
      </indexterm>
      <indexterm>
	<primary>resource agent (Pacemaker)</primary>
      </indexterm>
	Pacemaker manages resources by way of invoking
	standardized shell scripts known as <emphasis>resource
	  agents</emphasis> (RA's). In Pacemaker clusters,
	the following resource agent types are available:
	<itemizedlist>
	  <listitem>
	    <formalpara id="fp-pacemaker-heartbeat-compatible-ra">
	      <title>Heartbeat 1 compatible resource agents</title>
	      <para>These agents are commonly found in the
		<filename>/etc/ha.d/resource.d</filename> directory.
		They are supported in Pacemaker for compatibility
	      reasons only, and should generally not be used in
	      production clusters.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-pacemaker-lsb-ra">
	      <title>LSB resource agents</title>
	      <para>These are conventional, Linux Standard
		Base-compliant init scripts found in
		<filename>/etc/init.d</filename>, which Pacemaker
		simply invokes with the <code>start</code>,
		<code>stop</code>, or <code>status</code> argument.
		They take no positional parameters. Thus, the
		corresponding resources' configuration cannot be
		managed by Pacemaker; these services are expected to
		be configured by conventional configuration
		files.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-pacemaker-ocf-ra">
	      <title>OCF resource agents</title>
	      <para>These are resource agents that conform to the
		guidelines of the Open Cluster Framework. They are
		usually found in either
		<filename>/usr/lib/ocf/resource.d</filename> or
		<filename>/usr/lib64/ocf/resource.d</filename>,
		depending on system architecture and distribution.
		They are grouped by <emphasis>providers</emphasis>,
		where each provider corresponds to one subdirectory in
		the aforementioned directory. They take no positional
		parameters, but may be extensively configured via
		environment variables that the cluster management
		process derives from the cluster configuration, and
		passes in to the resource agent upon
		invocation.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-openais-communication-channels">
      <title>OpenAIS communication channels</title>
      <para><indexterm>
	  <primary>OpenAIS</primary>
	  <secondary>communication channels</secondary>
      </indexterm>
      <indexterm>
	  <primary>communication channels (OpenAIS)</primary>
      </indexterm>OpenAIS uses a UDP multicast based communication
	protocol to periodically check for node availability. This
	communcation protocol may be configured to use multiple
	network paths using the <emphasis>Redundant Ring
	  Protocol</emphasis> (<acronym>RRP</acronym>). The absolute
	minimum requirement for stable cluster operation is two
	independent communication channels in a redundant ring.
	<important>
	  <para>A bonded network interface (a virtual aggregation of
	    physical interfaces using the <indexterm>
	      <primary>bonding driver</primary>
	    </indexterm><code>bonding</code> driver) constitutes
	    <emphasis>one</emphasis> Pacemaker communication
	    channel.</para>
	  <para>Bonded links are not protected against bugs, known or
	    as-yet-unknown, in the <code>bonding</code> driver. Also,
	    bonded links are typically formed using identical network
	    interface models, thus they are vulnerable to bugs in the
	    NIC driver as well. Any such issue could lead to a cluster
	    partition if no independent second OpenAIS communication
	    channel were available.</para>
	  <para>It is thus <emphasis>not</emphasis> acceptable to omit
	    the inclusion of a second OpenAIS communication link in
	    the cluster configuration just because the first uses a
	    bonded interface.
	  </para>
	</important>
      </para>
    </section>
  </section>
  <section id="s-pacemaker-config">
    <title>Pacemaker configuration</title>
    <para><indexterm>
	<primary>Pacemaker</primary>
	<secondary>configuration</secondary>
      </indexterm>For any Pacemaker cluster, the following
      configuration files must be available:
      <itemizedlist>
	<listitem>
	  <para><indexterm>
	      <primary>openais.conf (OpenAIS configuration file)</primary>
	    </indexterm><filename>/etc/ais/openais.conf</filename> &mdash;
	    global cluster configuration.</para>
	</listitem>
	<listitem>
	  <para><indexterm>
	      <primary>authkey (OpenAIS configuration
		file)</primary>
	    </indexterm><filename>/etc/ais/authkey</filename>
	    &mdash; shared secret for mutual node authentication.</para>
	</listitem>
      </itemizedlist>
    </para>
    <section id="s-pacemaker-openais-conf">
      <title>The <filename>openais.conf</filename> file</title>
      <para><indexterm>
	<primary>openais.conf (OpenAIS configuration file)</primary>
      </indexterm>The following example is an example 
	<filename>openais.conf</filename> file:
<programlisting id="pl-openais-conf">totem {
  version: 2
  token: 3000
  token_retransmits_before_loss_const: 10
  join: 60
  consensus: 1500
  vsftype: none
  max_messages: 20
  clear_node_high_bit: yes
  secauth: on
  threads: 0
  rrp_mode: passive
  interface {
    ringnumber: 0
    bindnetaddr: 192.168.122.0
    mcastaddr: 239.94.1.1
    mcastport: 5405
  }
  interface {
    ringnumber: 1
    bindnetaddr: 192.168.133.0
    mcastaddr: 239.94.2.1
    mcastport: 5405
  }
}
logging {
  to_stderr: yes
  debug: on
  timestamp: on
  to_file: no
  to_syslog: yes
  syslog_facility: daemon
}
amf {
  mode: disabled
}
service {
  ver:       0
  name:      pacemaker
  use_mgmtd: yes
}
aisexec {
  user:   root
  group:  root
}</programlisting></para>
      <para>This example assumes that <code>192.168.122.0</code> is
	the <emphasis>network address</emphasis> of the cluster's
	interface to the shared network, and that
	<code>192.168.133.0</code> is the network address of the
	interface dedicated for DRBD replication between both
	nodes. Do not mistakenly add <emphasis>host</emphasis> IP
	address in <code>interface</code> stanzas.</para>
    </section>
    <section id="s-pacemaker-authkey">
      <title>The <filename>authkey</filename> file</title>
      <para><indexterm>
	<primary>authkey (Pacemaker configuration file)</primary>
      </indexterm><filename>/etc/ais/authkey</filename> contains
	a pre-shared secrets used for mutual cluster node
	authentication. It should only be readable by
	<code>root</code>.</para>
      <para>You may create an <filename>authkey</filename> file by
      issuing the following command without arguments:</para>
      <literallayout><userinput>ais-keygen</userinput></literallayout>
    </section>
    <section id="s-pacemaker-ha-propagate">
      <title>Propagating the cluster configuration to cluster
      nodes</title>
      <para>In order to propagate the OpenAIS configuration, copy the
      two configuration files to the peer node in a secure
      fashion:</para>
      <literallayout><userinput>scp /etc/ais/{authkey,openais.conf} bob:/etc/ais</userinput></literallayout>
    </section>
  </section>
  <section id="s-pacemaker-crm">
    <title>Using DRBD in Pacemaker clusters</title>
    <section id="s-pacemaker-crm-config">
      <title>Pacemaker CRM configuration</title>
      <para>Pacemaker's cluster configuration is maintained in the
	<emphasis>Cluster Information Base</emphasis>
	(<acronym>CIB</acronym>), covered in detail in <link
	  linkend="s-pacemaker-cib">the following section</link>.
	Contrary to the two relevant configuration files, the CIB need
	not be manually distributed among cluster nodes; the Pacemaker
	services take care of that automatically.</para> 
      <section id="s-pacemaker-cib">
	<title>The Cluster Information Base</title>
	<para><indexterm>
	    <primary>Pacemaker</primary>
	    <secondary>Cluster Information Base (CIB)</secondary>
	</indexterm>
	<indexterm>
	    <primary>Cluster Information Base (CIB)</primary>
	    <see>Pacemaker</see>
	</indexterm>The Cluster Information Base
	  (<acronym>CIB</acronym>) is kept in one XML file,
	  <indexterm>
	    <primary>cib.xml (Pacemaker configuration file)</primary>
	  </indexterm>
	  <filename>/var/lib/heartbeat/crm/cib.xml</filename>. It is,
	  however, not recommended to edit the contents of this file
	  directly, except in the case of creating a new cluster
	  configuration from scratch. Instead, Pacemaker comes with
	  both command-line applications and a GUI to modify the CIB.
	</para>
	<para>The CIB actually contains both the cluster
	  <emphasis>configuration</emphasis> (which is persistent and
	  is kept in the <filename>cib.xml</filename> file), and
	  information about the current cluster
	  <emphasis>status</emphasis> (which is volatile). Status
	  information, too, may be queried either using Pacemaker
	  command-line tools, and the Pacemaker GUI.</para>
	<para>After creating a new Pacemaker cluster &mdash; that is,
	  creating the <filename>openais.conf</filename> and
	  <filename>authkey</filename> files, distributing them among
	  cluster nodes, starting OpenAIS services, and waiting for
	  nodes to establish intra-cluster communications &mdash; a
	  new, empty CIB is created automatically. You can display
	  this bootstrap CIB using the <code>crm configure show</code>
	  command. Its output will be similar to this:</para>
	<literallayout><userinput>crm configure show</userinput>
<computeroutput>node alice
node bob
property $id="cib-bootstrap-options" \
	dc-version="1.0.3-unknown" \
	last-lrm-refresh="1243855262" \
	cluster-infrastructure="openais" \
	expected-quorum-votes="2"
</computeroutput></literallayout>
	<para>Based on this bootstrap configuration, there are two
	  configuration changes that you should make straight away:</para>
	<orderedlist>
	  <listitem>
	    <para>In a two-node cluster, the concept of quorum does
	      not apply. It is thus safe to set Pacemaker's
	      <quote>no quorum policy</quote> to ignore loss of
	      quorum.</para>
	  </listitem>
	  <listitem>
	    <para>In a DRBD backed cluster, node fencing (STONITH)
	      is not necessary, as fencing is achieved through other
	      means at the resource level. It is thus safe to
	      disable STONITH in the Pacemaker cluster
	      configuration.</para>
	  </listitem>
	</orderedlist>
	<warning>
	  <para><emphasis>Do not</emphasis> construe these
	    recommendations to be generally applicable to high
	    availability clusters. They are
	    <emphasis>only</emphasis> applicable to the special case
	    of having a DRBD based cluster consisting of only two
	    nodes.</para>
	</warning>
	<para>You can set the appropriate cluster properties with the
	  following commands:
	  <literallayout><userinput>crm configure</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>property no-quorum-policy=ignore</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>property stonith-enabled=false</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>commit</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>exit</userinput>
<computeroutput>bye</computeroutput>
</literallayout></para>
	<para>After this, your cluster configuration should look similar to this:
	  <literallayout><userinput>crm configure show</userinput>
<computeroutput>node alice
node bob
property $id="cib-bootstrap-options" \
	dc-version="1.0.3-unknown" \
	no-quorum-policy="ignore" \
	last-lrm-refresh="1243855262" \
	stonith-enabled="false" \
	default-resource-stickiness="200" \
	cluster-infrastructure="openais" \
	expected-quorum-votes="2"
</computeroutput></literallayout></para>
      </section>
      <section id="s-pacemaker-crm-drbd-backed-service">
	<title>Adding a DRBD-backed service to the cluster
	configuration</title>
	<para>This section explains how to enable a DRBD-backed
	  service in a Pacemaker cluster.</para>
	<para>DRBD version 8.3.2 and above include the
	  <code>drbd</code> OCF resource agent which provides
	  Master/Slave capability, allowing Pacemaker to start and
	  monitor the DRBD resource on multiple nodes and promoting
	  and demoting as needed. You must, however, understand that
	  the <code>drbd</code> RA disconnects and detaches all DRBD
	  resources it manages on Pacemaker shutdown, and also upon
	  enabling standby mode for a node.</para>
	<note>
	  <para>The OCF resource agent which ships with DRBD is
	    belongs to the <code>drbd</code> provider, and hence
	    installs
	    <filename>/usr/lib/ocf/resource.d/drbd/drbd</filename>. There
	    is a legacy resource agent that shipped with Heartbeat 2,
	    which installs into
	    <filename>/usr/lib/ocf/resource.d/heartbeat/drbd</filename>. Using
	    the legacy OCF RA is not recommended.</para>
	</note>
	<para>In order to enable a DRBD-backed configuration for a
	  MySQL database in a Pacemaker CRM cluster with the
	  <code>drbd</code> OCF resource agent, you must create both
	  the necessary resources, and Pacemaker constraints to ensure
	  your service only starts on a previously promoted DRBD
	  resource. You may do so using the <code>crm</code> shell, as
	  outlined in the following example:</para>
	  <literallayout><userinput>crm configure</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive drbd_mysql ocf:drbd:drbd \
                    params drbd_resource="mysql" \
                    op monitor interval="15s"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>ms ms_drbd_mysql drbd_mysql \
                    meta master-max="1" master-node-max="1" \
                         clone-max="2" clone-node-max="1" \
                         notify="true"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive fs_mysql ocf:heartbeat:Filesystem \
                    params device="/dev/drbd/by-res/mysql" directory="/var/lib/mysql" fstype="ext3" \</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive ip_mysql ocf:heartbeat:IPaddr2 \
                    params ip="10.9.42.1" nic="eth0"</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>primitive mysqld lsb:mysqld</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>group mysql fs_mysql ip_mysql mysqld</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>colocation mysql_on_drbd inf: fs_mysql ms_drbd_mysql:Master</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>order mysql_after_drbd inf: ms_drbd_mysql:promote fs_mysql:start</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>commit</userinput>
<computeroutput>crm(live)configure# </computeroutput><userinput>exit</userinput>
<computeroutput>bye</computeroutput></literallayout>
	<para>After this, your configuration should be enabled.
	  Pacemaker now selects a node on which it promotes the DRBD
	  resource, and then starts the DRBD-backed resource group on
	  that same node.</para>
      </section>
    </section>
    <section id="s-pacemaker-crm-manage">
      <title>Managing Pacemaker CRM clusters</title>
      <section id="s-pacemaker-crm-assume-resources">
	<title>Assuming control of cluster resources</title>
	<para>A Pacemaker CRM cluster node may assume control of
	 cluster resources in the following ways:</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Manual takeover of a single cluster
		resource</title>
	      <para>This is the approach normally taken if one simply
		wishes to test resource migration, or move a resource
		to the local node as a means of manual load balancing.
		Assuming we want to move the resource group named
		<code>mysql</code> to the node <code>alice</code>,
		this operation is performed using the following
		command:
		<literallayout><userinput>crm resource migrate mysql alice</userinput></literallayout>
		<note id="n-pacemaker-crm-migrate">
		  <para>It is important to understand that the
		    migration is <emphasis>permanent</emphasis>. That
		    is, unless told otherwise, Pacemaker will not move
		    the resource back to a node it was previouly
		    migrated away from &mdash; even if that node
		    happens to be the only surviving node in a
		    near-cluster-wide system failure. This is
		    undesirable under most circumstances. So, it is
		    prudent to immediately <quote>un-migrate</quote>
		    resources after successful migration, using the
		    the following command:
		    <literallayout><userinput>crm resource unmigrate mysql</userinput></literallayout></para>
		</note>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Manual takeover of all cluster resources</title>
	      <para>This procedure involves switching the peer node to
		standby mode. Assuming we want to switch node
		<code>bob</code> into standby mode, we would do so by
		issuing the following command:
		<literallayout><userinput>crm node standby bob</userinput></literallayout>
	      </para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
      <section id="s-pacemaker-crm-relinquish-resources">
	<title>Relinquishing cluster resources</title>
	<para>A Pacemaker CRM cluster node may be forced to give up
	  one or all of its resources in several ways.</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Giving up a single cluster resource</title>
	      <para>A node gives up control of a single resource when
		issued the following command:
		<literallayout><userinput>crm resource migrate mysql</userinput></literallayout>
		Using <code>crm resource migrate</code> with
	      just the resource name (and without an explicit host
	      name to migrate to) implies that the resource is to be
	      migrated <emphasis>away</emphasis> from where it is
	      currently running. Please note that <link
		linkend="n-pacemaker-crm-migrate">the considerations
		outlined in the previous section</link> apply here,
	      too.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Switching a cluster node to standby
		mode</title>
	      <para>This procedure involves switching the local node to
		standby mode. We do so by issuing the following command:
		<literallayout><userinput>crm node standby `uname -n`</userinput></literallayout>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Shutting down the local cluster manager
		instance</title>
	      <para>This approach is suited for local maintenance
		operations such as software updates which require that
		the node be temporarily removed from the cluster, but
		which do not necessitate a system reboot. To shut down
		cluster services on the local node, issue the
		following command:
		<literallayout><userinput>/etc/init.d/openais stop</userinput></literallayout>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Shutting down the local node</title>
	      <para>For hardware maintenance or other interventions
		that require a system shutdown or reboot, use a simple
		graceful shutdown command:
		<literallayout><userinput>shutdown -h now</userinput></literallayout>
		or
		<literallayout><userinput>poweroff</userinput></literallayout>
	      </para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
    </section>
  </section>
  <section id="s-pacemaker-fencing">
    <title>Using resource-level fencing in Pacemaker clusters</title>
    <para>This section outlines the steps necessary to prevent
      Pacemaker from promoting a <code>drbd</code> Master/Slave
      resource when its DRBD replication link has been interrupted.
      This keeps Pacemaker from starting a service with outdated data
      and causing an unwanted <quote>time warp</quote> in the process.
      <important>
	<para>It is absolutely vital to configure at least two
	  independent <link
	    linkend="s-pacemaker-communication-channels">Pacemaker
	    communication channels</link> for this functionality to
	  work correctly.</para>
      </important>
    </para>
    <para>In order to enable resource-level fencing for Pacemaker, you
      will have to set two options in
      <filename>drbd.conf</filename>. Assuming your DRBD resource is
      named <code>mysql</code>, and the corresponding Pacemaker
      Master/Slave resource is named <code>ms_drbd_mysql</code>, you would
      change your DRBD configuration as follows:
      <programlisting>resource mysql {
  disk {
    fencing resource-only;
    ...
  }
  handlers {
    fence-peer "/usr/lib/drbd/crm-fence-peer.sh ms_drbd_mysql";
    after-resync-target "/usr/lib/drbd/crm-unfence-peer.sh ms_drbd_mysql";
    ...
  }
  ...
}</programlisting>
      Thus, if the DRBD replication link becomes disconnected, the
      <filename>crm-fence-peer.sh</filename> script (which is a
      wrapper invoking the <code>crm</code> shell) contacts the
      cluster manager and ensures that the <code>ms_drbd_mysql</code>
      resource no longer gets promoted on any node other than the
      currently active one. Conversely, when the connection is
      re-established and DRBD completes its synchronization process,
      then that constraint is removed and the cluster manager is free
      to promote the resource on any node again.
    </para>
    <para>If you adopt a Pacemaker resource naming convention that
      incorporates the DRBD resource name, then you may also configure
      this functionality for <emphasis>all</emphasis> resources
      defined in <filename>drbd.conf</filename>, via the
      <code>common</code> section. The example below uses a convention
      where it is assumed that all Pacemaker DRBD Master/Slave
      resources in the cluster configuration use the prefix
      <code>ms_drbd_</code>:
      <programlisting>common {
  disk {
    fencing resource-only;
    ...
  }
  handlers {
    fence-peer "/usr/lib/drbd/crm-fence-peer.sh ms_drbd_${DRBD_RESOURCE}";
    after-resync-target "/usr/lib/drbd/crm-unfence-peer.sh ms_drbd_${DRBD_RESOURCE}";
  ...
}
</programlisting>
    </para>
  </section>
  <section id="s-pacemaker-stacked-resources">
    <title>Using stacked DRBD resources in Pacemaker clusters</title>
    <para>Stacked resources allow DRBD to be used for multi-level
    redundancy in multiple-node clusters, or to establish off-site
    disaster recovery capability. This section describes how to set
    configure DRBD and Pacemaker in such configurations.</para>
    <section>
      <title>Adding off-site disaster recovery to Pacemaker
      clusters</title>
      <para>In this configuration scenario, we would deal with a
	two-node high availability cluster in one site, plus a
	separate node which would presumably be housed off-site. The
	third node acts as a disaster recovery node and is a
	standalone server. Consider the following illustration to
	describe the concept.</para>
      <xi:include href="todo.xml"/>
      <para>In this example, <code>alice</code> and <code>bob</code>
	form a two-node Pacemaker cluster, whereas
      <code>charlie</code> is an off-site node not managed by
      Pacemaker.</para>
      <para>To create such a configuration, you would first 
	configure and initialize DRBD resources as described in <xref
      linkend="s-three-nodes"/>. Then, configure Pacemaker with the
      following CRM configuration:</para>
      <programlisting>primitive drbd_r0 ocf:drbd:drbd \
	params drbd_resource="r0"

primitive drbd_r0-U ocf:drbd:drbd \
	params drbd_resource="r0-U" \

primitive ip_stacked ocf:heartbeat:IPaddr2 \
	params ip="192.168.42.1" nic="eth0"

ms ms_drbd_r0 res_drbd_r0 \
	meta master-max="1" master-node-max="1" \
        clone-max="2" clone-node-max="1" \
        notify="true" globally-unique="false"

ms ms_drbd_r0-U res_drbd_r0-U \
	meta master-max="1" clone-max="1" \
        clone-node-max="1" master-node-max="1" \
        notify="true" globally-unique="false"

colocation c_drbd_r0-U_on_drbd_r0 \ 
        inf: ms_drbd_r0-U ms_drbd_r0:Master

colocation c_drbd_r0-U_on_ip \
        inf: ms_drbd_r0-U res_ip_stacked

colocation c_ip_on_r0_master \
        inf: res_ip_stacked ms_drbd_r0:Master

order o_ip_before_r0-U \
        inf: res_ip_stacked ms_drbd_r0-U:start

order o_drbd_r0_before_r0-U \
        inf: ms_drbd_r0:promote ms_drbd_r0-U:start
</programlisting>
      <para>Assuming you created this configuration in a temporary
        file named <filename>/tmp/crm.txt</filename>, you may import
        it into the live cluster configuration with the following
        command:
	<literallayout><userinput>crm configure &lt; /tmp/crm.txt</userinput></literallayout>
      </para>
      <para>This configuration will ensure that the following actions
        occur in the correct order on the
        <code>alice</code>/<code>bob</code> cluster:</para>
      <orderedlist>
	<listitem>
	  <para>Pacemaker starts the DRBD resource <code>r0</code> on
	  both cluster nodes, and promotes one node to the Master
	  (DRBD Primary) role.</para>
	</listitem>
	<listitem>
	  <para>Pacemaker then starts the IP address 192.168.42.1,
	    which the stacked resource is to use for replication to
	    the third node. It does so on the node it has previously
	    promoted to the Master role for <code>r0</code> DRBD
	    resource.</para>
	</listitem>
	<listitem>
	  <para>On the node which now has the Primary role for
	    <code>r0</code> and also the replication IP address for
	    <code>r0-U</code>, Pacemaker now starts the
	    <code>r0-U</code> DRBD resource, which connects and
	    replicates to the off-site node.</para>
	</listitem>
	<listitem>
	  <para>Pacemaker then promotes the <code>r0-U</code> resource
	  to the Primary role too, so it can be used by an
	  application.</para>
	</listitem>
      </orderedlist>
      <para>Thus, this Pacemaker configuration ensures that there is
	not only full data redundancy between cluster nodes, but also
	to the third, off-site node.</para>
      <note>
	<para>This type of setup is usually deployed together with
	  <link linkend="s-drbd-proxy">DRBD Proxy</link>.</para>
      </note>
    </section>
  </section>
</chapter>
