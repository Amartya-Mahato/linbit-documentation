<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-ocfs2">
  <title>Using OCFS2 with DRBD</title>
  <indexterm>
    <primary>OCFS2</primary>
  </indexterm>
  <indexterm>
    <primary>Oracle Cluster File System</primary>
    <see>OCFS</see>
  </indexterm>
  <para>This chapter outlines the steps necessary to set up a DRBD
  resource as a block device holding a shared Oracle Cluster File
  System, version 2 (OCFS2).</para>
  <para>In order to use OCFS2 on top of DRBD, you must configure DRBD in
    <indexterm>
      <primary>dual-primary mode</primary>
    </indexterm>
    dual-primary mode, which is available in DRBD 8.0 and
    later.</para>
  <section id="s-ocfs2-primer">
    <title>OCFS2 primer</title>
    <para>The Oracle Cluster File System, version 2 (OCFS2) is a
      concurrent access shared storage file system developed by Oracle
      Corporation. Unlike its predecessor OCFS, which was specifically
      designed and only suitable for Oracle database payloads, OCFS2
      is a general-purpose filesystem that implements most POSIX
      semantics. The most common use case for OCFS2 is arguably Oracle
      Real Application Cluster (RAC), but OCFS2 may also be used for
      load-balanced NFS clusters, for example.</para>
    <para>Although originally designed for use with conventional
      shared storage devices, OCFS2 is equally well suited to be
      deployed on <link linkend="s-dual-primary-mode">dual-Primary
	DRBD</link>. Applications reading from the filesystem may
      benefit from reduced read latency due to the fact that DRBD
      reads from and writes to local storage, as opposed to the SAN
      devices OCFS2 otherwise normally runs on. In addition, DRBD adds
      redundancy to OCFS2 by adding an additional copy to every
      filesystem image, as opposed to just a single filesystem image
      that is merely shared.</para>
    <para>Like other shared cluster file systems such as <link
	linkend="ch-gfs">GFS</link>, OCFS2 allows multiple nodes to
      access the same storage device, in read/write mode,
      simultaneously without risking data corruption. It does so by
      using a Distributed Lock Manager (DLM) which manages concurrent
      access from cluster nodes. The DLM itself uses a virtual file
      system (<code>ocfs2_dlmfs</code>) which is separate from the
      actual OCFS2 file systems present on the system.</para>
    <para>OCFS2 may either use an intrinsic cluster communication
      layer to manage cluster membership and filesystem mount and
      unmount operation, or alternatively defer those tasks to the
      <link linkend="ch-heartbeat">Linux-HA (Heartbeat)</link> cluster
      infrastructure.</para>
    <para>OCFS2 is available in SUSE Linux Enterprise Server (where it
      is the primarily supported shared cluster file system), CentOS,
      Debian GNU/Linux, and Ubuntu Server Edition. Oracle also
      provides packages for Red Hat Enterprise Linux (RHEL). This
      chapter assumes running OCFS2 on a SUSE Linux Enterprise Server
      system.</para>
  </section>
  <section id="s-ocfs-create-resource">
    <title>Creating a DRBD resource suitable for OCFS2</title>
    <para>Since OCFS2 is a shared cluster file system expecting
      concurrent read/write storage access from all cluster nodes, any
      DRBD resource to be used for storing a OCFS2 filesystem must be
      configured in <link linkend="s-dual-primary-mode">dual-primary
	mode</link>. Also, it is recommended to use some of DRBD's
      <link linkend="s-automatic-split-brain-recovery-configuration">features for
    automatic recovery from split brain</link>. And, it is necessary
    for the resource to switch to the primary role immediately after
    startup. To do all this, include the following lines in the
    resource configuration:
      <indexterm>
	<primary>drbd.conf</primary>
	<secondary>resource</secondary>
      </indexterm>
      <programlisting>resource <replaceable>resource</replaceable> {
  startup {
    become-primary-on both;
    ...
  }
  net {
    # allow-two-primaries;
    after-sb-0pri discard-zero-changes;
    after-sb-1pri discard-secondary;
    after-sb-2pri disconnect;
    ...
  }
  ...
}</programlisting>
      It is not recommended to enable the <option>allow-two-primaries</option>
      option upon initial configuration. You should do so after the initial
      resource synchronization has completed.
    </para>
    <para>
      Once you have added these options to <link
	linkend="ch-configure">your freshly-configured
	resource</link>, you may <link
	linkend="s-first-time-up">initialize your resource as you
	normally would</link>. After you enable the
      <indexterm>
	<primary>drbd.conf</primary>
	<secondary>allow-two-primaries</secondary>
      </indexterm>
      <option>allow-two-primaries</option> option for this
      resource, you will be able to <link
	linkend="s-switch-resource-roles">promote the resource</link>
      to the primary role on both nodes.</para>
  </section>
  <section id="s-ocfs2-enable">
    <title>Configuring your cluster to support OCFS2</title>
    <section id="s-ocfs2-create-cluster-conf">
      <title>Creating the configuration file</title>
      <para>OCFS2 uses a central configuration file,
	<filename>/etc/ocfs2/cluster.conf</filename>.</para>
      <para>When creating your OCFS2 cluster, be sure to add both your
	hosts to the cluster configuration. The default port (7777) is
	usually an acceptable choice for cluster interconnect
	communications. If you choose any other port number, be sure to
	choose one that does not clash with an existing port used by
	DRBD (or any other configured TCP/IP).</para>
      <para>If you feel less than comfortable editing the
	<filename>cluster.conf</filename> file directly, you may also
	use the <code>ocfs2console</code> graphical configuration
	utility which is usually more convenient. Regardless of the
	approach you selected, your
	<filename>/etc/ocfs2/cluster.conf</filename> file contents should
	look roughly like this:
	<programlisting>node:
    ip_port = 7777
    ip_address = 10.1.1.31
    number = 0
    name = alice
    cluster = ocfs2

node:
    ip_port = 7777
    ip_address = 10.1.1.32
    number = 1
    name = bob
    cluster = ocfs2

cluster:
    node_count = 2
    name = ocfs2
</programlisting></para>
      <para>When you have configured you cluster configuration, use
	<code>scp</code> to distribute the configuration to both nodes
	in the cluster.</para>
    </section>
    <section id="s-configure-o2cb-driver">
      <title>Configuring the O2CB driver</title>
      <itemizedlist>
	<listitem>
	  <formalpara>
	    <title>SUSE Linux Enterprise systems</title>
	    <para>On SLES, you may utilize the <option>configure</option>
	      option of the <code>o2cb</code> init script:
	      <literallayout><userinput>/etc/init.d/o2cb configure</userinput>
<computeroutput>Configuring the O2CB driver.

This will configure the on-boot properties of the O2CB driver.
The following questions will determine whether the driver is loaded on
boot.  The current values will be shown in brackets ('[]').  Hitting
&lt;ENTER&gt; without typing an answer will keep that current value.  Ctrl-C
will abort.

Load O2CB driver on boot (y/n) [y]:
Cluster to start on boot (Enter "none" to clear) [ocfs2]:
Specify heartbeat dead threshold (>=7) [31]:
Specify network idle timeout in ms (>=5000) [30000]:
Specify network keepalive delay in ms (>=1000) [2000]:
Specify network reconnect delay in ms (>=2000) [2000]:
Use user-space driven heartbeat? (y/n) [n]:
Writing O2CB configuration: OK
Loading module "configfs": OK
Mounting configfs filesystem at /sys/kernel/config: OK
Loading module "ocfs2_nodemanager": OK
Loading module "ocfs2_dlm": OK
Loading module "ocfs2_dlmfs": OK
Mounting ocfs2_dlmfs filesystem at /dlm: OK
Starting O2CB cluster ocfs2: OK</computeroutput></literallayout>
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title>Debian GNU/Linux systems</title>
	    <para>On Debian, the <code>configure</code> option to
	      <filename>/etc/init.d/o2cb</filename> is not
	      available. Instead, reconfigure the <code>ocfs2-tools</code>
	    package to enable the driver:
	      <literallayout><userinput>dpkg-reconfigure -p medium -f readline ocfs2-tools</userinput>
<computeroutput>Configuring ocfs2-tools</computeroutput>
<computeroutput>-----------------------</computeroutput>
<computeroutput>Would you like to start an OCFS2 cluster (O2CB) at boot time? </computeroutput><userinput>yes</userinput>
<computeroutput>Name of the cluster to start at boot time: </computeroutput><userinput>ocfs2</userinput>
<computeroutput>The O2CB heartbeat threshold sets up the maximum time in seconds that a node
awaits for an I/O operation. After it, the node "fences" itself, and you will
probably see a crash.

It is calculated as the result of: (threshold - 1) x 2.

Its default value is 31 (60 seconds).

Raise it if you have slow disks and/or crashes with kernel messages like:

o2hb_write_timeout: 164 ERROR: heartbeat write timeout to device XXXX after NNNN
milliseconds</computeroutput>
<computeroutput>O2CB Heartbeat threshold: </computeroutput><userinput>31</userinput>
		<computeroutput>Loading filesystem "configfs": OK
Mounting configfs filesystem at /sys/kernel/config: OK
Loading stack plugin "o2cb": OK
Loading filesystem "ocfs2_dlmfs": OK
Mounting ocfs2_dlmfs filesystem at /dlm: OK
Setting cluster stack "o2cb": OK
Starting O2CB cluster ocfs2: OK</computeroutput>
</literallayout>
	    </para>
	  </formalpara>
	</listitem>
      </itemizedlist>
    </section>
  </section>
  <section id="s-ocfs2-create">
    <title>Creating an OCFS2 filesystem</title>
    <para>When your cluster configuration is complete and you have
    configure O2CB on both hosts, you can create the actual file
    system.</para>
    <para>To that end, first check your cluster status:
      <literallayout><userinput>/etc/init.d/o2cb status</userinput>
	<computeroutput>Module "configfs": Loaded
Filesystem "configfs": Mounted
Module "ocfs2_nodemanager": Loaded
Module "ocfs2_dlm": Loaded
Module "ocfs2_dlmfs": Loaded
Filesystem "ocfs2_dlmfs": Mounted
Checking O2CB cluster ocfs2: Online
Heartbeat dead threshold = 31
  Network idle timeout: 30000
  Network keepalive delay: 2000
  Network reconnect delay: 2000
Checking O2CB heartbeat: Not active</computeroutput></literallayout>
    </para>
    <para>Now, use OCFS2's <code>mkfs</code> implementation to create
    the file system:
      <literallayout><userinput>mkfs -t ocfs2 -N 2 -L ocfs2_drbd0 /dev/drbd0</userinput>
	<computeroutput>mkfs.ocfs2 1.4.0
Filesystem label=ocfs2_drbd0
Block size=1024 (bits=10)
Cluster size=4096 (bits=12)
Volume size=205586432 (50192 clusters) (200768 blocks)
7 cluster groups (tail covers 4112 clusters, rest cover 7680 clusters)
Journal size=4194304
Initial number of node slots: 2
Creating bitmaps: done
Initializing superblock: done
Writing system files: done
Writing superblock: done
Writing backup superblock: 0 block(s)
Formatting Journals: done
Writing lost+found: done
mkfs.ocfs2 successful
</computeroutput>
</literallayout>
      This will create an OCFS2 file system with two node slots on
      <filename>/dev/drbd0</filename>, and set the filesystem label to
      <code>ocfs2_drbd0</code>. You may specify other options on
      <code>mkfs</code> invocation; please see the
      <code>mkfs.ocfs2</code> system manual page for details.
    </para>
    <para>After formatting, opening <code>ocfs2console</code> should
      display your newly created file system, albeit without a mount
      point.</para>
  </section>
  <section id="s-ocfs2-use">
    <title>Using your OCFS2 filesystem</title>
    <para>When you have completed cluster configuration and created
    your file system, you may mount it as any other file system:
      <literallayout><userinput>mount -t ocfs2 /dev/drbd0 /shared</userinput></literallayout>
      Your kernel log (accessible by issuing the command
    <code>dmesg</code>) should then contain a line similar to this
    one:
      <programlisting>ocfs2: Mounting device (147,0) on (node 0, slot 0) with ordered data mode.</programlisting>
    </para>
    <para>From that point forward, you should be able to
    simultaneously mount your OCFS2 filesystem on both your nodes,
    in read/write mode.</para>
  </section>
</chapter>
