<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-heartbeat">
  <title>Integrating DRBD with Heartbeat clusters</title>
  <indexterm>
    <primary>Heartbeat</primary>
  </indexterm>
  <para>
    <important>
      <para>This chapter talks about DRBD in combination with the legacy
      Linux-HA cluster manager found in Heartbeat 2.0 and 2.1. That
      cluster manager has been superseded by Pacemaker and the latter
      should be used whenever possible &mdash; please see <xref
      linkend="ch-pacemaker"/> for more information. This
      chapter outlines legacy Heartbeat configurations and is intended for
      users who must maintain existing legacy Heartbeat systems for
      policy reasons.</para>
      <para>The Heartbeat <emphasis>cluster messaging
      layer</emphasis>, a distinct part of the Linux-HA project that
      continues to be supported as of Heartbeat version 3, is fine to
      use in conjunction with the Pacemaker cluster manager. More
      information about configuring Heartbeat can be found as part of
      the Linux-HA User's Guide at <ulink
      url="http://www.linux-ha.org/doc/"/>.</para>
    </important>
  </para>
  <section id="s-heartbeat-primer">
    <title>Heartbeat primer</title>
    <section id="s-heartbeat-cluster-manager">
      <title>The Heartbeat cluster manager</title>
      <para><indexterm>
	  <primary>Heartbeat</primary>
	  <secondary>cluster manager</secondary>
      </indexterm>Heartbeat's purpose as a cluster manager is to
	ensure that the cluster maintains its services to the clients,
	even if single machines of the cluster fail. Applications that
	may be managed by Heartbeat as cluster services include, for
	example,
	<itemizedlist>
	  <listitem>
	    <para>a web server such as Apache,</para>
	  </listitem>
	  <listitem>
	    <para>a database server such as MySQL, Oracle, or
	      PostgreSQL,</para>
	  </listitem>
	  <listitem>
	    <para>a file server such as NFS or Samba, and many
	      others.</para>
	  </listitem>
	</itemizedlist>In essence, any server application may be
	managed by Heartbeat as a cluster service.</para>
      <para>Services managed by Heartbeat are typically removed from
	the system startup configuration; rather than being started at
	boot time, the cluster manager starts and stops them as
	required by the cluster configuration and status. If a machine
	(a physical cluster node) fails while running a particular set
	of services, Heartbeat will start the failed services on
	another machine in the cluster. These operations performed by
	Heartbeat are commonly referred to as (automatic)
	<indexterm>
	  <primary>fail-over</primary>
	</indexterm>
	<emphasis>fail-over</emphasis>.</para>
      <para>A migration of cluster services from one cluster node to
	another, by manual intervention, is commonly termed "manual
	fail-over". This being a slightly self-contradictory term, we
	use the alternative term <indexterm>
	  <primary>switch-over</primary>
	</indexterm><indexterm>
	  <primary>fail-over</primary>
	  <secondary>manual</secondary>
	  <see>switch-over</see>
	</indexterm><emphasis>switch-over</emphasis> for the purposes
	of this guide.</para>
      <para>Heartbeat is also capable of automatically migrating
	resources back to a previously failed node, as soon as the
	latter recovers. This process is called<indexterm>
	  <primary>fail-back</primary>
	</indexterm>
	<emphasis>fail-back</emphasis>.</para>
    </section>
    <section id="s-heartbeat-resources">
      <title>Heartbeat resources</title>
      <para><indexterm>
	  <primary>Heartbeat</primary>
	  <secondary>resources</secondary>
      </indexterm>
      <indexterm>
	  <primary>resource (Heartbeat)</primary>
      </indexterm>Usually, there will be certain requirements in order
	to be able to start a cluster service managed by Heartbeat on
	a node. Consider the example of a typical database-driven web
	application:
	<itemizedlist>
	  <listitem>
	    <para>Both the web server and the database server assume
	      that their designated <emphasis>IP addresses</emphasis>
	      are available (i.e. configured) on the node.</para>
	  </listitem>
	  <listitem>
	    <para>The database will require a <emphasis>file
		system</emphasis> to retrieve data files from.</para>
	  </listitem>
	  <listitem>
	    <para>That file system will require its underlying
	      <emphasis>block device</emphasis> to read from and write
	      to (this is where DRBD comes in, as we will see
	      later).</para>
	  </listitem>
	  <listitem>
	    <para>The web server will also depend on the database
	      being started, assuming it cannot serve dynamic content
	      without an available database.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <para>The services Heartbeat controls, and any additional
	requirements those services depend on, are referred to as
	<emphasis>resources</emphasis> in Heartbeat terminology. Where
	resources form a co-dependent collection, that collection is
	called a <emphasis>resource group</emphasis>.</para>
    </section>
    <section id="s-resource-agents">
      <title>Heartbeat resource agents</title>
      <para><indexterm>
	<primary>Heartbeat</primary>
	<secondary>resource agent</secondary>
      </indexterm>
      <indexterm>
	<primary>resource agent (Heartbeat)</primary>
      </indexterm>
	Heartbeat manages resources by way of invoking
	standardized shell scripts known as <emphasis>resource
	  agents</emphasis> (RA's). In Heartbeat clusters,
	the following resource agent types are available:
	<itemizedlist>
	  <listitem>
	    <formalpara id="fp-heartbeat-ra">
	      <title>Heartbeat resource agents</title>
	      <para>These agents are found in the
		<filename>/etc/ha.d/resource.d</filename> directory.
		They may take zero or more positional, unnamed
		parameters, and one operation argument
		(<code>start</code>, <code>stop</code>, or
		<code>status</code>). Heartbeat translates resource
		parameters it finds for a matching resource in
		<filename>/etc/ha.d/haresources</filename> into
		positional parameters for the RA, which then uses
		these to configure the resource.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-lsb-ra">
	      <title>LSB resource agents</title>
	      <para>These are conventional, Linux Standard
		Base-compliant init scripts found in
		<filename>/etc/init.d</filename>, which Heartbeat
		simply invokes with the <code>start</code>,
		<code>stop</code>, or <code>status</code> argument.
		They take no positional parameters. Thus, the
		corresponding resources' configuration cannot be
		managed by Heartbeat; these services are expected to
		be configured by conventional configuration
		files.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-ocf-ra">
	      <title>OCF resource agents</title>
	      <para>These are resource agents that conform to the
		guidelines of the Open Cluster Framework, and they
		<emphasis>only</emphasis> work with clusters in CRM
		mode. They are usually found in either
		<filename>/usr/lib/ocf/resource.d/heartbeat</filename>
		or
		<filename>/usr/lib64/ocf/resource.d/heartbeat</filename>,
		depending on system architecture and distribution.
		They take no positional parameters, but may be
		extensively configured via environment variables that
		the cluster management process derives from the
		cluster configuration, and passes in to the resource
		agent upon invocation.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-heartbeat-communication-channels">
      <title>Heartbeat communication channels</title>
      <para><indexterm>
	<primary>Heartbeat</primary>
	<secondary>communication channels</secondary>
      </indexterm>
      <indexterm>
	<primary>communication channels (Heartbeat)</primary>
      </indexterm>Heartbeat uses a UDP-based communication protocol to
	periodically check for node availability (the "heartbeat"
	proper). For this purpose, Heartbeat can use several
	communication methods, including:
	<itemizedlist>
	  <listitem>
	    <para>IP multicast,</para>
	  </listitem>
	  <listitem>
	    <para>IP broadcast,</para>
	  </listitem>
	  <listitem>
	    <para>IP unicast,</para>
	  </listitem>
	  <listitem>
	    <para>serial line.</para>
	  </listitem>
	</itemizedlist>Of these, IP multicast and IP broadcast are the
	most relevant in practice. The absolute minimum requirement
	for stable cluster operation is two independent communication
	channels.
	<important>
	  <para>A bonded network interface (a virtual aggregation of
	    physical interfaces using the <indexterm>
	      <primary>bonding driver</primary>
	    </indexterm><code>bonding</code> driver)
	    constitutes <emphasis>one</emphasis> Heartbeat
	    communication channel.</para>
	  <para>Bonded links are not protected against bugs, known or
	    as-yet-unknown, in the <code>bonding</code> driver. Also,
	    bonded links are typically formed using identical network
	    interface models, thus they are vulnerable to bugs in the
	    NIC driver as well. Any such issue could lead to a cluster
	    partition if no independent second Heartbeat communication
	    channel were available.</para>
	  <para>It is thus <emphasis>not</emphasis> acceptable to omit the
	    inclusion of a second Heartbeat link in the cluster
	    configuration just because the first uses a bonded
	    interface.
	  </para>
	</important>
      </para>
    </section>
  </section>
  <section id="s-heartbeat-config">
    <title>Heartbeat configuration</title>
    <para><indexterm>
	<primary>Heartbeat</primary>
	<secondary>configuration</secondary>
      </indexterm>For any Heartbeat cluster, the following
      configuration files must be available:
      <itemizedlist>
	<listitem>
	  <para><indexterm>
	      <primary>ha.cf (Heartbeat configuration file)</primary>
	    </indexterm><filename>/etc/ha.d/ha.cf</filename> &mdash;
	    global cluster configuration.</para>
	</listitem>
	<listitem>
	  <para><indexterm>
	      <primary>authkeys (Heartbeat configuration
		file)</primary>
	    </indexterm><filename>/etc/ha.d/authkeys</filename>
	    &mdash; keys for mutual node authentication.</para>
	</listitem>
      </itemizedlist>
    </para>
    <para>Depending on whether Heartbeat is running in R1-compatible
      or in CRM mode, additional configuration files are required.
      These are covered in <xref
	  linkend="s-heartbeat-r1"/> and <xref
	  linkend="s-heartbeat-crm"/>.</para>
    <section id="s-heartbeat-hacf">
      <title>The <filename>ha.cf</filename> file</title>
      <para><indexterm>
	<primary>ha.cf (Heartbeat configuration file)</primary>
      </indexterm>The following example is a small and simple
	<filename>ha.cf</filename> file:
<programlisting id="pl-ha-cf-crm-no">autojoin none
mcast bond0 239.0.0.43 694 1 0
bcast eth2
warntime 5
deadtime 15
initdead 60
keepalive 2
node alice
node bob</programlisting></para>
      <para>Setting <option>autojoin</option> to <code>none</code>
	disables cluster node auto-discovery and requires that cluster
	nodes be listed explicitly, using the <option>node</option>
	options. This speeds up cluster start-up in clusters with a
	fixed number of nodes (which is always the case in R1-style
	Heartbeat clusters).</para>
      <para>This example assumes that <code>bond0</code> is the
	cluster's interface to the shared network, and that
	<code>eth2</code> is the interface dedicated for DRBD
	replication between both nodes. Thus, <code>bond0</code> can
	be used for Multicast heartbeat, whereas on <code>eth2</code>
	broadcast is acceptable as <code>eth2</code> is not a shared
	network.</para>
      <para>The next options configure node failure detection. They
	set the time after which Heartbeat issues a warning that a no
	longer available peer node <emphasis>may</emphasis> be dead
	(<option>warntime</option>), the time after which Heartbeat
	considers a node <emphasis>confirmed</emphasis> dead
	(<option>deadtime</option>), and the maximum time it waits for
	other nodes to check in at cluster startup
	(<option>initdead</option>). <option>keepalive</option> sets
	the interval at which Heartbeat keep-alive packets are sent.
	All these options are given in seconds.</para>
      <para>The <option>node</option> option identifies cluster
	members. The option values listed here must match the exact
	host names of cluster nodes as given by <command>uname
	  -n</command>.</para>
      <para>Not adding a <option>crm</option> option implies that the
	cluster is operating in <link
	  linkend="s-heartbeat-r1">R1-compatible mode</link> with CRM
	disabled. If <code>crm yes</code> were included in the
	configuration, Heartbeat would be running in <link
	  linkend="s-heartbeat-crm">CRM mode</link>.</para>
    </section>
    <section id="s-heartbeat-authkeys">
      <title>The <filename>authkeys</filename> file</title>
      <para><indexterm>
	<primary>authkeys (Heartbeat configuration file)</primary>
      </indexterm><filename>/etc/ha.d/authkeys</filename> contains
	pre-shared secrets used for mutual cluster node
	authentication. It should only be readable by
	<code>root</code> and follows this format:
<programlisting>auth <replaceable>num</replaceable>
<replaceable>num</replaceable> <replaceable>algorithm</replaceable> <replaceable>secret</replaceable></programlisting>
      </para>
      <para><replaceable>num</replaceable> is a simple key index,
	starting with 1. Usually, you will only have one key in your
	<filename>authkeys</filename> file.</para>
      <para><replaceable>algorithm</replaceable> is the signature
	algorithm being used. You may use either <code>md5</code> or
	<code>sha1</code>; the use of <code>crc</code> (a simple
	cyclic redundancy check, not secure) is not
	recommended.</para>
      <para><replaceable>secret</replaceable> is the actual
	authentication key.</para>
      <para>You may create an <filename>authkeys</filename> file,
	using a generated secret, with the following shell
	hack:</para>
      <programlisting id="pl-heartbeat-authkeys-autogenerate">( echo -ne "auth 1\n1 sha1 "; \
  dd if=/dev/urandom bs=512 count=1 | openssl md5 ) \
  &gt; /etc/ha.d/authkeys
chmod 0600 /etc/ha.d/authkeys</programlisting>
    </section>
    <section id="s-heartbeat-ha-propagate">
      <title>Propagating the cluster configuration to cluster
      nodes</title>
      <para>In order to propagate the contents of the
	<filename>ha.cf</filename> and <filename>authkeys</filename>
	configuration files, you may use the
	<command>ha_propagate</command> command, which you would
	invoke using either
	<screen><userinput>/usr/lib/heartbeat/ha_propagate</userinput></screen>
	or
	<screen><userinput>/usr/lib64/heartbeat/ha_propagate</userinput></screen>
      </para>
      <para>This utility will copy the configuration files over to any
	<code>node</code> listed in
	<filename>/etc/ha.d/ha.cf</filename> using
	<command>scp</command>. It will afterwards also connect to the
	nodes using <command>ssh</command> and issue <code>chkconfig
	heartbeat on</code> in order to enable Heartbeat services on
	system startup.</para>
    </section>
  </section>
  <section id="s-heartbeat-r1">
    <title>Using DRBD in Heartbeat R1-style clusters</title>
    <para>Running Heartbeat clusters in release 1 compatible
      configuration is now considered obsolete by the Linux-HA
      development team. However, it is still widely used in the field,
      which is why it is documented here in this section.</para>
    <formalpara id="fp-heartbeat-r1-advantages">
      <title>Advantages</title>
      <para>Configuring Heartbeat in R1 compatible mode has some
	advantages over using CRM configuration. In particular,
	<itemizedlist>
	  <listitem>
	    <para>Heartbeat R1 compatible clusters are simple and easy
	    to configure;</para>
	  </listitem>
	  <listitem>
	    <para>it is fairly straightforward to extend Heartbeat's
	      functionality with custom, R1-style resource
	      agents.</para>
	  </listitem>
	</itemizedlist>
      </para>
    </formalpara>
    <formalpara id="fp-heartbeat-r1-disadvantages">
      <title>Disadvantages</title>
      <para>Disadvantages of R1 compatible configuration, as opposed
	to CRM configurations, include:
	<itemizedlist>
	  <listitem>
	    <para>Cluster configuration must be manually kept in sync
	      between cluster nodes, it is not propagated
	      automatically.</para>
	  </listitem>
	  <listitem>
	    <para>While node monitoring is available, resource-level
	      monitoring is not. Individual resources must be
	      monitored by an external monitoring system.</para>
	  </listitem>
	  <listitem>
	    <para>Resource group support is limited to two resource
	      groups. CRM clusters, by contrast, support any number,
	      and also come with a complex resource-level constraint
	      framework.</para>
	  </listitem>
	</itemizedlist> Another disadvantage, namely the fact that R1
	style configuration limits cluster size to 2 nodes (whereas
	CRM clusters support up to 255) is largely irrelevant for
	setups involving DRBD, DRBD itself being limited to two
	nodes.</para>
    </formalpara>
    <section id="s-heartbeat-r1-config">
      <title>Heartbeat R1-style configuration</title>
      <para>In R1-style clusters, Heartbeat keeps its complete
	configuration in three simple configuration files:
	<itemizedlist>
	  <listitem>
	    <para><filename>/etc/ha.d/ha.cf</filename>, as described
	      in <xref linkend="s-heartbeat-hacf"/>.</para>
	  </listitem>
	  <listitem>
	    <para><filename>/etc/ha.d/authkeys</filename>, as described
	      in <xref linkend="s-heartbeat-authkeys"/>.</para>
	  </listitem>
	  <listitem>
	    <para><filename>/etc/ha.d/haresources</filename> &mdash;
	      the resource configuration file, described below.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <section id="s-heartbeat-haresources">
	<title>The <filename>haresources</filename> file</title>
	<para><indexterm>
	    <primary>haresources (Heartbeat configuration
	      file)</primary>
	</indexterm>The following is an example of a Heartbeat
	  R1-compatible resource configuration involving a MySQL
	  database backed by DRBD:
	  <programlisting id="pl-haresources">bob drbddisk::mysql Filesystem::/dev/drbd0::/var/lib/mysql::ext3 \
    10.9.42.1 mysql</programlisting></para>
	<para>This resource configuration contains one resource group
	  whose <emphasis>home node</emphasis> (the node where its
	  resources are expected to run under normal circumstances) is
	  named <code>bob</code>. Consequentially, this resource group
	  would be considered the <emphasis>local</emphasis> resource
	  group on host <code>bob</code>, whereas it would be the
	  <emphasis>foreign</emphasis> resource group on its peer
	  host.
	</para>
	<para>The resource group includes a DRBD resource named
	  <code>mysql</code>, which will be promoted to the primary
	  role by the cluster manager (specifically, the
	  <code>drbddisk</code> <link
	    linkend="fp-heartbeat-ra">resource agent</link>) on
	  whichever node is currently the active node. Of course, a
	  corresponding resource must exist and be configured in
	  <filename>/etc/drbd.conf</filename> for this to work.</para>
	<para>That DRBD resource translates to the block device named
	  <filename>/dev/drbd0</filename>, which contains an ext3
	  filesystem that is to be mounted at
	  <filename>/var/lib/mysql</filename> (the default location
	  for MySQL data files).</para>
	<para>The resource group also contains a service IP address,
	  10.9.42.1. Heartbeat will make sure that this IP address
	  is configured and available on whichever node is currently
	  active.</para>
	<para>Finally, Heartbeat will use the <link
	    linkend="fp-lsb-ra">LSB resource agent</link> named
	  <code>mysql</code> in order to start the MySQL daemon, which
	  will then find its data files at
	  <filename>/var/lib/mysql</filename> and be able to listen on
	  the service IP address, 192.168.42.1.</para>
	<para>It is important to understand that the resources listed
	  in the <filename>haresources</filename> file are always
	  evaluated from left to right when resources are being
	  started, and from right to left when they are being
	  stopped.</para>
      </section>
      <section id="s-heartbeat-stacked">
	<title>Stacked resources in Heartbeat R1-style
	  configurations</title>
	<subtitle>Available in DRBD version 8.3.0 and above</subtitle>
	<para>In <link linkend="s-three-way-repl">three-way
	    replication</link> with stacked resources, it is usually
	  desirable to have the stacked resource managed by Heartbeat
	  just as other cluster resources. Then, your two-node cluster
	  will manage the stacked resource as a floating resource
	  that runs on whichever node is currently the active one in
	  the cluster. The third node, which is set aside from the
	  Heartbeat cluster, will have the <quote>other half</quote>
	  of the stacked resource available permanently.</para>
	<note>
	  <para>To have a stacked resource managed by Heartbeat, you
	    must first configure it as outlined in
	    <xref linkend="s-three-node-config"/>.</para>
	</note>
	<para>The stacked resource is managed by Heartbeat by way of
	  the <code>drbdupper</code> resource agent. That resource
	  agent is distributed, as all other
	  Heartbeat R1 resource agents, in
	  <filename>/etc/ha.d/resource.d</filename>. It is to stacked
	  resources what the <code>drbddisk</code> resource agent is
	  to conventional, unstacked resources.</para>
	<para><code>drbdupper</code> takes care of managing both the
	  lower-level resource <emphasis>and</emphasis> the stacked
	  resource. Consider the following
	  <filename>haresources</filename> example, which would
	  replace the one given in the previous section:
	  <programlisting>bob&nbsp;192.168.42.1&nbsp;\
&nbsp;&nbsp;drbdupper::mysql-U&nbsp;Filesystem::/dev/drbd1::/var/lib/mysql::ext3&nbsp;\
&nbsp;&nbsp;mysql</programlisting></para>
	<para>Note the following differences to the earlier example:
	  <itemizedlist>
	    <listitem>
	      <para>You start the cluster IP address
		<emphasis>before</emphasis> all other resources. This
		is necessary because stacked resource replication uses
		a connection from the cluster IP address to the node
		IP address of the third node. Lower-level resource
		replication, by contrast, uses a connection between
		the <quote>physical</quote> node IP addresses of the
		two cluster nodes.</para>
	    </listitem>
	    <listitem>
	      <para>You pass the stacked resource name to
		<code>drbdupper</code> (in this example,
		<code>mysql-U</code>).</para>
	    </listitem>
	    <listitem>
	      <para>You configure the <code>Filesystem</code>
	      resource agent to mount the DRBD device associated with
		the stacked resource (in this example,
	      <filename>/dev/drbd1</filename>), not the lower-level one.</para>
	    </listitem>
	  </itemizedlist>
	</para>
      </section>
    </section>
    <section id="s-heartbeat-r1-manage">
      <title>Managing Heartbeat R1-style clusters</title>
      <section id="s-heartbeat-r1-assume-resources">
	<title>Assuming control of cluster resources</title>
	<para>A Heartbeat R1-style cluster node may assume control of
	  cluster resources in the following way:</para>
	<formalpara id="fp-heartbeat-r1-manual-resource-takeover">
	  <title>Manual resource takeover</title>
	  <para>This is the approach normally taken if one simply
	    wishes to test resource migration, or assume control
	    of resources for any reason other than the peer having
	    to leave the cluster. This operation is performed
	    using the following command:
	    <screen><userinput>/usr/lib/heartbeat/hb_takeover</userinput></screen>
	    On some distributions and architectures, you may be
	    required to enter:
	    <screen><userinput>/usr/lib64/heartbeat/hb_takeover</userinput></screen>
	  </para>
	</formalpara>
      </section>
      <section id="s-heartbeat-r1-relinquish-resources">
	<title>Relinquishing cluster resources</title>
	<para>A Heartbeat R1-style cluster node may be forced to give
	  up its resources in several ways.</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Switching a cluster node to standby
		mode</title>
	      <para>This is the approach normally taken if one simply
		wishes to test resource migration, or perform some
		other activity that does not require the node to leave
		the cluster. This operation is performed using the
		following command:
		<screen><userinput>/usr/lib/heartbeat/hb_standby</userinput></screen>
		On some distributions and architectures, you may be
		required to enter:
		<screen><userinput>/usr/lib64/heartbeat/hb_standby</userinput></screen>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-heartbeat-r1-shutdown-local-cluster-manager">
	      <title>Shutting down the local cluster manager
		instance</title>
	      <para>This approach is suited for local maintenance
		operations such as software updates which require that
		the node be temporarily removed from the cluster, but
		which do not necessitate a system reboot. It involves
		shutting down all processes associated with the local
		cluster manager instance:
		<screen><userinput>/etc/init.d/heartbeat stop</userinput></screen>
		Prior to stopping its services, Heartbeat will
		gracefully migrate any currently running resources to
		the peer node. This is the approach to be followed,
		for example, if you are upgrading DRBD to a new
		release, without also upgrading your kernel.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-heartbeat-r1-shutdown-local-node">
	      <title>Shutting down the local node</title>
	      <para>For hardware maintenance or other interventions
		that require a system shutdown or reboot, use a simple
		graceful shutdown command, such as
		<screen><userinput>reboot</userinput></screen>
		or
		<screen><userinput>poweroff</userinput></screen>
		Since Heartbeat services will be shut down gracefully
		in the process of a normal system shutdown, the
		previous paragraph applies to this situation, too.
		This is also the approach you would use in case of a
		kernel upgrade (which also requires the installation
		of a matching DRBD version).</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
    </section>
  </section>
  <section id="s-heartbeat-crm">
    <title>Using DRBD in Heartbeat CRM-enabled clusters</title>
    <para>Running Heartbeat clusters in CRM configuration mode is the
      recommended approach as of Heartbeat release 2 (per the Linux-HA
      development team).</para>
    <formalpara id="fp-heartbeat-crm-advantages">
      <title>Advantages</title>
      <para>Advantages of using CRM configuration mode, as opposed
	to R1 compatible configuration, include:
	<itemizedlist>
	  <listitem>
	    <para>Cluster configuration is distributed cluster-wide
	      and automatically, by the Cluster Resource Manager. It
	      need not be propagated manually.</para>
	  </listitem>
	  <listitem>
	    <para>CRM mode supports both node-level and resource-level
	      monitoring, and configurable responses to both node and
	      resource failure. It is still advisable to also monitor
	      cluster resources using an external monitoring
	      system.</para>
	  </listitem>
	  <listitem>
	    <para>CRM clusters support any number of resource groups,
	      as opposed to Heartbeat R1-style clusters which only
	      support two.</para>
	  </listitem>
	  <listitem>
	    <para>CRM clusters support a powerful (if complex)
	      constraints framework. This enables you to ensure
	      correct resource startup and shutdown order, resource
	      co-location (forcing resources to always run on the same
	      physical node), and to set preferred nodes for
	      particular resources.</para>
	  </listitem>
	</itemizedlist> Another advantage, namely the fact that CRM
	clusters support up to 255 nodes in a single cluster, is
	somewhat irrelevant for setups involving DRBD (DRBD itself
	being limited to two nodes).</para>
    </formalpara>
    <formalpara id="fp-heartbeat-crm-disadvantages">
      <title>Disadvantages</title>
      <para>Configuring Heartbeat in CRM mode also has some
	disadvantages in comparison to using R1-compatible
	configuration. In particular,
	<itemizedlist>
	  <listitem>
	    <para>Heartbeat CRM clusters are comparatively complex to
	      configure and administer;</para>
	  </listitem>
	  <listitem>
	    <para>Extending Heartbeat's functionality with custom OCF
	      resource agents is non-trivial.<note>
		<para>This disadvantage is somewhat mitigated by the
		  fact that you do have the option of using custom (or
		  legacy) R1-style resource agents in CRM
		  clusters.</para>
	      </note>
	    </para>
	  </listitem>
	</itemizedlist>
      </para>
    </formalpara>
    <section id="s-heartbeat-crm-config">
      <title>Heartbeat CRM configuration</title>
      <para>In CRM clusters, Heartbeat keeps part of configuration in the
	following configuration files:
	<itemizedlist>
	  <listitem>
	    <para><indexterm>
		<primary>ha.cf (Heartbeat configuration
		  file)</primary>
	      </indexterm><filename>/etc/ha.d/ha.cf</filename>, as
	      described in <xref linkend="s-heartbeat-hacf"/>. You
	      must include
	      the following line in this configuration file to enable
	      CRM mode:
	      <programlisting>crm yes</programlisting></para>
	  </listitem>
	  <listitem>
	    <para><indexterm>
		<primary>authkeys (Heartbeat configuration
		  file)</primary>
	      </indexterm><filename>/etc/ha.d/authkeys</filename>. The
	      contents of this file are the same as for R1 style
	      clusters. See <xref
		linkend="s-heartbeat-authkeys"/> for details.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <para>The remainder of the cluster configuration is maintained
	in the <emphasis>Cluster Information Base</emphasis>
	(<acronym>CIB</acronym>), covered in detail in <link
	  linkend="s-heartbeat-cib">the following section</link>.
	Contrary to the two relevant configuration files, the CIB need
	not be manually distributed among cluster nodes; the Heartbeat
	services take care of that automatically.</para>
      <section id="s-heartbeat-cib">
	<title>The Cluster Information Base</title>
	<para><indexterm>
	    <primary>Heartbeat</primary>
	    <secondary>Cluster Information Base (CIB)</secondary>
	</indexterm>
	<indexterm>
	    <primary>Cluster Information Base (CIB)</primary>
	    <see>Heartbeat</see>
	</indexterm>The Cluster Information Base
	  (<acronym>CIB</acronym>) is kept in one XML file,
	  <indexterm>
	    <primary>cib.xml (Heartbeat configuration file)</primary>
	  </indexterm>
	  <filename>/var/lib/heartbeat/crm/cib.xml</filename>. It is,
	  however, not recommended to edit the contents of this file
	  directly, except in the case of creating a new cluster
	  configuration from scratch. Instead, Heartbeat comes with
	  both command-line applications and a GUI to modify the CIB.
	</para>
	<para>The CIB actually contains both the cluster
	  <emphasis>configuration</emphasis> (which is persistent and
	  is kept in the <filename>cib.xml</filename> file), and
	  information about the current cluster
	  <emphasis>status</emphasis> (which is volatile). Status
	  information, too, may be queried either using Heartbeat
	  command-line tools, and the Heartbeat GUI.</para>
	<para>After creating a new Heartbeat CRM cluster &mdash; that is,
	  creating the <filename>ha.cf</filename> and
	  <filename>authkeys</filename> files, distributing them among
	  cluster nodes, starting Heartbeat services, and waiting for
	  nodes to establish intra-cluster communications &mdash; a
	  new, empty CIB is created automatically. Its contents will
	  be similar to this:</para>
	<programlisting><![CDATA[<cib>
   <configuration>
     <crm_config>
       <cluster_property_set id="cib-bootstrap-options">
         <attributes/>
       </cluster_property_set>
     </crm_config>
     <nodes>
       <node uname="alice" type="normal"
             id="f11899c3-ed6e-4e63-abae-b9af90c62283"/>
       <node uname="bob" type="normal"
             id="663bae4d-44a0-407f-ac14-389150407159"/>
     </nodes>
     <resources/>
     <constraints/>
   </configuration>
 </cib>
]]></programlisting>
	<para>The exact format and contents of this file are
	  documented at length <ulink
	    url="http://www.linux-ha.org/ClusterInformationBase/UserGuide">on
	    the Linux-HA web site</ulink>, but for practical purposes
	  it is important to understand that this cluster has two
	  nodes named <code>alice</code> and <code>bob</code>, and
	  that neither any resources nor any resource constraints have
	  been configured at this point.</para>
      </section>
      <section id="s-heartbeat-crm-drbd-backed-service">
	<title>Adding a DRBD-backed service to the cluster
	configuration</title>
	<para>This section explains how to enable a DRBD-backed
	  service in a Heartbeat CRM cluster. The examples used in
	  this section mimic, in functionality, those described in
	  <xref
	linkend="s-heartbeat-resources"/>, dealing with R1-style
	  Heartbeat clusters.</para>
	<para>The complexity of the configuration steps described in
	  this section may seem overwhelming to some, particularly
	  those having previously dealt only with R1-style Heartbeat
	  configurations. While the configuration of Heartbeat CRM
	  clusters is indeed complex (and sometimes not very
	  user-friendly), <link
	    linkend="fp-heartbeat-crm-advantages">the CRM's
	    advantages</link> may outweigh <link
	    linkend="fp-heartbeat-r1-advantages">those of R1-style
	    clusters</link>. Which approach to follow is entirely up
	  to the administrator's discretion.</para>
	<section id="s-heartbeat-crm-drbddisk-ra">
	  <title>Using the <code>drbddisk</code> resource agent in a
	  Heartbeat CRM configuration</title>
	  <para>Even though you are using Heartbeat in CRM mode, you
	    may still utilize R1-compatible resource agents such as
	    <code>drbddisk</code>. This resource agent provides no
	  secondary node monitoring, and ensures only resource
	  promotion and demotion.</para>
	  <para>In order to enable a DRBD-backed configuration
	    for a MySQL database in a Heartbeat CRM cluster with
	    <code>drbddisk</code>, you would use a configuration like
	  this:<programlisting id="pl-resource-group-mysql"><![CDATA[
<group ordered="true" collocated="true" id="rg_mysql">
  <primitive class="heartbeat" type="drbddisk"
             provider="heartbeat" id="drbddisk_mysql">
    <meta_attributes>
      <attributes>
        <nvpair name="target_role" value="started"/>
      </attributes>
    </meta_attributes>
    <instance_attributes>
      <attributes>
        <nvpair name="1" value="mysql"/>
      </attributes>
    </instance_attributes>
  </primitive>
  <primitive class="ocf" type="Filesystem"
             provider="heartbeat" id="fs_mysql">
    <instance_attributes>
      <attributes>
        <nvpair name="device" value="/dev/drbd0"/>
        <nvpair name="directory" value="/var/lib/mysql"/>
        <nvpair name="type" value="ext3"/>
      </attributes>
    </instance_attributes>
  </primitive>
  <primitive class="ocf" type="IPaddr2"
             provider="heartbeat" id="ip_mysql">
    <instance_attributes>
      <attributes>
        <nvpair name="ip" value="192.168.42.1"/>
        <nvpair name="cidr_netmask" value="24"/>
        <nvpair name="nic" value="eth0"/>
      </attributes>
    </instance_attributes>
  </primitive>
  <primitive class="lsb" type="mysqld"
             provider="heartbeat" id="mysqld"/>
</group>
]]></programlisting></para>
	<para>Assuming you created this configuration in a temporary
	  file named <filename>/tmp/hb_mysql.xml</filename>, you would
	  add this resource group to the cluster configuration using
	  the following command (on any cluster node):
	  <indexterm>
	    <primary>Heartbeat</primary>
	    <secondary>cibadmin command</secondary>
	  </indexterm>
	  <indexterm>
	    <primary>cibadmin (Heartbeat command)</primary>
	  </indexterm>
	  <screen><userinput>cibadmin -o resources -C -x <filename>/tmp/hb_mysql.xml</filename></userinput></screen>
	  After this, Heartbeat will automatically propagate the
	  newly-configured resource group to all cluster nodes.
	  </para>
	</section>
	<section id="s-heartbeat-crm-drbd-ocf-ra">
	  <title>Using the <code>drbd</code> OCF resource agent in a
	    Heartbeat CRM configuration</title>
	  <para>The <code>drbd</code> resource agent is a
	    <quote>pure-bred</quote> OCF RA which provides
	    Master/Slave capability, allowing Heartbeat to start and
	    monitor the DRBD resource on multiple nodes and promoting
	    and demoting as needed. You must, however, understand that
	    the <code>drbd</code> RA disconnects and detaches all DRBD
	    resources it manages on Heartbeat shutdown, and also upon
	    enabling standby mode for a node.</para>
	  <para>In order to enable a DRBD-backed configuration for a
	    MySQL database in a Heartbeat CRM cluster with the
	    <code>drbd</code> OCF resource agent, you must create both
	    the necessary resources, and Heartbeat constraints to
	    ensure your service only starts on a previously promoted
	    DRBD resource. It is recommended that you start with the
	    constraints, such as shown in this example:
	    <programlisting
	    id="pl-constraints-mysql-drbd-ocf"><![CDATA[<constraints>
  <rsc_order id="mysql_after_drbd" from="rg_mysql" action="start"
             to="ms_drbd_mysql" to_action="promote" type="after"/>
  <rsc_colocation id="mysql_on_drbd" to="ms_drbd_mysql"
                  to_role="master" from="rg_mysql" score="INFINITY"/>
</constraints>]]></programlisting></para>
	  <para>Assuming you put these settings in a file named
	    <filename>/tmp/constraints.xml</filename>, here is how you
	    would enable them:
	    <screen><userinput>cibadmin -U -x /tmp/constraints.xml</userinput></screen></para>
	  <para>Subsequently, you would create your relevant resources:
	    <programlisting
	      id="pl-resource-group-mysql-drbd-ocf"><![CDATA[<resources>
  <master_slave id="ms_drbd_mysql">
    <meta_attributes id="ms_drbd_mysql-meta_attributes">
      <attributes>
        <nvpair name="notify" value="yes"/>
        <nvpair name="globally_unique" value="false"/>
      </attributes>
    </meta_attributes>
    <primitive id="drbd_mysql" class="ocf" provider="heartbeat"
        type="drbd">
      <instance_attributes id="ms_drbd_mysql-instance_attributes">
        <attributes>
          <nvpair name="drbd_resource" value="mysql"/>
        </attributes>
      </instance_attributes>
      <operations id="ms_drbd_mysql-operations">
        <op id="ms_drbd_mysql-monitor-master"
	    name="monitor" interval="29s"
            timeout="10s" role="Master"/>
        <op id="ms_drbd_mysql-monitor-slave"
            name="monitor" interval="30s"
            timeout="10s" role="Slave"/>
      </operations>
    </primitive>
  </master_slave>
  <group id="rg_mysql">
    <primitive class="ocf" type="Filesystem"
               provider="heartbeat" id="fs_mysql">
      <instance_attributes id="fs_mysql-instance_attributes">
        <attributes>
          <nvpair name="device" value="/dev/drbd0"/>
          <nvpair name="directory" value="/var/lib/mysql"/>
          <nvpair name="type" value="ext3"/>
        </attributes>
      </instance_attributes>
    </primitive>
    <primitive class="ocf" type="IPaddr2"
               provider="heartbeat" id="ip_mysql">
      <instance_attributes id="ip_mysql-instance_attributes">
        <attributes>
          <nvpair name="ip" value="10.9.42.1"/>
          <nvpair name="nic" value="eth0"/>
        </attributes>
      </instance_attributes>
    </primitive>
    <primitive class="lsb" type="mysqld"
               provider="heartbeat" id="mysqld"/>
  </group>
</resources>
]]></programlisting>
	  </para>
	  <para>Assuming you put these settings in a file named
	    <filename>/tmp/resources.xml</filename>, here is how you
	    would enable them:
	    <screen><userinput>cibadmin -U -x /tmp/resources.xml</userinput></screen></para>
	  <para>After this, your configuration should be enabled.
	    Heartbeat now selects a node on which it promotes the DRBD
	    resource, and then starts the DRBD-backed resource group
	    on that same node.</para>
	</section>
      </section>
    </section>
    <section id="s-heartbeat-crm-manage">
      <title>Managing Heartbeat CRM clusters</title>
      <section id="s-heartbeat-crm-assume-resources">
	<title>Assuming control of cluster resources</title>
	<para>A Heartbeat CRM cluster node may assume control of
	 cluster resources in the following ways:</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Manual takeover of a single cluster
		resource</title>
	      <para>This is the approach normally taken if one simply
		wishes to test resource migration, or move a resource
		to the local node as a means of manual load balancing.
		This operation is performed using the following
		command:<indexterm>
		  <primary>Heartbeat</primary>
		  <secondary>crm_resource command</secondary>
		</indexterm>
		<indexterm>
		  <primary>crm_resource (Heartbeat command)</primary>
		</indexterm>
		<screen><userinput>crm_resource -r <replaceable>resource</replaceable> -M -H `uname -n`</userinput></screen>
		<note id="n-heartbeat-crm-migrate">
		  <para>The <option>-M</option> (or
		    <option>--migrate</option>) option for the
		    <command>crm_resource</command> command, when used
		    without the <option>-H</option> option, implies a
		    resource migration <emphasis>away</emphasis> from
		    the local host. You must initiate a migration
		    <emphasis>to</emphasis> the local host by
		    specifying the <option>-H</option> option, giving
		    the local host name as the option argument.</para>
		  <para>It is also important to understand that the
		    migration is <emphasis>permanent</emphasis>, that
		    is, unless told otherwise, Heartbeat will not move
		    the resource back to a node it was previously
		    migrated away from &mdash; even if that node
		    happens to be the only surviving node in a
		    near-cluster-wide system failure. This is
		    undesirable under most circumstances. So, it is
		    prudent to immediately <quote>un-migrate</quote>
		    resources after successful migration, using the
		    the following command:
		    <indexterm>
		      <primary>Heartbeat</primary>
		      <secondary>crm_resource command</secondary>
		    </indexterm>
		    <indexterm>
		      <primary>crm_resource (Heartbeat command)</primary>
		    </indexterm>
		    <screen><userinput>crm_resource -r <replaceable>resource</replaceable> -U</userinput></screen>
		  </para>
		  <para>Finally, it is important to know that during
		    resource migration, Heartbeat may simultaneously
		    migrate resources other than the one explicitly
		    specified (as required by existing resource groups
		    or colocation and order constraints).
		  </para>
		</note>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Manual takeover of all cluster resources</title>
	      <para>This procedure involves switching the peer node to
		standby mode (where
		<replaceable>hostname</replaceable> is the peer node's
		host name):
		<indexterm>
		  <primary>Heartbeat</primary>
		  <secondary>crm_standby command</secondary>
		</indexterm>
		<indexterm>
		  <primary>crm_standby (Heartbeat command)</primary>
		</indexterm>
		<screen><userinput>crm_standby -U <replaceable>hostname</replaceable> -v on</userinput></screen>
	      </para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
      <section id="s-heartbeat-crm-relinquish-resources">
	<title>Relinquishing cluster resources</title>
	<para>A Heartbeat CRM cluster node may be forced to give up
	  one or all of its resources in several ways.</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Giving up a single cluster resource</title>
	      <para>A node gives up control of a single resource when
		issued the following command (note that <link
		  linkend="n-heartbeat-crm-migrate">the considerations
		  outlined in the previous section</link> apply here,
		too):
		<indexterm>
		  <primary>Heartbeat</primary>
		  <secondary>crm_resource command</secondary>
		</indexterm>
		<indexterm>
		  <primary>crm_resource (Heartbeat command)</primary>
		</indexterm>
		<screen><userinput>crm_resource -r <replaceable>resource</replaceable> -M </userinput></screen>
		If you want to migrate to a specific host, use this
		variant:
		<screen><userinput>crm_resource -r <replaceable>resource</replaceable> -M -H <replaceable>hostname</replaceable></userinput></screen>
		However, the latter syntax is usually of little
		relevance to CRM clusters using DRBD, DRBD being
		limited to two nodes (so the two variants are,
		essentially, identical in meaning).
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Switching a cluster node to standby
		mode</title>
	      <para>This is the approach normally taken if one simply
		wishes to test resource migration, or perform some
		other activity that does not require the node to leave
		the cluster. This operation is performed using the
		following command:
		<indexterm>
		  <primary>Heartbeat</primary>
		  <secondary>crm_standby command</secondary>
		</indexterm>
		<indexterm>
		  <primary>crm_standby (Heartbeat command)</primary>
		</indexterm>
		<screen><userinput>crm_standby -U `uname -n` -v on</userinput></screen>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Shutting down the local cluster manager
		instance</title>
	      <para>This approach is suited for local maintenance
		operations such as software updates which require that
		the node be temporarily removed from the cluster, but
		which do not necessitate a system reboot. The
		procedure is <link
		  linkend="fp-heartbeat-r1-shutdown-local-cluster-manager">the
		  same as for Heartbeat R1 style
		  clusters</link>.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Shutting down the local node</title>
	      <para>For hardware maintenance or other interventions
		that require a system shutdown or reboot, use a simple
		graceful shutdown command, just as previously outlined
		<link
		  linkend="fp-heartbeat-r1-shutdown-local-node">for
		  Heartbeat R1 style clusters</link>.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
    </section>
  </section>
  <section id="s-heartbeat-dopd">
    <title>Using Heartbeat with <code>dopd</code></title>
    <para><indexterm>
	<primary>dopd</primary>
      </indexterm>The steps outlined in this section enable DRBD to
      deny services access to <link linkend="s-outdate">outdated
	data</link>. The Heartbeat component that implements this
      functionality is the <emphasis>DRBD outdate-peer
	daemon</emphasis>, or <code>dopd</code> for short. It works,
      and uses identical configuration, on both <link
	linkend="s-heartbeat-r1">R1-compatible</link> and <link
	linkend="s-heartbeat-crm">CRM</link> clusters.
      <important>
	<para>It is absolutely vital to configure at least two
	  independent <link
	    linkend="s-heartbeat-communication-channels">Heartbeat
	    communication channels</link> for <code>dopd</code> to
	  work correctly.</para>
      </important></para>
    <section id="s-dopd-heartbeat-config">
      <title>Heartbeat configuration</title>
      <para>To enable dopd, you must add these lines to your
	<indexterm>
	  <primary>ha.cf (Heartbeat configuration file)</primary>
	</indexterm>
	<filename>/etc/ha.d/ha.cf</filename> file:
	<programlisting>respawn hacluster /usr/lib/heartbeat/dopd
apiauth dopd gid=haclient uid=hacluster</programlisting>
	You may have to adjust <code>dopd</code>'s path according to
	your preferred distribution. On some distributions and
	architectures, the correct path is
	<filename>/usr/lib64/heartbeat/dopd</filename>.</para>
      <para>After you have made this change and copied
	<filename>ha.cf</filename> to the peer node, you must run
	<command>/etc/init.d/heartbeat reload</command> to have
	Heartbeat re-read its configuration file. Afterwards, you
	should be able to verify that you now have a running
	<code>dopd</code> process.<note>
	  <para>You can check for this process either by running
	    <command>ps ax | grep dopd</command>
	    or by issuing
	    <command>killall -0 dopd</command>.</para>
	</note>
      </para>
    </section>
    <section id="s-dopd-drbd-config">
      <title>DRBD Configuration</title>
      <para>Then, add these items to your DRBD resource configuration:
	<programlisting>resource <replaceable>resource</replaceable> {
    handlers {
        fence-peer "/usr/lib/heartbeat/drbd-peer-outdater -t 5";
        ...
    }
    disk {
        fencing resource-only;
        ...
    }
    ...
}</programlisting>As with <code>dopd</code>, your
	distribution may place the
	<filename>drbd-peer-outdater</filename> binary in
	<filename>/usr/lib64/heartbeat</filename> depending on your
	system architecture.</para>
      <para>Finally, copy your <filename>drbd.conf</filename> to the
	peer node and issue
	<command>drbdadm adjust <replaceable>resource</replaceable></command>
	to reconfigure your resource and reflect your changes.</para>
    </section>
    <section id="s-dopd-test">
      <title>Testing <code>dopd</code> functionality</title>
      <para>To test whether your <code>dopd</code> setup is working
      correctly, interrupt the replication link of a configured and
      connected resource while Heartbeat services are running
      normally. You may do so simply by physically unplugging the
      network link, but that is fairly invasive. Instead, you may
      insert a temporary <code>iptables</code> rule to drop incoming
      DRBD traffic to the TCP port used by your resource.</para>
      <para>
	After this, you will be able to observe the
	resource <link linkend="s-connection-states">connection
	  state</link> change from <indexterm>
	  <primary>connection state</primary>
	  <secondary>Connected</secondary>
	</indexterm>
	<indexterm>
	  <primary>Connected (connection state)</primary>
	</indexterm>
	<code>Connected</code> to
	<indexterm>
	  <primary>connection state</primary>
	  <secondary>WFConnection</secondary>
	</indexterm>
	<indexterm>
	  <primary>WFConnection (connection state)</primary>
	</indexterm>
	<code>WFConnection</code>. Allow a few seconds to pass, and
	you should see the <link linkend="s-disk-states">disk
	  state</link> become
	<indexterm>
	  <primary>disk state</primary>
	  <secondary>Outdated</secondary>
	</indexterm>
	<indexterm>
	  <primary>Outdated (disk state)</primary>
	</indexterm>
	<code>Outdated/DUnknown</code>. That is
	what <code>dopd</code> is responsible for.</para>
      <para>Any attempt to switch the outdated resource to the primary
	role will fail after this.</para>
      <para>When re-instituting network connectivity (either by
	plugging the physical link or by removing the temporary
	<code>iptables</code> rule you inserted previously),
	the connection state will change to <code>Connected</code>,
	and then promptly to <code>SyncTarget</code> (assuming changes
	occurred on the primary node during the network interruption).
	Then you will be able to observe a brief synchronization
	period, and finally, the previously outdated resource will be
	marked as
	<indexterm>
	  <primary>disk state</primary>
	  <secondary>UpToDate</secondary>
	</indexterm>
	<indexterm>
	  <primary>UpToDate (disk state)</primary>
	</indexterm>
	<code>UpToDate</code> again.</para>
    </section>
  </section>
</chapter>
