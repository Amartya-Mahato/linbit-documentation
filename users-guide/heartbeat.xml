<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-heartbeat" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Integrating DRBD with Heartbeat clusters</title>
  <para>Using DRBD in conjunction with the Linux-HA cluster manager
    ("Heartbeat") is arguably DRBD's most frequently found use case.
    Heartbeat is also one of the applications that make DRBD extremely
    powerful in a wide variety of usage scenarios. Hence, this is one
    of the more detailed chapters in this guide.</para>
  <para>This chapter describes using DRBD as replicated storage for
    Linux-HA High Availability clusters. It covers both
    traditionally-configured, Heartbeat release 1-compatible clusters,
    and the more advanced CRM-enabled Heartbeat 2 clusters.</para>
  <section id="s-heartbeat-primer">
    <title>Heartbeat primer</title>
    <section id="s-heartbeat-cluster-manager">
      <title>The Heartbeat cluster manager</title>
      <para>Heartbeat's purpose as a cluster manager is to ensure that
	the cluster maintains its services to the clients, even if
	single machines of the cluster fail. Applications that may be
	managed by Heartbeat as cluster services include, for example,
	<itemizedlist>
	  <listitem>
	    <para>a web server such as Apache,</para>
	  </listitem>
	  <listitem>
	    <para>a database server such as MySQL, Oracle, or
	    PostgreSQL,</para>
	  </listitem>
	  <listitem>
	    <para>a file server such as NFS or Samba, and many
	      others.</para>
	  </listitem>
	</itemizedlist> In essence, any server application may be
	managed by Heartbeat as a cluster service.</para>
      <para>Services managed by Heartbeat are typically removed from
	the system startup configuration; rather than being started at
	boot time, they are be started and stopped by the cluster
	manager as required by the cluster configuration. If a machine
	(a physical cluster node) fails while running a particular set
	of services, Heartbeat will start the failed services on
	another machine in the cluster. These operations performed by
	Heartbeat are commonly referred to as (automatic)
	<emphasis>fail-over</emphasis>.</para>
      <para>A migration of cluster services from one cluster node to
	another, by manual intervention, is commonly termed "manual
	fail-over". This being a slightly self-contradictory term, we
	use the alternative term <emphasis>switch-over</emphasis> for
	the purposes of this guide.</para>
      <para>Heartbeat is also capable of automatically migrating
	resources back to a previously failed node, as soon as the
	latter recovers. This process is called
	<emphasis>fail-back</emphasis>.</para>
    </section>
    <section id="s-heartbeat-resources">
      <title>Heartbeat resources</title>
      <para>Usually, there will be certain requirements in order to be able to start a
      cluster service managed by Heartbeat on a node. Consider the
      example of a complex, database-driven web application:
	<itemizedlist>
	  <listitem>
	    <para>Both the web server and the database server assume
	      that their designated <emphasis>IP addresses</emphasis>
	      are available (i.e. configured) on the node.</para>
	  </listitem>
	  <listitem>
	    <para>The database will require a <emphasis>file
	    system</emphasis> to retrieve data files from.</para>
	  </listitem>
	  <listitem>
	    <para>That file system will require its underlying
	      <emphasis>block device</emphasis> to read from and write
	      to (this is where DRBD comes in, as we will see later).</para>
	  </listitem>
	  <listitem>
	    <para>The web server will also depend on the database
	    being started, assuming it cannot server dynamic content
	    without an available database.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <para>The services Heartbeat controls, and any additional
	requirements those services depend on, are referred to as
	<emphasis>resources</emphasis> in Heartbeat terminology. Where
	resources form a co-dependent collection, that collection is
	called a <emphasis>resource group</emphasis>.</para>
    </section>
    <section id="s-resource-agents">
      <title>Heartbeat resource agents</title>
      <para>Heartbeat manages resources by way of invoking
	standardized shell scripts known as <emphasis>resource
	  agents</emphasis> (RA's). In Heartbeat R1-style clusters,
	the following resource agent types are available:
	<itemizedlist>
	  <listitem>
	    <formalpara id="fp-heartbeat-ra">
	      <title>Heartbeat resource agents</title>
	      <para>These agents are found in the
		<filename>/etc/ha.d/resource.d</filename> directory.
		They may take zero or more positional, unnamed
		parameters, and one operation argument
		(<code>start</code>, <code>stop</code>, or
		<code>status</code>). Heartbeat translates resource
		parameters it finds for a matching resource in
		<filename>/etc/ha.d/haresources</filename> into
		positional parameters for the RA, which then uses
		these to configure the resource.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-lsb-ra">
	      <title>LSB resource agents</title>
	      <para>These are conventional, Linux Standard
		Base-compliant init scripts found in
		<filename>/etc/init.d</filename>, which Heartbeat
		simply invokes with the <code>start</code>,
		<code>stop</code>, or <code>status</code> argument.
		They take no positional parameters. Thus, the
		corresponding resources' configuration cannot be
		managed by Heartbeat; these services are expected to
		be configured by conventional configuration
		files.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-ocf-ra">
	      <title>OCF resource agents</title>
	      <para>These are resource agents that conform to the
		guidelines of the Open Cluster Framework. They are
		usually found in either
		<filename>/usr/lib/ocf/heartbeat/resource.d</filename>
		or
		<filename>/usr/lib64/ocf/heartbeat/resource.d</filename>, 
		depending on system architecture and distribution.
		They take no positional parameters, but may be
		extensively configured via environment variables that
		the cluster management process derives from the
		cluster configuration, and passes in to the resource
		agent upon invocation. OCF resource agents are only
		available in CRM-enabled Heartbeat clusters.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-heartbeat-communication-channels">
      <title>Heartbeat communication channels</title>
      <para>Heartbeat uses a UDP-based communication protocol to
	periodically check for node availability (the "heartbeat"
	proper). For this purpose, Heartbeat can use several
	communication methods, including:
	<itemizedlist>
	  <listitem>
	    <para>IP multicast,</para>
	  </listitem>
	  <listitem>
	    <para>IP broadcast,</para>
	  </listitem>
	  <listitem>
	    <para>IP unicast,</para>
	  </listitem>
	  <listitem>
	    <para>serial line.</para>
	  </listitem>
	</itemizedlist>Of these, IP multicast and IP broadcast are the
	most relevant in practice. The absolute minimum requirement
	for stable cluster operation is two independent communication
	channels.
	<important>
	  <para>A bonded network interface (a virtual aggregation of
	    physical interfaces using the <code>bonding</code> driver)
	    constitutes <emphasis>one</emphasis> Heartbeat
	    communication channel.</para>
	  <para>Bonded links are not protected against bugs, known or
	    as-yet-unknown, in the <code>bonding</code> driver. Also,
	    bonded links are typically formed using identical network
	    interface models, thus they are vulnerable to bugs in the
	    NIC driver as well. Any such issue could lead to a cluster
	    partition if no independent second Heartbeat communication
	    channel were available.</para>
	  <para>It is thus <emphasis>not</emphasis> acceptable to omit the
	    inclusion of a second Heartbeat link in the cluster
	    configuration just because the first uses a bonded
	    interface.
	  </para>
	</important>
      </para>
    </section>
  </section>
  <section id="s-heartbeat-config">
    <title>Heartbeat configuration</title>
    <para>For any Heartbeat cluster, the following configuration files
      must be available:
      <itemizedlist>
	<listitem>
	  <para><filename>/etc/ha.d/ha.cf</filename> &mdash; global
	    cluster configuration.</para>
	</listitem>
	<listitem>
	  <para><filename>/etc/ha.d/authkeys</filename> &mdash; keys
	    for mutual node authentication.</para>
	</listitem>
      </itemizedlist>
    </para>
    <para>Depending on whether Heartbeat is running in R1-compatible
      or in CRM mode, additional configuration files are required.
      These are covered in <xref
	  linkend="s-heartbeat-r1"/> and <xref
	  linkend="s-heartbeat-crm"/>.</para>
    <section id="s-heartbeat-hacf">
      <title>The <filename>ha.cf</filename> file</title>
      <para>The following example is a small and simple
	<filename>ha.cf</filename> file:
<programlisting>autojoin none 
mcast bond0 239.0.0.43 694 1 0 
bcast eth2 
warntime 15
deadtime 30 
initdead 120 
keepalive 2 
node alice
node bob 
crm no</programlisting></para>
      <para>Setting <option>autojoin</option> to <code>none</code>
	disables cluster node auto-discovery and requires that cluster
	nodes be listed explicitly, using the <option>node</option>
	options. This speeds up cluster start-up in clusters with a
	fixed number of nodes (which is always the case in R1-style
	Heartbeat clusters).</para>
      <para>This example assumes that <code>bond0</code> is the
	cluster's interface to the shared network, and that
	<code>eth2</code> is the interface dedicated for DRBD
	replication between both nodes. Thus, <code>bond0</code> can
	be used for Multicast heartbeat, whereas on <code>eth2</code>
	broadcast is acceptable as <code>eth2</code> is not a shared
	network.</para>
      <para>The next options configure node failure detection. They
	set the time after which Heartbeat issues a warning that a no
	longer available peer node <emphasis>may</emphasis> be dead
	(<option>warntime</option>), the time after which Heartbeat
	considers a node <emphasis>confirmed</emphasis> dead
	(<option>deadtime</option>), and the maximum time it waits for
	other nodes to check in at cluster startup
	(<option>initdead</option>). <option>keepalive</option> sets
	the interval at which Heartbeat keep-alive packets are sent.
	All these options are given in seconds.</para>
      <para>The <option>node</option> option identifies cluster
	members. The option values listed here must match the exact
	host names of cluster nodes as given by <command>uname
	  -n</command>.</para>
      <para><code>crm no</code> indicates that this cluster is a <link
	  linkend="s-heartbeat-r1">R1-compatible cluster</link> with
	CRM disabled. We this option set to <code>yes</code>,
	Heartbeat would be running in <link
	  linkend="s-heartbeat-crm">CRM mode</link>.</para>
    </section>
    <section id="s-heartbeat-authkeys">
      <title>The <filename>authkeys</filename> file</title>
      <para><filename>/etc/ha.d/authkeys</filename> contains
	pre-shared secrets used for mutual cluster node
	authentication. It should only be readable by
	<code>root</code>and follows this format: 
<programlisting>auth <replaceable>num</replaceable>
<replaceable>num</replaceable> <replaceable>algorithm</replaceable> <replaceable>secret</replaceable></programlisting>
      </para>
      <para><replaceable>num</replaceable> is a simple key index,
	starting with 1. Usually, you will only have one key in your
	<filename>authkeys</filename> file.</para>
      <para><replaceable>algorithm</replaceable> is the signature
	algorithm being used. You may use either <code>md5</code> or
	<code>sha1</code>; the use of <code>crc</code> (a simple
	cyclic redundancy check, not secure) is not
	recommended.</para>
      <para><replaceable>secret</replaceable> is the actual
	authentication key.</para>
      <para>You may create an <filename>authkeys</filename> file,
	using a generated secret, with the following shell
	hack:</para>
      <programlisting>echo -e "auth 1\n1 sha1 $(date | openssl md5)" &gt;/etc/ha.d/authkeys</programlisting>
    </section>
  </section>
  <section id="s-heartbeat-r1">
    <title>Using DRBD in Heartbeat R1-style clusters</title>
    <para>Running Heartbeat clusters in release 1 compatible
      configuration is now considered obsolete by the Linux-HA
      development team. However, it is still widely used in the field,
      which is why it is documented here in this section.</para>
    <formalpara id="fp-heartbeat-r1-advantages">
      <title>Advantages</title>
      <para>Configuring Heartbeat in R1 compatible mode has some
	advantages over using CRM configuration, notably
	<itemizedlist>
	  <listitem>
	    <para>Simplicity of configuration,</para>
	  </listitem>
	  <listitem>
	    <para>Ease of functionality extension with custom resource
	      agents.</para>
	  </listitem>
	</itemizedlist>
      </para>
    </formalpara>
    <formalpara id="fp-heartbeat-r1-disadvantages">
      <title>Disadvantages</title>
      <para>Disadvantages of R1 compatible configuration, as opposed
	to CRM configurations, include:
	<itemizedlist>
	  <listitem>
	    <para>Cluster configuration must be manually kept in sync
	      between cluster nodes, it is not propagated
	      automatically.</para>
	  </listitem>
	  <listitem>
	    <para>While node monitoring is available, resource-level
	      monitoring is not. Individual resources must be
	      monitored by an external monitoring system.</para>
	  </listitem>
	  <listitem>
	    <para>Resource group support is limited to two resource
	      groups. CRM clusters, by contrast, support any number,
	      and also come with a complex resource-level constraint
	      framework.</para>
	  </listitem>
	</itemizedlist> Another disadvantage, namely the fact that R1
	style configuration limits cluster size to 2 nodes (whereas
	CRM cluster support up to 255) is largely irrelevant for
	setups involving DRBD, DRBD itself being limited to two
	nodes.</para>
    </formalpara>
    <section id="s-heartbeat-r1-config">
      <title>Heartbeat R1-style configuration</title>
      <para>In R1-style clusters, Heartbeat keeps its complete
	configuration in three simple configuration files:
	<itemizedlist>
	  <listitem>
	    <para><filename>/etc/ha.d/ha.cf</filename>, as described
	      in <xref linkend="s-heartbeat-hacf"/>.</para>
	  </listitem>
	  <listitem>
	    <para><filename>/etc/ha.d/authkeys</filename>, as described
	      in <xref linkend="s-heartbeat-authkeys"/>.</para>
	  </listitem>
	  <listitem>
	    <para><filename>/etc/ha.d/haresources</filename> &mdash;
	      the resource configuration file, described below.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <section id="s-heartbeat-haresources">
	<title>The <filename>haresources</filename> file</title>
	<para>The following is an example of a Heartbeat R1-compatible
	resource configuration involving a MySQL database backed by
	DRBD:
	  <programlisting>bob drbddisk::mysql Filesystem::/dev/drbd0::/var/lib/mysql::ext3 \
    192.168.42.1 mysql</programlisting></para>
	<para>This resource configuration contains one resource group
	  whose <emphasis>home node</emphasis> (the node where its
	  resources are expected to run under normal circumstances) is
	  named <code>bob</code>.<footnote>
	    <para>Consequentially, this resource group would be
	      considered the <emphasis>local</emphasis> resource group
	      on host <code>bob</code>, whereas it would be the
	      <emphasis>foreign</emphasis> resource group on its peer
	      host.</para>
	  </footnote>
	</para>
	<para>The resource group includes a DRBD resource named
	  <code>mysql</code>, which will be promoted to the primary
	  role by the cluster manager (specifically, the
	  <code>drbddisk</code> <link
	    linkend="fp-heartbeat-ra">resource agent</link>) on
	  whichever node is currently the active node. Of course, a
	  corresponding resource must exist and be configured in
	  <filename>/etc/drbd.conf</filename> for this to work.</para>
	<para>That DRBD resource translates to the block device named
	  <filename>/dev/drbd0</filename>, which contains an ext3
	  filesystem that is to be mounted at
	  <filename>/var/lib/mysql</filename> (the default location
	  for MySQL data files).</para>
	<para>The resource group also contains a service IP address,
	  192.168.42.1. Heartbeat will make sure that this IP address
	  is configured and available on whichever node is currently
	  active.</para>
	<para>Finally, Heartbeat will use the <link
	    linkend="fp-lsb-ra">LSB resource agent</link> named
	  <code>mysql</code> in order to start the MySQL daemon, which
	  will then find its data files at
	  <filename>/var/lib/mysql</filename> and be able to listen on
	  the service IP address, 192.168.42.1.</para>
	<para>It is important to understand that the resources listed
	  in the <filename>haresources</filename> file are always
	  evaluated from left to right when resources are being
	  started, and from right to left when they are being
	  stopped.</para>
      </section>
    </section>
    <section id="s-heartbeat-r1-manage">
      <title>Managing Heartbeat R1-style clusters</title>
      <section id="s-heartbeat-r1-assume-resources">
	<title>Assuming control of cluster resources</title>
	<para>A Heartbeat R1-style cluster node may assume control of
	  cluster resources in the following way:</para>
	<formalpara id="fp-heartbeat-r1-manual-resource-takeover">
	  <title>Manual resource takeover</title>
	  <para>This is the approach normally taken if one simply
	    wishes to test resource migration, or assume control
	    of resources for any reason other than the peer having
	    to leave the cluster. This operation is performed
	    using the following command:
	    <literallayout><userinput>/usr/lib/heartbeat/hb_takeover</userinput></literallayout> 
	    On some distributions and architectures, you may be
	    required to enter:
	    <literallayout><userinput>/usr/lib64/heartbeat/hb_takeover</userinput></literallayout>
	  </para>
	</formalpara>
      </section>
      <section id="s-heartbeat-r1-relinquish-resources">
	<title>Relinquishing cluster resources</title>
	<para>A Heartbeat R1-style cluster node may be forced to give
	  up its resources in several ways.</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Switching a cluster node to standby
		mode</title>
	      <para>This is the approach normally taken if one simply
		wishes to test resource migration, or perform some
		other activity that does not require the node to leave
		the cluster. This operation is performed using the
		following command:
		<literallayout><userinput>/usr/lib/heartbeat/hb_standby</userinput></literallayout> 
		On some distributions and architectures, you may be
		required to enter:
		<literallayout><userinput>/usr/lib64/heartbeat/hb_standby</userinput></literallayout>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-heartbeat-r1-shutdown-local-cluster-manager">
	      <title>Shutting down the local cluster manager
		instance</title>
	      <para>This approach is suited for local maintenance
		operations such as software updates which require that
		the node be temporarily removed from the cluster, but
		which do not necessitate a system reboot. It involves
		shutting down all processes associated with the local
		cluster manager instance:
		<literallayout><userinput>/etc/init.d/heartbeat stop</userinput></literallayout> 
		Prior to stopping its services, Heartbeat will
		gracefully migrate any currently running resources to
		the peer node. This is the approach to be followed,
		for example, if you are upgrading DRBD to a new
		release, without also upgrading your kernel.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-heartbeat-r1-shutdown-local-node">
	      <title>Shutting down the local node</title>
	      <para>For hardware maintenance or other interventions
		that require a system shutdown or reboot, use a simple
		graceful shutdown command, such as
		<literallayout><userinput>reboot</userinput></literallayout> 
		or
		<literallayout><userinput>poweroff</userinput></literallayout> 
		Since Heartbeat services will be shut down gracefully
		in the process of a normal system shutdown, the
		previous paragraph applies to this situation, too.
		This is also the approach you would use in case of a
		kernel upgrade (which also requires the installation
		of a matching DRBD version).</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
    </section>
  </section>
  <section id="s-heartbeat-crm">
    <title>Using DRBD in Heartbeat CRM-enabled clusters</title>
    <para>Running Heartbeat clusters in CRM configuration mode is the
      recommended approach as of Heartbeat release 2 (per the Linux-HA
      development team).</para>
    <formalpara id="fp-heartbeat-crm-advantages">
      <title>Advantages</title>
      <para>Disadvantages of using CRM configuration mode, as opposed
	to R1 compatible configuration, include:
	<itemizedlist>
	  <listitem>
	    <para>Cluster configuration is distributed cluster-wide
	      and automatically, by the Cluster Resource Manager. It
	      need not be propagated manually.</para>
	  </listitem>
	  <listitem>
	    <para>CRM mode supports both node-level and resource-level
	      monitoring, and configurable responses to both node and
	      resource failure. It is still advisable to also monitor
	      cluster resources using an external monitoring
	      system.</para>
	  </listitem>
	  <listitem>
	    <para>CRM clusters support any number of resource groups,
	      as opposed to Heartbeat R1-style clusters which only
	      support two.</para>
	  </listitem>
	  <listitem>
	    <para>CRM clusters support a powerful (if complex)
	      constraints framework. This enables you to ensure
	      correct resource startup and shutdown order, resource
	      co-location (forcing resources to always run on the same
	      physical node), and to set preferred nodes for
	      particular resources.</para>
	  </listitem>
	</itemizedlist> Another advantage, namely the fact that CRM
	clusters support up to 255 nodes in a single cluster, is
	somewhat irrelevant for setups involving DRBD (DRBD itself
	being limited to two nodes).</para>
    </formalpara>
    <formalpara id="fp-heartbeat-crm-disadvantages">
      <title>Disadvantages</title>
      <para>Configuring Heartbeat in CRM mode also has some
	disadvantages in comparison to using R1-compatible
	configuration, notably
	<itemizedlist>
	  <listitem>
	    <para>Complexity of configuration and
	      administration,</para>
	  </listitem>
	  <listitem>
	    <para>Complexity of functionality extension with custom
	      resource agents.</para>
	  </listitem>
	</itemizedlist>
      </para>
    </formalpara>
    <section id="s-heartbeat-crm-config">
      <title>Heartbeat CRM configuration</title>
      <para>In CRM clusters, Heartbeat keeps part of configuration in the
	following configuration files:
	<itemizedlist>
	  <listitem>
	    <para><filename>/etc/ha.d/ha.cf</filename>, as described
	      in <xref linkend="s-heartbeat-hacf"/>. You must include
	      the following line in this configuration file to enable
	      CRM mode:
	      <programlisting>crm yes</programlisting></para>
	  </listitem>
	  <listitem>
	    <para><filename>/etc/ha.d/authkeys</filename>. The
	      contents of this file are the same as for R1 style
	      clusters. See <xref
		linkend="s-heartbeat-authkeys"/> for details.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <para>The remainder of the cluster configuration is maintained
	in the <emphasis>Cluster Information Base</emphasis>
	(<acronym>CIB</acronym>), covered in detail in <link
	  linkend="s-heartbeat-cib">the following section</link>.
	Contrary to the two relevant configuration files, the CIB need
	not be manually distributed among cluster nodes; the Heartbeat
	services take care of that automatically.</para> 
      <section id="s-heartbeat-cib">
	<title>The Cluster Information Base</title>
	<para>The Cluster Information Base (<acronym>CIB</acronym>) is
	  kept in one XML file,
	  <filename>/var/lib/heartbeat/crm/cib.xml</filename>. It is,
	  however, not recommended to edit the contents of this file
	  directly, except in the case of creating a new cluster
	  configuration from scratch. Instead, Heartbeat comes with
	  both command-line applications and a GUI modify the CIB.
	</para>
	<para>The CIB actually contains both the cluster
	  <emphasis>configuration</emphasis> (which is persistent and
	  is kept in the <filename>cib.xml</filename> file), and
	  information about the current cluster
	  <emphasis>status</emphasis> (which is volatile). Status
	  information, too, may be queried either using Heartbeat
	  command-line tools, and the Heartbeat GUI.</para>
	<para>After creating a new Heartbeat CRM cluster &mdash; that is,
	  creating the <filename>ha.cf</filename> and
	  <filename>authkeys</filename> files, distributing them among
	  cluster nodes, starting Heartbeat services, and waiting for
	  nodes to establish intra-cluster communications &mdash; a
	  new, empty CIB is created automatically. Its contents will
	  be similar to this:</para>
	<programlisting><![CDATA[
 <cib>
   <configuration>
     <crm_config>
       <cluster_property_set id="cib-bootstrap-options">
         <attributes/>
       </cluster_property_set>
     </crm_config>
     <nodes>
       <node uname="alice" type="normal" 
             id="f11899c3-ed6e-4e63-abae-b9af90c62283"/>
       <node uname="bob" type="normal" 
             id="663bae4d-44a0-407f-ac14-389150407159"/>
     </nodes>
     <resources/>
     <constraints/>
   </configuration>
 </cib>
]]></programlisting>
	<para>The exact format and contents of this file are
	  documented at length <ulink
	    url="http://wiki.linux-ha.org/ClusterInformationBase/UserGuide">in 
	    the Linux-HA wiki</ulink>, but for practical purposes it
	  is important to understand that this cluster has two nodes
	  named <code>alice</code> and <code>bob</code>, and that
	  neither any resources nor any resource constraints have been
	  configured at this point.</para>
      </section>
      <section>
	<title>Adding a DRBD-backed service to the cluster
	configuration</title>
	<para>This section explains how to enable a DRBD-backed
	  service in a Heartbeat CRM cluster. The examples used in
	  this section mimic, in functionality, those described in
	  <xref
	linkend="s-heartbeat-resources"/>, dealing with R1-style
	  Heartbeat clusters.</para>
	<para>The complexity of the configuration steps described in
	  this section may seem overwhelming to some, particularly
	  those having previously dealt only with R1-style Heartbeat
	  configurations. While the configuration of Heartbeat CRM
	  clusters is indeed complex (and sometimes not very
	  user-friendly), <link
	    linkend="fp-heartbeat-crm-advantages">the CRM's
	    advantages</link> may outweigh <link
	    linkend="fp-heartbeat-r1-advantages">those of R1-style
	    clusters</link>. Which approach to follow is entirely up
	  to the administrator's discretion.</para>
	<para>In order to enable the same DRBD-backed configuration
	for a MySQL database in a Heartbeat CRM cluster, you would use
	a configuration like this:
	  <programlisting><![CDATA[
<group ordered="true" collocated="true" id="rg_mysql">
  <primitive class="heartbeat" type="drbddisk" 
             provider="heartbeat" id="drbddisk_mysql">
    <instance_attributes>
      <attributes>
        <nvpair name="target_role" value="started"/>
        <nvpair name="1" value="mysql"/>
      </attributes>
    </instance_attributes>
  </primitive>
  <primitive class="ocf" type="Filesystem" 
             provider="heartbeat" id="fs_mysql">
    <instance_attributes>
      <nvpair name="device" value="/dev/drbd0"/>
      <nvpair name="directory" value="/var/lib/mysql"/>
      <nvpair name="type" value="ext3"/>
    </instance_attributes>
  </primitive>
  <primitive class="ocf" type="IPaddr" 
             provider="heartbeat" id="ip_mysql">
    <instance_attributes>
      <nvpair name="ip" value="192.168.42.1"/>
    </instance_attributes>
  </primitive>
  <primitive class="lsb" type="mysql" 
             provider="heartbeat" id="mysql"/>
</group>
]]></programlisting></para>
	<para>Assuming you created this configuration in a temporary
	  file named <filename>/tmp/hb_mysql.xml</filename>, you would
	  add this resource group to the cluster configuration using
	  the following command (on any cluster node):
	  <literallayout><userinput>cibadmin -o resources -C -x <filename>/tmp/hb_mysql.xml</filename></userinput></literallayout> 
	  After this, Heartbeat will automatically propagate the
	  newly-configured resource group to all cluster nodes.
	<note>
	    <para>There are numerous other ways to enable the above
	      example configuration; the approach shown is by no means
	      authoritative.</para>
	    <para> For example, you might use the <code>drbd</code>
	      OCF resource agent instead of the R1-compatible
	      <code>drbddisk</code> resource agent mentioned here.
	      Likewise, you may want to create four otherwise
	      unrelated resources and then establish colocation and
	      order constraints to make sure they are started in the
	      correct fashion. You may also create place constraints
	      to select a preferred node for a resource.</para>
	</note></para>
      </section>
    </section>
    <section id="s-heartbeat-crm-manage">
      <title>Managing Heartbeat CRM clusters</title>
      <section id="s-heartbeat-crm-assume-resources">
	<title>Assuming control of cluster resources</title>
	<para>A Heartbeat CRM cluster node may assume control of
	 cluster resources in the following ways:</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Manual takeover of a single cluster
		resource</title>
	      <para>This is the approach normally taken if one simply
		wishes to test resource migration, or move a resource
		to the local node as a means of manual load balancing.
		This operation is performed using the following
		command:
		<literallayout><userinput>crm_resource -r <replaceable>resource</replaceable> -M -H `uname -n`</userinput></literallayout> 
		<note id="n-heartbeat-crm-migrate">
		  <para>The <option>-M</option> (or
		    <option>--migrate</option>) option for the
		    <command>crm_resource</command> command, when used
		    without the <option>-H</option> option, implies a
		    resource migration <emphasis>away</emphasis> from
		    the local host. You must initiate a migration
		    <emphasis>to</emphasis> the local host by
		    specifying the <option>-H</option> option, giving
		    the local host name as the option argument.</para>
		  <para>It is also important to understand that the
		    migration is <emphasis>permanent</emphasis>, that
		    is, unless told otherwise, Heartbeat will not move
		    the resource back to a node it was previouly
		    migrated away from &mdash; even if that node
		    happens to be the only surviving node in a
		    near-cluster-wide system failure. This is
		    undesirable under most circumstances. So, it is
		    prudent to immediately <quote>un-migrate</quote>
		    resources after successful migration, using the
		    the following command:
		    <literallayout><userinput>crm_resource -r <replaceable>resource</replaceable> -U</userinput></literallayout> 
		  </para>
		  <para>Finally, it is important to know that during
		    resource migration, Heartbeat may simultaneously
		    migrate resources other than the one explicitly
		    specified (as required by existing resource groups
		    or colocation and order constraints).
		  </para>
		</note>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Manual takeover of all cluster resources</title>
	      <para>This procedure involves switching the peer node to
		standby mode (where
		<replaceable>hostname</replaceable> is the peer node's
		host name):
		<literallayout><userinput>crm_standby -U <replaceable>hostname</replaceable> -v on</userinput></literallayout>
	      </para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
      <section id="s-heartbeat-crm-relinquish-resources">
	<title>Relinquishing cluster resources</title>
	<para>A Heartbeat CRM cluster node may be forced to give up
	  one or all of its resources in several ways.</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Giving up a single cluster resource</title>
	      <para>A node gives up control of a single resource when
		issued the following command (note that <link
		  linkend="n-heartbeat-crm-migrate">the considerations
		  outlined in the previous section</link> apply here,
		too):
		<literallayout><userinput>crm_resource -r <replaceable>resource</replaceable> -M </userinput></literallayout> 
		If you want to migrate to a specific host, use this
		variant:
		<literallayout><userinput>crm_resource -r <replaceable>resource</replaceable> -M -H <replaceable>hostname</replaceable></userinput></literallayout> 
		However, the latter syntax is usually of little
		relevance to CRM clusters using DRBD, DRBD being
		limited to two nodes (so the two variants are,
		essentially, identical in meaning).
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Switching a cluster node to standby
		mode</title>
	      <para>This is the approach normally taken if one simply
		wishes to test resource migration, or perform some
		other activity that does not require the node to leave
		the cluster. This operation is performed using the
		following command:
		<literallayout><userinput>crm_standby -U `uname -n` -v on</userinput></literallayout>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Shutting down the local cluster manager
		instance</title>
	      <para>This approach is suited for local maintenance
		operations such as software updates which require that
		the node be temporarily removed from the cluster, but
		which do not necessitate a system reboot. The
		procedure is <link
		  linkend="fp-heartbeat-r1-shutdown-local-cluster-manager">the 
		  same as for Heartbeat R1 style
		  clusters</link>.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Shutting down the local node</title>
	      <para>For hardware maintenance or other interventions
		that require a system shutdown or reboot, use a simple
		graceful shutdown command, just as previously outlined
		<link
		  linkend="fp-heartbeat-r1-shutdown-local-node">for
		  Heartbeat R1 style clusters</link>.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
    </section>
  </section>
  <section id="s-heartbeat-dopd">
    <title>Using Heartbeat with <code>dopd</code></title>
    <para>The steps outlined in this section enable DRBD to deny
      services access to <link linkend="s-outdate">outdated
	data</link>. The Heartbeat component that implements this
      functionality is the <emphasis>DRBD outdate-peer
	daemon</emphasis>, or <code>dopd</code> for short. It works,
      and uses identical configuration, on both <link
	linkend="s-heartbeat-r1">R1-compatible</link> and <link
	linkend="s-heartbeat-crm">CRM</link> clusters.
      <important>
	<para>It is absolutely vital to configure at least two
	  independent <link
	    linkend="s-heartbeat-communication-channels">Heartbeat
	    communication channels</link> for <code>dopd</code> to
	  work correctly.</para>
      </important></para>
    <section id="s-dopd-heartbeat-config">
      <title>Heartbeat configuration</title>
      <para>To enable dopd, you must add these lines to your
	<filename>/etc/ha.d/ha.cf</filename> file:
	<programlisting>respawn hacluster /usr/lib/heartbeat/dopd 
apiauth dopd gid=haclient uid=hacluster</programlisting> 
	You may have to adjust <code>dopd</code>'s path according to
	your preferred distribution. On some distributions and
	architectures, the correct path is
	<filename>/usr/lib64/heartbeat/dopd</filename>.</para>
      <para>After having made this change and copied
	<filename>ha.cf</filename> to the peer node, you must run
	<command>/etc/init.d/heartbeat reload</command> to have
	Heartbeat re-read its configuration file. Afterwards, you
	should be able to verify that you now have a running
	<code>dopd</code> process.<footnote>
	  <para>You can check for this process either by running
	    <command>ps ax | grep dopd</command>
	    or by issuing
	    <command>killall -0 dopd</command>.</para>
	</footnote>
      </para>
    </section>
    <section id="s-dopd-drbd-config">
      <title>DRBD Configuration</title>
      <para>Then, add these items to your DRBD resource configuration:
	<programlisting>resource <replaceable>resource</replaceable> {
    handlers {
        outdate-peer "/usr/lib/heartbeat/drbd-peer-outdater";
        ...
    }
    disk {
        fencing resource-only;
        ...
    }
    ...
}</programlisting>As with <code>dopd</code>, your
	distribution may place the
	<filename>drbd-peer-outdater</filename> binary in
	<filename>/usr/lib64/heartbeat</filename> depending on your
	system architecture.</para>
      <para>Finally, copy your <filename>drbd.conf</filename> to the
	peer node and issue
	<command>drbdadm adjust <replaceable>resource</replaceable></command> 
	to reconfigure your resource and reflect your changes.</para>
    </section>
    <section>
      <title>Testing <code>dopd</code> functionality</title>
      <para>To test whether your <code>dopd</code> setup is working
	correctly, interrupt the replication link of a configured and
	connected resource while Heartbeat services are running
	normally. You may do this by either physically unplugging the
	network link, or by running
	<command>ifconfig <replaceable>interface</replaceable> down</command> 
	on the interface used for DRBD replication.
	<caution>
	  <para>This step is invasive; it affects
	    <emphasis>all</emphasis> enabled DRBD resources, not just
	    the one you are testing.</para>
	</caution> After this, you will be able to observe the
	resource <link linkend="s-connection-states">connection
	  state</link> change from <code>Connected</code> to
	<code>WFConnection</code>. Allow a few seconds to pass, and
	you should see the <link linkend="s-disk-states">disk
	  state</link> become <code>Outdated/DUnknown</code>. That is
	what <code>dopd</code> is responsible for.</para>
      <para>Any attempt to switch the outdated resource to the primary
	role will fail after this.</para>
      <para>When re-instituting network connectivity (either by
	plugging the physical link or by issuing
	<command>ifconfig <replaceable>interface</replaceable> up</command>), 
	the connection state will change to <code>Connected</code>,
	and then promptly to <code>SyncTarget</code> (assuming changes
	occurred on the primary node during the network interruption).
	Then you will be able to observe a brief resynchronization
	period, and finally, the previously outdated resource will be
	marked as <code>UpToDate</code> again.</para>
    </section>
  </section>
</chapter>
