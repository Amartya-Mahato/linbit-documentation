<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-heartbeat" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Integrating DRBD with Heartbeat clusters</title>
  <para>Using DRBD in conjunction with the Linux-HA cluster manager
    ("Heartbeat") is arguably DRBD's most frequently found use case.
    Heartbeat is also one of the applications that make DRBD extremely
    powerful in a wide variety of usage scenarios. Hence, this is one
    of the more detailed chapters in this guide.</para>
  <para>This chapter describes using DRBD as replicated storage for
    Linux-HA High Availability clusters. It covers both
    traditionally-configured, Heartbeat release 1-compatible clusters,
    and the more advanced CRM-enabled Heartbeat 2 clusters.</para>
  <section id="s-heartbeat-primer">
    <title>Heartbeat primer</title>
    <section id="s-heartbeat-cluster-manager">
      <title>The Heartbeat cluster manager</title>
      <para>Heartbeat's purpose as a cluster manager is to ensure that
	the cluster maintains its services to the clients, even if
	single machines of the cluster fail. Applications that may be
	managed by Heartbeat as cluster services include, for example,
	<itemizedlist>
	  <listitem>
	    <para>a web server such as Apache,</para>
	  </listitem>
	  <listitem>
	    <para>a database server such as MySQL, Oracle, or
	    PostgreSQL,</para>
	  </listitem>
	  <listitem>
	    <para>a file server such as NFS or Samba, and many
	      others.</para>
	  </listitem>
	</itemizedlist> In essence, any server application may be
	managed by Heartbeat as a cluster service.</para>
      <para>Services managed by Heartbeat are typically removed from
	the system startup configuration; rather than being started at
	boot time, they are be started and stopped by the cluster
	manager as required by the cluster configuration. If a machine
	(a physical cluster node) fails while running a particular set
	of services, Heartbeat will start the failed services on
	another machine in the cluster. These operations performed by
	Heartbeat are commonly referred to as (automatic)
	<emphasis>fail-over</emphasis>.</para>
      <para>A migration of cluster services from one cluster node to
	another, by manual intervention, is commonly termed "manual
	fail-over". This being a slightly self-contradictory term, we
	use the alternative term <emphasis>switch-over</emphasis> for
	the purposes of this guide.</para>
      <para>Heartbeat is also capable of automatically migrating
	resources back to a previously failed node, as soon as the
	latter recovers. This process is called
	<emphasis>fail-back</emphasis>.</para>
    </section>
    <section id="s-heartbeat-resources">
      <title>Heartbeat resources</title>
      <para>Usually, there will be certain requirements in order to be able to start a
      cluster service managed by Heartbeat on a node. Consider the
      example of a complex, database-driven web application:
	<itemizedlist>
	  <listitem>
	    <para>Both the web server and the database server assume
	      that their designated <emphasis>IP addresses</emphasis>
	      are available (i.e. configured) on the node.</para>
	  </listitem>
	  <listitem>
	    <para>The database will require a <emphasis>file
	    system</emphasis> to retrieve data files from.</para>
	  </listitem>
	  <listitem>
	    <para>That file system will require its underlying
	      <emphasis>block device</emphasis> to read from and write
	      to (this is where DRBD comes in, as we will see later).</para>
	  </listitem>
	  <listitem>
	    <para>The web server will also depend on the database
	    being started, assuming it cannot server dynamic content
	    without an available database.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <para>The services Heartbeat controls, and any additional
	requirements those services depend on, are referred to as
	<emphasis>resources</emphasis> in Heartbeat terminology. Where
	resources form a co-dependent collection, that collection is
	called a <emphasis>resource group</emphasis>.</para>
    </section>
    <section id="s-resource-agents">
      <title>Heartbeat resource agents</title>
      <para>Heartbeat manages resources by way of invoking
	standardized shell scripts known as <emphasis>resource
	  agents</emphasis> (RA's). In Heartbeat R1-style clusters,
	the following resource agent types are available:
	<itemizedlist>
	  <listitem>
	    <formalpara id="fp-heartbeat-ra">
	      <title>Heartbeat resource agents</title>
	      <para>These agents are found in the
		<filename>/etc/ha.d/resource.d</filename> directory.
		They may take zero or more positional, unnamed
		parameters, and one operation argument
		(<code>start</code>, <code>stop</code>, or
		<code>status</code>). Heartbeat translates resource
		parameters it finds for a matching resource in
		<filename>/etc/ha.d/haresources</filename> into
		positional parameters for the RA, which then uses
		these to configure the resource.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-lsb-ra">
	      <title>LSB resource agents</title>
	      <para>These are conventional, Linux Standard
		Base-compliant init scripts found in
		<filename>/etc/init.d</filename>, which Heartbeat
		simply invokes with the <code>start</code>,
		<code>stop</code>, or <code>status</code> argument.
		They take no positional parameters. Thus, the
		corresponding resources' configuration cannot be
		managed by Heartbeat; these services are expected to
		be configured by conventional configuration
		files.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara id="fp-ocf-ra">
	      <title>OCF resource agents</title>
	      <para>These are resource agents that conform to the
		guidelines of the Open Cluster Framework. They are
		usually found in either
		<filename>/usr/lib/ocf/heartbeat/resource.d</filename>
		or
		<filename>/usr/lib64/ocf/heartbeat/resource.d</filename>, 
		depending on system architecture and distribution.
		They take no positional parameters, but may be
		extensively configured via environment variables that
		the cluster management process derives from the
		cluster configuration, and passes in to the resource
		agent upon invocation. OCF resource agents are only
		available in CRM-enabled Heartbeat clusters.</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </para>
    </section>
    <section id="s-heartbeat-communication-channels">
      <title>Heartbeat communication channels</title>
      <para>Heartbeat uses a UDP-based communication protocol to
	periodically check for node availability (the "heartbeat"
	proper). For this purpose, Heartbeat can use several
	communication methods, including:
	<itemizedlist>
	  <listitem>
	    <para>IP multicast,</para>
	  </listitem>
	  <listitem>
	    <para>IP broadcast,</para>
	  </listitem>
	  <listitem>
	    <para>IP unicast,</para>
	  </listitem>
	  <listitem>
	    <para>serial line.</para>
	  </listitem>
	</itemizedlist>Of these, IP multicast and IP broadcast are the
	most relevant in practice. The absolute minimum requirement
	for stable cluster operation is two independent communication
	channels.</para>
    </section>
  </section>
  <section id="s-heartbeat-config">
    <title>Heartbeat configuration</title>
    <para>For any Heartbeat cluster, the following configuration files
      must be available:
      <itemizedlist>
	<listitem>
	  <para><filename>/etc/ha.d/ha.cf</filename> &mdash; global
	    cluster configuration.</para>
	</listitem>
	<listitem>
	  <para><filename>/etc/ha.d/authkeys</filename> &mdash; keys
	    for mutual node authentication.</para>
	</listitem>
      </itemizedlist>
    </para>
    <para>Depending on whether Heartbeat is running in R1-compatible
      or in CRM mode, additional configuration files are required.
      These are covered in <xref
	  linkend="s-heartbeat-r1"/> and <xref
	  linkend="s-heartbeat-crm"/>.</para>
    <section id="s-heartbeat-hacf">
      <title>The <filename>ha.cf</filename> file</title>
      <para>The following example is a small and simple
	<filename>ha.cf</filename> file:
<programlisting>autojoin none 
mcast bond0 239.0.0.43 694 1 0 
bcast eth2 
warntime 15
deadtime 30 
initdead 120 
keepalive 2 
node joe 
node jane
crm no</programlisting></para>
      <para>Setting <option>autojoin</option> to <code>none</code>
	disables cluster node auto-discovery and requires that cluster
	nodes be listed explicitly, using the <option>node</option>
	options. This speeds up cluster start-up in clusters with a
	fixed number of nodes (which is always the case in R1-style
	Heartbeat clusters).</para>
      <para>This example assumes that <code>bond0</code> is the
	cluster's interface to the shared network, and that
	<code>eth2</code> is the interface dedicated for DRBD
	replication between both nodes. Thus, <code>bond0</code> can
	be used for Multicast heartbeat, whereas on <code>eth2</code>
	broadcast is acceptable as <code>eth2</code> is not a shared
	network.</para>
      <para>The next options configure node failure detection. They
	set the time after which Heartbeat issues a warning that a no
	longer available peer node <emphasis>may</emphasis> be dead
	(<option>warntime</option>), the time after which Heartbeat
	considers a node <emphasis>confirmed</emphasis> dead
	(<option>deadtime</option>), and the maximum time it waits for
	other nodes to check in at cluster startup
	(<option>initdead</option>). <option>keepalive</option> sets
	the interval at which Heartbeat keep-alive packets are sent.
	All these options are given in seconds.</para>
      <para>The <option>node</option> option identifies cluster
	members. The option values listed here must match the exact
	host names of cluster nodes as given by <command>uname
	  -n</command>.</para>
      <para><code>crm no</code> indicates that this cluster is a <link
	  linkend="s-heartbeat-r1">R1-compatible cluster</link> with
	CRM disabled. We this option set to <code>yes</code>,
	Heartbeat would be running in <link
	  linkend="s-heartbeat-crm">CRM mode</link>.</para>
    </section>
    <section id="s-heartbeat-authkeys">
      <title>The <filename>authkeys</filename> file</title>
      <para><filename>/etc/ha.d/authkeys</filename> contains
	pre-shared secrets used for mutual cluster node
	authentication. It should only be readable by
	<code>root</code>and follows this format: 
<programlisting>auth <replaceable>num</replaceable>
<replaceable>num</replaceable> <replaceable>algorithm</replaceable> <replaceable>secret</replaceable></programlisting>
      </para>
      <para><replaceable>num</replaceable> is a simple key index,
	starting with 1. Usually, you will only have one key in your
	<filename>authkeys</filename> file.</para>
      <para><replaceable>algorithm</replaceable> is the signature
	algorithm being used. You may use either <code>md5</code> or
	<code>sha1</code>; the use of <code>crc</code> (a simple
	cyclic redundancy check, not secure) is not
	recommended.</para>
      <para><replaceable>secret</replaceable> is the actual
	authentication key.</para>
      <para>You may create an <filename>authkeys</filename> file,
	using a generated secret, with the following shell
	hack:</para>
      <programlisting>echo -e "auth 1\n1 sha1 $(date | openssl md5)" &gt;/etc/ha.d/authkeys</programlisting>
    </section>
  </section>
  <section id="s-heartbeat-r1">
    <title>Using DRBD in Heartbeat R1-style clusters</title>
    <para>Running Heartbeat clusters in release 1 compatible
      configuration is now considered obsolete by the Linux-HA
      development team. However, it is still widely used in the field,
      which is why it is documented here in this section.</para>
    <formalpara>
      <title>Advantages</title>
      <para>Configuring Heartbeat in R1 compatible mode has some
	advantages over using CRM configuration, notably
	<itemizedlist>
	  <listitem>
	    <para>Simplicity of configuration,</para>
	  </listitem>
	  <listitem>
	    <para>Ease of functionality extension with custom resource
	      agents.</para>
	  </listitem>
	</itemizedlist>
      </para>
    </formalpara>
    <formalpara>
      <title>Disadvantages</title>
      <para>Disadvantages of R1 compatible configuration, as opposed
	to CRM configurations, include:
	<itemizedlist>
	  <listitem>
	    <para>Cluster configuration must be manually kept in sync
	      between cluster nodes, it is not propagated
	      automatically.</para>
	  </listitem>
	  <listitem>
	    <para>While node monitoring is available, resource-level
	      monitoring is not. Individual resources must be
	      monitored by an external monitoring system.</para>
	  </listitem>
	  <listitem>
	    <para>Resource group support is limited to two resource
	      groups. CRM clusters, by contrast, support any number,
	      and also come with a complex resource-level constraint
	      framework.</para>
	  </listitem>
	</itemizedlist> Another disadvantage, namely the fact that R1
	style configuration limits cluster size to 2 nodes (whereas
	CRM cluster support up to 255) is largely irrelevant for
	setups involving DRBD, DRBD itself being limited to two
	nodes.</para>
    </formalpara>
    <section id="s-heartbeat-r1-config">
      <title>Heartbeat R1-style configuration</title>
      <para>In R1-style clusters, Heartbeat keeps its complete
	configuration in three simple configuration files:
	<itemizedlist>
	  <listitem>
	    <para><filename>/etc/ha.d/ha.cf</filename>, as described
	      in <xref linkend="s-heartbeat-hacf"/>.</para>
	  </listitem>
	  <listitem>
	    <para><filename>/etc/ha.d/authkeys</filename>, as described
	      in <xref linkend="s-heartbeat-authkeys"/>.</para>
	  </listitem>
	  <listitem>
	    <para><filename>/etc/ha.d/haresources</filename> &mdash;
	      the resource configuration file, described below.</para>
	  </listitem>
	</itemizedlist>
      </para>
      <section id="s-heartbeat-haresources">
	<title>The <filename>haresources</filename> file</title>
	<para>The following is an example of a Heartbeat R1-compatible
	resource configuration involving a MySQL database backed by
	DRBD:
	  <programlisting>joe drbddisk::mysql Filesystem::/dev/drbd0::/var/lib/mysql::ext3 \
    192.168.42.1 mysql</programlisting></para>
	<para>This resource configuration contains one resource group
	  whose <emphasis>home node</emphasis> (the node where its
	  resources are expected to run under normal circumstances) is
	  named <code>joe</code>.<footnote>
	    <para>Consequentially, this resource group would be
	      considered the <emphasis>local</emphasis> resource group
	      on host <code>joe</code>, whereas it would be the
	      <emphasis>foreign</emphasis> resource group on its peer
	      host.</para>
	  </footnote>
	</para>
	<para>The resource group includes a DRBD resource named
	  <code>mysql</code>, which will be promoted to the primary
	  role by the cluster manager (specifically, the
	  <code>drbddisk</code> <link
	    linkend="fp-heartbeat-ra">resource agent</link>) on
	  whichever node is currently the active node. Of course, a
	  corresponding resource must exist and be configured in
	  <filename>/etc/drbd.conf</filename> for this to work.</para>
	<para>That DRBD resource translates to the block device named
	  <filename>/dev/drbd0</filename>, which contains an ext3
	  filesystem that is to be mounted at
	  <filename>/var/lib/mysql</filename> (the default location
	  for MySQL data files).</para>
	<para>The resource group also contains a service IP address,
	  192.168.42.1. Heartbeat will make sure that this IP address
	  is configured and available on whichever node is currently
	  active.</para>
	<para>Finally, Heartbeat will use the <link
	    linkend="fp-lsb-ra">LSB resource agent</link> named
	  <code>mysql</code> in order to start the MySQL daemon, which
	  will then find its data files at
	  <filename>/var/lib/mysql</filename> and be able to listen on
	  the service IP address, 192.168.42.1.</para>
	<para>It is important to understand that the resources listed
	  in the <filename>haresources</filename> file are always
	  evaluated from left to right when resources are being
	  started, and from right to left when they are being
	  stopped.</para>
      </section>
    </section>
    <section id="s-heartbeat-r1-manage">
      <title>Managing Heartbeat R1-style clusters</title>
      <section id="s-heartbeat-r1-assume-resources">
	<title>Assuming control of cluster resources</title>
	<para>A Heartbeat R1-style cluster node may assume control of
	  cluster resources in the following way:</para>
	<formalpara>
	  <title>Manual resource takeover</title>
	  <para>This is the approach normally taken if one simply
	    wishes to test resource migration, or assume control
	    of resources for any reason other than the peer having
	    to leave the cluster. This operation is performed
	    using the following command:
	    <literallayout><userinput>/usr/lib/heartbeat/hb_takeover</userinput></literallayout> 
	    On some distributions and architectures, you may be
	    required to enter:
	    <literallayout><userinput>/usr/lib64/heartbeat/hb_takeover</userinput></literallayout>
	  </para>
	</formalpara>
      </section>
      <section id="s-heartbeat-r1-relinquish-resources">
	<title>Relinquishing cluster resources</title>
	<para>A Heartbeat R1-style cluster node may be forced to give
	  up its resources in several ways.</para>
	<itemizedlist>
	  <listitem>
	    <formalpara>
	      <title>Switching a cluster node to standby
		mode</title>
	      <para>This is the approach normally taken if one simply
		wishes to test resource migration, or perform some
		other activity that does not require the node to leave
		the cluster. This operation is performed using the
		following command:
		<literallayout><userinput>/usr/lib/heartbeat/hb_standby</userinput></literallayout> 
		On some distributions and architectures, you may be
		required to enter:
		<literallayout><userinput>/usr/lib64/heartbeat/hb_standby</userinput></literallayout>
	      </para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Shutting down the local cluster manager
		instance</title>
	      <para>This approach is suited for local maintenance
		operations such as software updates which require that
		the node be temporarily removed from the cluster, but
		which do not necessitate a system reboot. It involves
		shutting down all processes associated with the local
		cluster manager instance:
		<literallayout><userinput>/etc/init.d/heartbeat&nbsp;stop</userinput></literallayout> 
		Prior to stopping its services, Heartbeat will
		gracefully migrate any currently running resources to
		the peer node. This is the approach to be followed,
		for example, if you are upgrading DRBD to a new
		release, without also upgrading your kernel.</para>
	    </formalpara>
	  </listitem>
	  <listitem>
	    <formalpara>
	      <title>Shutting down the local node</title>
	      <para>For hardware maintenance or other interventions
		that require a system shutdown or reboot, use a simple
		graceful shutdown command, such as
		<literallayout><userinput>reboot</userinput></literallayout> 
		or
		<literallayout><userinput>poweroff</userinput></literallayout> 
		Since Heartbeat services will be shut down gracefully
		in the process of a normal system shutdown, the
		previous paragraph applies to this situation, too.
		This is also the approach you would use in case of a
		kernel upgrade (which also requires the installation
		of a matching DRBD version).</para>
	    </formalpara>
	  </listitem>
	</itemizedlist>
      </section>
    </section>
  </section>
  <section id="s-heartbeat-crm">
    <title>Using DRBD in Heartbeat CRM-enabled clusters</title>
    <xi:include href="todo.xml"/>
  </section>
</chapter>
