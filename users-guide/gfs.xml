<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-gfs" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Using GFS with DRBD</title>
  <indexterm>
    <primary>GFS</primary>
  </indexterm>
  <indexterm>
    <primary>Global File System</primary>
    <see>GFS</see>
  </indexterm>
  <para>This chapter outlines the steps necessary to set up a DRBD
  resource as a block device holding a shared Global File System
  (GFS). It covers both GFS and GFS2.</para>
  <para>In order to use GFS on top of DRBD, you must configure DRBD in
    <indexterm>
      <primary>dual-primary mode</primary>
    </indexterm> <link linkend="s-dual-primary-mode">dual-primary
      mode</link>, which is available in DRBD 8.0 and later.</para>
  <section id="s-gfs-primer">
    <title>GFS primer</title>
    <para>The Red Hat Global File System (GFS) is Red Hat's
      implementation of a concurrent-access shared storage file
      system. As any such filesystem, GFS allows multiple nodes to
      access the same storage device, in read/write fashion,
      simultaneously without risking data corruption. It does so by
      using a Distributed Lock Manager (DLM) which manages concurrent
      access from cluster members.</para>
    <para>GFS was designed, from the outset, for use with conventional
      shared storage devices. Regardless, it is perfectly possible to
      use DRBD, in dual-primary mode, as a replicated storage device
      for GFS. Applications may benefit from reduced read/write
      latency due to the fact that DRBD normally reads from and writes
      to local storage, as opposed to the SAN devices GFS is normally
      configured to run from. Also, of course, DRBD adds an additional
      physical copy to every GFS filesystem, thus adding redundancy to
      the concept.</para>
    <para>GFS makes use of a cluster-aware variant of LVM,
      <indexterm>
	<primary>LVM</primary>
      </indexterm>termed Cluster Logical Volume Manager or CLVM. As
      such, some parallelism exists between using DRBD as the data
      storage for GFS, and using <link linkend="s-lvm-drbd-as-pv">DRBD
	as a Physical Volume for conventional LVM</link>.</para>
    <para>GFS file systems are usually tightly integrated with Red
      Hat's own cluster management framework, the
      <indexterm>
	<primary>Red Hat Cluster Suite</primary>
      </indexterm><link linkend="ch-rhcs">Red Hat Cluster Suite</link>
      (RHCS). This chapter explains the use of DRBD in conjunction
      with GFS in the RHCS context.</para>
    <para>GFS, CLVM, and the Red Hat Cluster Suite are available in
      Red Hat Enterprise Linux (RHEL) and distributions derived from
      it, such as <indexterm>
	<primary>CentOS</primary>
      </indexterm>CentOS. Packages built from the same sources are
      also available in <indexterm>
	<primary>Debian GNU/Linux</primary>
      </indexterm>Debian GNU/Linux. This chapter assumes running GFS
      on a Red Hat Enterprise Linux system.</para>
  </section>
  <section id="s-gfs-create-resource">
    <title>Creating a DRBD resource suitable for GFS</title>
    <para>Since GFS is a shared cluster file system expecting
      concurrent read/write storage access from all cluster nodes, any
      DRBD resource to be used for storing a GFS filesystem must be
      configured in <link linkend="s-dual-primary-mode">dual-primary
	mode</link>. Also, it is recommended to use some of DRBD's
      <link linkend="s-automatic-split-brain-recovery-configuration">features for
    automatic recovery from split brain</link>. And, it is necessary
    for the resource to switch to the primary role immediately after
    startup. To do all this, include the following lines in the
    resource configuration:
      <indexterm>
	<primary>drbd.conf</primary>
	<secondary>resource</secondary>
      </indexterm>
      <programlisting>resource <replaceable>resource</replaceable> {
  startup {
    become-primary-on both;
    ...
  }
  net {
    allow-two-primaries;
    after-sb-0pri discard-zero-changes;
    after-sb-1pri discard-secondary;
    after-sb-2pri disconnect;
    ...
  }
  ...
}</programlisting>
      </para>
    <para>
      Once you have added these options to <link
	linkend="ch-configure">your freshly-configured
	resource</link>, you may <link
	linkend="s-first-time-up">initialize your resource as you
	normally would</link>. Since the
      <indexterm>
	<primary>drbd.conf</primary>
	<secondary>allow-two-primaries</secondary>
      </indexterm>
      <option>allow-two-primaries</option> option is set for this
      resource, you will be able to <link
	linkend="s-switch-resource-roles">promote the resource</link>
      to the primary role on both nodes.</para>
  </section>
  <section id="s-gfs-configure-lvm">
    <title>Configuring LVM to recognize the DRBD resource</title>
    <para>GFS uses CLVM, the cluster-aware version of LVM, to manage
      block devices to be used by GFS. In order to use CLVM with DRBD,
      ensure that your LVM configuration
      <itemizedlist>
	<listitem>
	  <para>uses clustered locking. To do this, set the following
	    option in <filename>/etc/lvm/lvm.conf</filename>:</para>
	  <programlisting>locking_type = 3</programlisting>
	</listitem>
	<listitem>
	  <para>scans your DRBD devices to recognize DRBD-based
	  Physical Volumes (PVs). This applies as to conventional
	  (non-clustered) LVM; see <xref linkend="s-lvm-drbd-as-pv"/>
	  for details.</para>
	</listitem>
      </itemizedlist>
    </para>
  </section>
  <section id="s-gfs-enable">
    <title>Configuring your cluster to support GFS</title>
    <para>After you have created your new DRBD resource and <link
	linkend="s-rhcs-config">completed your initial cluster
	configuration</link>, you must enable and start the
      following system services on both nodes of your GFS cluster:
      <itemizedlist>
	<listitem>
	  <para><code>cman</code> (this also starts
	    <code>ccsd</code> and <code>fenced</code>),</para>
	</listitem>
	<listitem>
	  <para><code>clvmd</code>.</para>
	</listitem>
      </itemizedlist>
    </para>
  </section>
  <section id="s-gfs-create">
    <title>Creating a GFS filesystem</title>
    <para>In order to create a GFS filesystem on your dual-primary
      DRBD resource, you must first initialize it as a <link
	linkend="s-lvm-primer">Logical Volume for LVM</link>.</para>
    <para>Contrary to conventional, non-cluster-aware LVM
      configurations, the following steps must be completed on only
      one node due to the cluster-aware nature of CLVM:
      <indexterm>
	<primary>LVM</primary>
	<secondary>pvcreate command</secondary>
      </indexterm>
      <indexterm>
	<primary>pvcreate (LVM command)</primary>
      </indexterm>
      <indexterm>
	<primary>LVM</primary>
	<secondary>vgcreate command</secondary>
      </indexterm>
      <indexterm>
	<primary>vgcreate (LVM command)</primary>
      </indexterm>
      <indexterm>
	<primary>LVM</primary>
	<secondary>lvcreate command</secondary>
      </indexterm>
      <indexterm>
	<primary>lvcreate (LVM command)</primary>
      </indexterm>
      <literallayout><userinput>pvcreate /dev/drbd/by-res/<replaceable>resource</replaceable></userinput>
<computeroutput>  Physical volume "/dev/drbd<replaceable>num</replaceable>" successfully created</computeroutput>
<userinput>vgcreate <replaceable>vg-name</replaceable> /dev/drbd/by-res/<replaceable>resource</replaceable></userinput>
<computeroutput>  Volume group "<replaceable>vg-name</replaceable>" successfully created</computeroutput>
<userinput>lvcreate --size <replaceable>size</replaceable> --name <replaceable>lv-name</replaceable> <replaceable>vg-name</replaceable></userinput>
<computeroutput>  Logical volume "<replaceable>lv-name</replaceable>" created</computeroutput></literallayout>
    </para>
    <para>CLVM will immediately notify the peer node of these changes;
      <indexterm>
	<primary>LVM</primary>
	<secondary>lvdisplay command</secondary>
      </indexterm>
      <indexterm>
	<primary>lvdisplay (LVM command)</primary>
      </indexterm>
      <indexterm>
	<primary>LVM</primary>
	<secondary>lvs command</secondary>
      </indexterm>
      <indexterm>
	<primary>lvs (LVM command)</primary>
      </indexterm>
      issuing <command>lvs</command> (or <command>lvdisplay</command>)
      on the peer node will list the newly created logical
      volume.</para>
    <para>
      <indexterm>
	<primary>GFS</primary>
	<secondary>creating a file system</secondary>
      </indexterm>Now, you may proceed by creating the actual filesystem:
      <literallayout><userinput>mkfs -t gfs -p lock_dlm -j 2 /dev/<replaceable>vg-name</replaceable>/<replaceable>lv-name</replaceable></userinput></literallayout>
      Or, for a GFS2 filesystem:
      <literallayout><userinput>mkfs -t gfs2 -p lock_dlm -j 2 -t <replaceable>cluster</replaceable>:<replaceable>name</replaceable> /dev/<replaceable>vg-name</replaceable>/<replaceable>lv-name</replaceable></userinput></literallayout></para>
    <para>The <option>-j</option> option in this command refers to the
      number of journals to keep for GFS. This must be identical to
      the number of nodes in the GFS cluster; since DRBD does not
      support more than two nodes, the value to set here is always
      2.</para>
    <para>The <option>-t</option> option, applicable only for GFS2
      filesystems, defines the lock table name. This follows the
      format
      <replaceable>cluster</replaceable>:<replaceable>name</replaceable>,
      where <replaceable>cluster</replaceable> must match your cluster
      name as defined in
      <filename>/etc/cluster/cluster.conf</filename>. Thus, only
      members of that cluster will be permitted to use the filesystem.
      By contrast, <replaceable>name</replaceable> is an arbitrary
      file system name unique in the cluster.</para>
  </section>
  <section id="s-gfs-use">
    <title>Using your GFS filesystem</title>
    <para>After you have created your filesystem, you may add it to
      <filename>/etc/fstab</filename>:
      <programlisting>/dev/<replaceable>vg-name</replaceable>/<replaceable>lv-name</replaceable> <replaceable>mountpoint</replaceable> gfs defaults 0 0</programlisting>
      For a GFS2 filesystem, simply change the defined filesystem type to:
      <programlisting>/dev/<replaceable>vg-name</replaceable>/<replaceable>lv-name</replaceable> <replaceable>mountpoint</replaceable> gfs2 defaults 0 0</programlisting>
      Do not forget to make this change on both cluster nodes.
    </para>
    <para>After this, you may mount your new filesystem by starting the
    <code>gfs</code> service (on both nodes):
      <indexterm>
	<primary>GFS</primary>
	<secondary>mounting a file system</secondary>
      </indexterm>
      <literallayout><userinput>service gfs start</userinput></literallayout>
    </para>
    <para>From then onwards, as long as you have DRBD configured to
      start automatically on system startup, before the RHCS services
      and the <code>gfs</code> service, you will be able to use this
      GFS file system as you would use one that is configured on
      traditional shared storage.
    </para>
  </section>

</chapter>
