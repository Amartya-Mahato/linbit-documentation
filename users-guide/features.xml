<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-features" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>DRBD Features</title>
  <para>
    <para>This chapter discusses various useful DRBD features, and
      gives some background information about them. Some of these
      features will be important to most users, some will only be
      relevant in very specific deployment scenarios.</para>
    <para><xref linkend="ch-admin"/> and <xref
      linkend="ch-troubleshooting"/> contain instructions on how to
      enable and use these features during day-to-day
      operation.</para>
  </para>
  <section id="s-single-primary-mode">
    <title>Single-primary mode</title>
    <para><indexterm>
	<primary>single-primary mode</primary>
    </indexterm>In single-primary mode, any <link
	linkend="s-resources">resource</link> is, at any given time,
      in the primary role on only one cluster member. Since it is thus
      guaranteed that only one cluster node manipulates the data at
      any moment, this mode can be used with any conventional file
      system (ext3, ext4, XFS etc.).</para>
    <para>Deploying DRBD in single-primary mode is the canonical
      approach for high availability (fail-over capable)
      clusters.</para>
    </section>
  <section id="s-dual-primary-mode">
    <title>Dual-primary mode</title>
    <para>This feature is available in DRBD 8.0 and later.</para>
    <para><indexterm>
	<primary>dual-primary mode</primary>
    </indexterm>In dual-primary mode, any resource is, at any given
      time, in the primary role on both cluster nodes. Since
      concurrent access to the data is thus possible, this mode
      requires the use of a shared cluster file system that utilizes a
      distributed lock manager. Examples include 
      <indexterm>
	<primary>GFS</primary>
      </indexterm> <link linkend="ch-gfs">GFS</link> and
      <indexterm>
	<primary>OCFS2</primary>
      </indexterm> OCFS2.</para>
    <para>Deploying DRBD in dual-primary mode is the preferred
      approach for load-balancing clusters which require concurrent
      data access from two nodes. This mode is disabled by default,
      and must be enabled explicitly in DRBD's configuration
      file.</para>
    <para>See <xref linkend="s-enable-dual-primary"/> for information
      on enabling dual-primary mode for specific resources.</para>
  </section>
  <section id="s-replication-protocols">
    <title>Replication modes</title>
    <para>DRBD supports three distinct replication modes, allowing
      three degrees of replication synchronicity.</para>
    <formalpara id="fp-protocol-a">
      <title>Protocol A</title>
      <indexterm>
	<primary>replication mode</primary>
	<secondary>asynchronous</secondary>
      </indexterm>
      <para>Asynchronous replication protocol. Local write operations
	on the primary node are considered completed as soon as the
	local disk write has occurred, and the replication packet has
	been placed in the local TCP send buffer. In the event of
	forced fail-over, data loss may occur. The data on the standby
	node is consistent after fail-over, however, the most recent
	updates performed prior to the crash could be lost.</para>
    </formalpara>
    <formalpara id="fp-protocol-b">
      <title>Protocol B</title>
      <indexterm>
	<primary>replication mode</primary>
	<secondary>semi-synchronous</secondary>
      </indexterm>
      <para>Memory synchronous (semi-synchronous) replication
	protocol. Local write operations on the primary node are
	considered completed as soon as the local disk write has
	occurred, and the replication packet has reached the peer
	node. Normally, no writes are lost in case of forced
	fail-over. However, in the event of simultaneous power failure
	on both nodes and concurrent, irreversible destruction of the
	primary's data store, the most recent writes completed on the
	primary may be lost.</para> 
    </formalpara>
    <formalpara id="fp-protocol-c">
      <title>Protocol C</title>
      <indexterm>
	<primary>replication mode</primary>
	<secondary>synchronous</secondary>
      </indexterm>
      <para>Synchronous replication protocol. Local write operations
	on the primary node are considered completed only after both
	the local and the remote disk write have been confirmed. As a
	result, loss of a single node is guaranteed not to lead to any
	data loss. Data loss is, of course, inevitable even with this
	replication protocol if both nodes (or their storage
	subsystems) are irreversibly destroyed at the same
	time.</para>
    </formalpara>
    <para>By far, the most commonly used replication protocol in DRBD
      setups is protocol C.</para>
    <note>
      <para>The choice of replication protocol influences two factors
	of your deployment: <emphasis>protection</emphasis> and
	<emphasis>latency</emphasis>. <emphasis>Throughput</emphasis>,
	by contrast, is largely independent of the replication
	protocol selected.</para>
    </note>
    <para>See <xref linkend="s-configure-resource"/> for an example
      resource configuration which demonstrates replication protocol
      configuration.</para>
  </section>
  <section id="s-resync">
    <title>Efficient synchronization</title>
    <para><indexterm>
	<primary>synchronization</primary>
    </indexterm>
    <indexterm>
	<primary>resynchronization</primary>
	<see>synchronization</see>
    </indexterm>(Re-)synchronization is distinct from device
      replication. While replication occurs on any write event to a
      resource in the primary role, synchronization is decoupled from
      incoming writes. Rather, it affects the device as a
      whole.</para>
    <para>
      Synchronization is necessary if the replication link has been
      interrupted for any reason, be it due to failure of the primary
      node, failure of the secondary node, or interruption of the
      replication link. Synchronization is efficient in the sense
      that DRBD does not synchronize modified blocks in the order
      they were originally written, but in linear order, which has the
      following consequences:
      <itemizedlist>
	<listitem>
	  <para>Synchronization is fast, since blocks in which
	    several successive write operations occurred are only
	    synchronized once.</para>
	</listitem>
	<listitem>
	  <para>Synchronization is also associated with few disk
	    seeks, as blocks are synchronized according to the natural
	    on-disk block layout.</para>
	</listitem>
	<listitem>
	  <para><indexterm>
	      <primary>disk state</primary>
	      <secondary>Inconsistent</secondary>
	    </indexterm>
	    <indexterm>
	      <primary>Inconsistent (disk state)</primary>
	    </indexterm>During synchronization, the data set on the
	    standby node is partly obsolete and partly already
	    updated. This state of data is called
	    <emphasis>inconsistent</emphasis>.</para></listitem>
      </itemizedlist>
    </para>
    <para>A node with inconsistent data generally cannot be put into
      operation, thus it is desirable to keep the time period during
      which a node is inconsistent as short as possible. The service
      continues to run uninterrupted on the active node, while
      background synchronization is in progress.</para>
    <para>You may estimate the expected sync time based on the
      following simple formula:
      <indexterm>
	<primary>synchronization</primary>
	<secondary>estimating duration</secondary>
      </indexterm>
      <equation id="eq-resync-time">
	<title>Synchronization time</title>
	<alt>\[t_{sync}=\frac{D}{R}\]</alt>
	<graphic fileref="resync-time"/>
      </equation>
      <replaceable>t<subscript>sync</subscript></replaceable> is the
      expected sync time. <replaceable>D</replaceable> is the amount
      of data to be synchronized, which you are unlikely to have any
      influence over (this is the amount of data that was modified by
      your application while the replication link was broken).
      <replaceable>R</replaceable> is the rate of synchronization,
      which is configurable &mdash; bounded by the throughput
      limitations of the replication network and I/O subsystem.</para>
    <para>See <xref linkend="s-configure-syncer-rate"/> for
      configuration suggestions with respect to
      synchronization.</para>
  </section>
  <section id="s-online-verify">
    <title>On-line device verification</title>
    <para>This feature is available in DRBD 8.2.5 and later.</para>
    <para><indexterm>
	<primary>on-line device verification</primary>
    </indexterm>
    <indexterm>
	<primary>device verification</primary>
	<see>on-line device verification</see>
    </indexterm>On-line device verification enables users to do a
      block-by-block data integrity check between nodes in a very
      efficient manner.
      <note>
	<para>Note that <quote>efficient</quote> refers to efficient
	  use of network bandwidth here, and to the fact that
	  verification does not break redundancy in any way. On-line
	  verification is still a resource-intensive operation, with a
	  noticeable impact on CPU utilization and load
	  average.</para>
      </note>
    </para>
    <para>It works by one node (the <emphasis>verification
	source</emphasis>) sequentially calculating a cryptographic
      digest of every block stored on the lower-level storage device
      of a particular resource. DRBD then transmits that digest to the
      peer node (the <emphasis>verification target</emphasis>), where
      it is checked against a digest of the local copy of the affected
      block. If the digests do not match, the block is marked
      out-of-sync and may later be synchronized. Because DRBD
      transmits just the digests, not the full blocks, on-line
      verification uses network bandwidth very efficiently.</para>
    <para>The process is termed <emphasis>on-line</emphasis>
      verification because it does not require that the DRBD resource
      being verified is unused at the time of verification. Thus,
      though it does carry a slight performance penalty while it is
      running, on-line verification does not cause service
      interruption or system down time &mdash; neither during the
      verification run nor during subsequent synchronization.</para>
    <para>It is a common use case to have on-line verification managed
      by the local <code>cron</code> daemon, running it, for example,
      one a week or once a month.</para>
    <para>See <xref linkend="s-use-online-verify"/> for information on
      how to enable, invoke, and automate on-line verification.</para>
  </section>
  <section id="s-integrity-check">
    <title>Replication traffic integrity checking</title>
    <para>This feature is available in DRBD 8.2.0 and later.</para>
    <para><indexterm>
	<primary>replication traffic integrity checking</primary>
    </indexterm><indexterm>
	<primary>data integrity checksums</primary>
	<see>replication traffic integrity checking</see>
    </indexterm><indexterm>
	<primary>checksums</primary>
	<see>replication traffic integrity checking</see>
    </indexterm>DRBD optionally performs end-to-end message integrity
      checking using cryptographic message digest algorithms such as
      MD5, SHA-1 or CRC-32C.
      <note>
	<para>These message digest algorithms are not
	  <emphasis>provided</emphasis> by DRBD. The Linux kernel
	  crypto API provides these; DRBD merely uses them. Thus, DRBD
	  is capable of utilizing any message digest algorithm
	  available in a particular system's kernel
	  configuration.</para>
      </note>
    </para>
    <para>With this feature enabled, DRBD generates a message digest
      of every data block it replicates to the peer, which the peer
      then uses to verify the integrity of the replication packet. If
      the replicated block can not be verified against the digest, the
      peer requests retransmission. Thus, DRBD replication is
      protected against several error sources, all of which, if
      unchecked, would potentially lead to data corruption during the
      replication process:
      <itemizedlist>
	<listitem>
	  <para>Bitwise errors ("bit flips") occurring on data in
	    transit between main memory and the network interface on
	    the sending node (which goes undetected by TCP
	    checksumming if it is offloaded to the network card, as is
	    common in recent implementations);</para>
	</listitem>
	<listitem>
	  <para>bit flips occuring on data in transit from the network
	    interface to main memory on the receiving node (the same
	    considerations apply for TCP checksum offloading);</para>
	</listitem>
	<listitem>
	  <para>any form of corruption due to a race conditions or
	    bugs in network interface firmware or drivers;</para>
	</listitem>
	<listitem>
	  <para>bit flips or random corruption injected by some
	    reassembling network component between nodes (if not using
	    direct, back-to-back connections).</para>
	</listitem>
      </itemizedlist>
    </para>
  </section>
  <section id="s-auto-split-brain-recovery">
    <title>Automatic split brain recovery</title>
    <para>This feature, in its current incarnation, is available in
      DRBD 8.0 and later. Automatic split brain recovery was available
      in DRBD 0.7, albeit using only the <quote>discard modifications
	on the younger primary</quote> strategy, which was not
      configurable. Automatic split brain recovery is disabled by
      default from DRBD 8 onwards.</para>
    <para><indexterm>
	<primary>split brain</primary>
	<secondary>automatic recovery</secondary>
    </indexterm>Split brain is a situation where, due to temporary
      failure of all network links between cluster nodes, and possibly
      due to intervention by a cluster management software or human
      error, both nodes switched to the primary role while
      disconnected. This is a potentially harmful state, as it implies
      that modifications to the data might have been made on either
      node, without have been replicated to the peer. Thus, it is
      likely in this situation that two diverging sets of data have
      been created, which cannot be trivially merged.</para>
    <para>While the recommended course of action in this scenario is
      to <link linkend="s-manual-split-brain-recovery">manually resolve the
	split brain</link> and then eliminate its root cause, it may
      be desirable, in some cases, to automate the process. DRBD has
      several resolution algorithms available for doing so:
      <itemizedlist>
	<listitem>
	  <formalpara>
	    <title>Discarding modifications made on the
	      <quote>younger</quote> primary</title>
	    <para>In this mode, when the network connection is
	      re-established and split brain is discovered, DRBD will
	      discard modifications made, in the meantime, on the node
	      which switched to the primary role
	      <emphasis>last</emphasis>.
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title>Discarding modifications made on the
	      <quote>older</quote> primary</title>
	    <para>In this mode, DRBD will discard modifications made,
	      in the meantime, on the node which switched to the
	      primary role <emphasis>first</emphasis>.
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title>Discarding modifications on the primary with fewer
	      changes</title>
	    <para>In this mode, DRBD will check which of the two nodes
	      has recorded fewer modifications, and will then discard
	      <emphasis>all</emphasis> modifications made on that
	      host.
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title>Graceful recovery from split brain if one host has
	      had no intermediate changes</title>
	    <para>In this mode, if one of the hosts has made no
	      modifications at all during split brain, DRBD will
	      simply recover gracefully and declare the split brain
	      resolved. Note that this is a fairly unlikely scenario.
	      Even if both hosts only mounted the file system on the
	      DRBD block device (even read-only), the device contents
	      would be modified, ruling out the possibility of
	      automatic recovery.
	    </para>
	  </formalpara>
	</listitem>
      </itemizedlist>
    </para>
    <caution>
      <para><indexterm>
	  <primary>split brain</primary>
	  <secondary>automatic recovery</secondary>
	  <tertiary>caveats</tertiary>
      </indexterm>Whether or not automatic split brain recovery is
	acceptable depends largely on the individual application.
	Consider the example of DRBD hosting a database. The
	<quote>discard modifications from host with fewer
	  changes</quote> approach may be fine for a web application
	click-through database. By contrast, it may be totally
	unacceptable to automatically discard <emphasis>any</emphasis>
	modifications made to a financial database, requiring manual
	recovery in any split brain event. Consider your application's
	requirements carefully before enabling automatic split brain
	recovery.</para>
      </caution>
  </section>
  <section id="s-disk-flush-support" status="draft">
    <title>Support for disk flushes</title>
    <para><indexterm>
	<primary>disk flushes</primary>
      </indexterm>
      <indexterm>
	<primary>flush()</primary>
	<see>disk flushes</see>
      </indexterm>When local block devices such as hard drives or RAID
      logical disks have write caching enabled, writes to these
      devices are considered <quote>completed</quote> as early as they
      have reached reached the volatile cache. Controller
      manufacturers typically refer to this as
      <phrase>write-back</phrase> mode, the opposite being
      <phrase>write-through</phrase>. If a power outage occurs on a
      controller in write-back mode, the most recent pending writes
      last writes are never committed to the disk, potentially causing
      data loss.</para>
    <para>To counteract this, DRBD makes use of disk flushes. A disk
      flush is a write operation that completes only when the
      associated data has been committed to stable (non-volatile)
      storage &mdash; that is to say, it has effectively been written
      to disk, rather than to the cache. DRBD uses disk flushes for
      write operations both to its replicated data set and to its meta
      data. In effect, DRBD circumvents the write cache in situations
      it deems necessary, as in <link
	linkend="s-activity-log">activity log</link> updates or
      enforcement of implicit write-after-write dependencies. This
      means additional reliability even in the face of power
      failure.</para>
    <para>It is important to understand that DRBD can use disk flushes
      only when layered on top of backing devices that support them.
      Most reasonably recent kernels support disk flushes for most
      SCSI and SATA devices. Linux software RAID (md) supports disk
      flushes for RAID-1, provided all component devices support them
      too. The same is true for device-mapper devices (LVM2, dm-raid,
      multipath).</para>
    <para>Controllers with battery-backed write cache (BBWC) use a
      battery to back up their volatile storage. On such devices, when
      power is restored after an outage, the controller flushes the
      most recent pending writes out to disk from the battery-backed
      cache, ensuring all writes committed to the volatile cache are
      actually transferred to stable storage. When running DRBD on top
      of such devices, it may be acceptable to disable disk flushes,
      thereby improving DRBD's write performance. See <xref
	linkend="s-disable-flushes"/> for
      details.</para>
  </section>
  <section id="s-handling-disk-errors">
    <title>Disk error handling strategies</title>
    <para><indexterm>
	<primary>disk errors</primary>
	<seealso>I/O errors</seealso>
    </indexterm>
    <indexterm>
	<primary>I/O errors</primary>
    </indexterm>If a hard drive that is used as a backing block
      device for DRBD on one of the nodes fails, DRBD may either pass
      on the I/O error to the upper layer (usually the file system) or
      it can mask I/O errors from upper layers.</para>
    <formalpara id="fp-io-error-pass-on">
      <title>Passing on I/O errors</title>
      <para>If DRBD is configured to <quote>pass on</quote> I/O
	errors, any such errors occuring on the lower-level device are
	transparently passed to upper I/O layers. Thus, it is left to
	upper layers to deal with such errors (this may result in a
	file system being remounted read-only, for example). This
	strategy does not ensure service continuity, and is hence not
	recommended for most users.</para>
    </formalpara>
    <formalpara id="fp-io-error-detach">
      <title>Masking I/O errors</title>
      <para>
	If DRBD is configured to <emphasis>detach</emphasis> on
	lower-level I/O error, DRBD will do so, automatically, upon
	occurrence of the first lower-level I/O error. The I/O error
	is masked from upper layers while DRBD transparently fetches
	the affected block from the peer node, over the network. From
	then onwards, DRBD is said to operate in <emphasis>diskless
	  mode</emphasis>, and carries out all subsequent I/O
	operations, read and write, on the peer node. Performance in
	this mode is inevitably expected to suffer, but the service
	continues without interruption, and can be moved to the peer
	node in a deliberate fashion at a convenient time.</para>
    </formalpara>
    <para>See <xref linkend="s-configure-io-error-behavior"/> for
      information on configuring I/O error handling strategies for
      DRBD.</para>
  </section>
  <section id="s-outdate">
    <title>Strategies for dealing with outdated data</title>
    <para><indexterm>
	<primary>disk state</primary>
	<secondary>Outdated</secondary>
    </indexterm><indexterm>
	<primary>Outdated (disk state)</primary>
    </indexterm><indexterm>
	<primary>disk state</primary>
	<secondary>Inconsistent</secondary>
    </indexterm><indexterm>
	<primary>Inconsistent (disk state)</primary>
    </indexterm>DRBD distinguishes between
      <emphasis>inconsistent</emphasis> and
      <emphasis>outdated</emphasis> data. Inconsistent data is data
      that cannot be expected to be accessible and useful in any
      manner. The prime example for this is data on a node that is
      currently the target of an on-going synchronization. Data on
      such a node is part obsolete, part up to date, and impossible to
      identify as either. Thus, for example, if the device holds a
      filesystem (as is commonly the case), that filesystem would be
      unexpected to mount or even pass an automatic filesystem
      check.</para>
    <para>Outdated data, by contrast, is data on a secondary node that
      is consistent, but no longer in sync with the primary node. This
      would occur in any interruption of the replication link, whether
      temporary or permanent. Data on an outdated, disconnected
      secondary node is expected to be clean, but it reflects a state
      of the peer node some time past. In order to avoid services
      using outdated data, DRBD disallows <link
	linkend="s-resource-roles">promoting</link> a resource that is
      in the outdated state.</para>
    <para>DRBD has interfaces that allows an external application to
      outdate a secondary node as soon as a network interruption
      occurs. DRBD will then refuse to switch the node to the primary
      role, preventing applications from using the outdated data. A
      complete implementation of this functionality exists for the
      <link linkend="ch-heartbeat">Heartbeat cluster management
	framework</link> (where it uses a communication channel
      separate from the DRBD replication link). However, the
      interfaces are generic and may be easily used by any other
      cluster management application.</para>
    <para>Whenever an outdated resource has its replication link
      re-established, its outdated flag is automatically cleared. A
      <link linkend="s-resync">background synchronization</link> then
      follows.</para>
    <para>See <xref linkend="s-heartbeat-dopd"/> for an example
      DRBD/Heartbeat configuration enabling protection against
      inadvertent use of outdated data.</para>
  </section>
</chapter>
