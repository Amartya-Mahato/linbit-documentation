<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-features">
  <title>DRBD Features</title>
  <abstract>
    <para>This chapter discusses various useful DRBD features, and
      gives some background information about them. Some of these
      features will be important to most users, some will only be
      relevant in very specific deployment scenarios.</para>
    <para><xref linkend="ch-admin"/> and <xref
      linkend="ch-troubleshooting"/> contain instructions on how to
      enable and use these features during day-to-day
      operation.</para>
  </abstract>
  <section id="s-primary">
    <title>Single vs. dual primary mode</title>
    <section id="s-single-primary-mode">
      <title>Single-primary mode</title>
      <para>In single-primary mode, any <link
	  linkend="s-resources">resource</link>is, at any given time,
	in the primary role on only one cluster member. Since it is
	thus guaranteed that only one cluster node manipulates the
	data at any moment, this mode can be used with any
	conventional file system (ext3, ext4, XFS etc.).</para>
      <para>Deploying DRBD in single-primary mode is the canonical
	approach for high availability (fail-over capable)
	clusters.</para>
    </section>
    <section id="s-dual-primary-mode">
      <title>Dual-primary mode</title>
      <subtitle>Available in DRBD versions 8.0 and later</subtitle>
      <para>In dual-primary mode, any resource is, at any
	given time, in the primary role on both cluster nodes.
	Since concurrent access to the data is thus unrestricted,
	this mode requires the use of a shared cluster file system
	that utilizes a distributed lock manager. Examples include GFS
	and OCFS2.</para>
      <para>Deploying DRBD in dual-primary mode is the preferred
	approach for load-balancing clusters which require concurrent
	data access from two nodes. This mode is disabled by default,
	and must be enabled explicitly in DRBD's configuration
	file.</para>
    </section>
  </section>
  <section id="s-replication-protocols">
    <title>Replication modes</title>
    <para>DRBD supports three distinct replication modes, allowing
      three degrees of replication synchronicity.</para>
    <formalpara>
      <title>Protocol A</title>
      <para>Asynchronous replication protocol. Local write operations
	on the Primary node are considered completed as soon as the
	local disk write has occurred, and the replication packet has
	been placed in the local TCP send buffer. In the event of
	forced failover, data loss may occur. The data on the standby
	node are consistent after the failover, however, the most
	recent updates performed prior to the crash could be
	lost.</para>
    </formalpara>
    <formalpara>
      <title>Protocol B</title>
      <para>Memory synchronous replication protocol. Local write
	operations on the Primary node are considered completed as
	soon as the local disk write has occurred, and the replication
	packet has reached the peer node. No writes are lost in case
	of forced failover. However, in the event of simultaneous
	power failure on both nodes and concurrent, irrevocable
	destruction of the Primary's data store, the most recent
	writes completed on the Primary may be lost.</para> 
    </formalpara>
    <formalpara>
      <title>Protocol C</title>
      <para>Synchronous replication protocol. Local write operations
	on the Primary node are considered completed only after both
	the local and the remote disk write have been confirmed. As a
	result, loss of a single node is guaranteed not to lead to any
	data loss.</para> 
    </formalpara>
    <note>
      <para>Data loss is, of course, inevitable with every replication
	protocol if both nodes are irrevocably destroyed at the same
	time.</para>
    </note>
    <para>Note that the choice of replication protocol influences two
      factors of your deployment: <emphasis>protection</emphasis> and
      <emphasis>latency</emphasis>. <emphasis>Throughput</emphasis>,
      by contrast, is largely independent of the replication protocol
      selected.</para>
    <para>By far, the most commonly used replication protocol in DRBD
      setups is protocol C.</para>
  </section>
  <section id="s-online-verify">
    <title>On-line device verification</title>
    <subtitle>Available in DRBD 8.2.4 and later</subtitle>
    <para></para>
  </section>
  <section id="s-integrity-check">
    <title>Replication traffic integrity checking</title>
    <subtitle>Available in DRBD 8.2.0 and later</subtitle>
    <para>DRBD optionally performs end-to-end message integrity
      checking using cryptographic message digest algorithms such as
      MD5, SHA-1 or CRC-32C.<footnote>
	<para>These message digest algorithms are not
	  <emphasis>provided</emphasis> by DRBD. The Linux kernel
	  crypto API provides these; DRBD merely uses them. Thus, DRBD
	  is capable of utilizing any message digest algorithm
	  available in a particular system's kernel
	  configuration.</para>
      </footnote>
    </para>
    <para>With this feature enabled, DRBD generates a message digest
      of every data block it replicates to the peer, which the peer
      then uses to verify the integrity of the replication packet. If
      the replicated block can not be verified against the digest, the
      peer requests retransmission. Thus, DRBD replication is
      protected against several error sources, all of which, if
      unchecked, would potentially lead to data corruption during the
      replication process:
      <itemizedlist>
	<listitem>
	  <para>Bitwise errors ("bit flips") occurring on data in
	    transit between main memory and the network interface on
	    the sending node (which goes undetected by TCP
	    checksumming if it is offloaded to the network card, as is
	    common in recent implementations);</para>
	</listitem>
	<listitem>
	  <para>bit flips occuring on data in transit from the network
	    interface to main memory on the receiving node (the same
	    considerations apply for TCP checksum offloading);</para>
	</listitem>
	<listitem>
	  <para>any form of corruption due to a race conditions or
	    bugs in network interface firmware or drivers;</para>
	</listitem>
	<listitem>
	  <para>bit flips or random corruption injected by some
	    reassembling network component between nodes (if not using
	    direct, back-to-back connections).</para>
	</listitem>
      </itemizedlist>
    </para>
  </section>
  <section id="s-auto-split-brain-recovery">
    <title>Automatic split brain recovery</title>
    <subtitle>Available in DRBD 8.0 and later</subtitle>
  </section>
  <section id="s-handling-disk-errors">
    <title>Methods of handling disk errors</title>
        <para>If a hard drive that is used as a backing block device for
      DRBD on one of the nodes fails, DRBD may either pass on the I/O
      error to the upper layer (usually the file system) or it can
      hide I/O errors from upper layers from IO errors. This is set by
      the <option>on-io-error</option> option.</para>
    <formalpara id="fp-io-error-pass-on">
      <title>Passing on I/O errors</title>
      <para>If <option>on-io-error</option> is set to
	<code>pass-on</code>, DRBD transparently passes on I/O errors
	from the backing device to upper I/O layers. Thus, it is left
	to upper layers to deal with such errors.<footnote>
	  <para>This may result in a file system being remounted
	    read-only, for example.</para></footnote> This option does
	not ensure service continuity, and is hence not recommended
	for most users.</para>
    </formalpara>
    <formalpara id="fp-io-error-detach">
      <title>Hiding I/O errors</title>
      <para>
	If <option>on-io-error</option> is set to <code>detach</code>
	(the default), DRBD will automatically detach from the backing
	device upon occurrence of the first lower-level I/O error. The
	I/O error is hidden from upper layers while DRBD transparently
	fetches the affected block from the peer node, over the
	network. From then onwards, DRBD is said to operate in
	<emphasis>diskless mode</emphasis>, and carries out all
	subsequent I/O operations, read and write, on the peer node.
	Performance in this mode in inevitably expected to suffer, but
	the service continues without interruption, and can be moved
	to the peer node in a deliberate fashion at a convenient
	time.</para>
    </formalpara>
  </section>
  <section id="s-outdate">
    <title>Dealing with outdated data</title>
  </section>
</chapter>
