<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-features">
  <title>DRBD Features</title>
  <abstract>
    <para>This chapter contains discussions of various useful DRBD
    features, and information on how to use and work with them. Some
    of these will be important to most users, some will only be
    relevant in very specific deployment scenarios.</para>
  </abstract>
  <section id="s-replication-protocols">
    <title>Replication protocols</title>
    <para>DRBD supports three distinct replication protocols.</para>
    <formalpara>
      <title>Protocol A</title>
      <para>Asynchronous replication protocol. Local write operations
	on the Primary node are considered completed as soon as the
	local disk write has occurred, and the replication packet has
	been placed in the local TCP send buffer. In the event of
	forced failover, data loss may occur. The data on the standby
	node are consistent after the failover, however, the most
	recent updates performed prior to the crash could be
	lost.</para>
    </formalpara>
    <formalpara>
      <title>Protocol B</title>
      <para>Memory synchronous replication protocol. Local write
	operations on the Primary node are considered completed as
	soon as the local disk write has occurred, and the replication
	packet has reached the peer node. No writes are lost in case
	of forced failover. However, in the event of simultaneous
	power failure on both nodes and concurrent, irrevocable
	destruction of the Primary's data store, the most recent
	writes completed on the Primary may be lost.</para> 
    </formalpara>
    <formalpara>
      <title>Protocol C</title>
      <para>Synchronous replication protocol. Local write operations
	on the Primary node are considered completed only after both
	the local and the remote disk write have been confirmed. As a
	result, loss of a single node is guaranteed not to lead to any
	data loss.</para> 
    </formalpara>
    <note>
      <para>Data loss is, of course, inevitable with every replication
	protocol if both nodes are irrevocably destroyed at the same
	time.</para>
    </note>
    <para>Note that the choice of replication protocol influences two
      factors of your deployment: <emphasis>protection</emphasis> and
      <emphasis>latency</emphasis>. <emphasis>Throughput</emphasis>,
      by contrast, is largely independent of the replication protocol
      selected.</para>
    <para>By far, the most commonly used replication protocol in DRBD
      setups is protocol C.</para>
  </section>
  <section id="s-handling-disk-errors">
    <title>Methods of handling disk errors</title>
        <para>If a hard drive that is used as a backing block device for
      DRBD on one of the nodes fails, DRBD may either pass on the I/O
      error to the upper layer (usually the file system) or it can
      hide I/O errors from upper layers from IO errors. This is set by
      the <option>on-io-error</option> option.</para>
    <formalpara id="fp-io-error-pass-on">
      <title>Passing on I/O errors</title>
      <para>If <option>on-io-error</option> is set to
	<code>pass-on</code>, DRBD transparently passes on I/O errors
	from the backing device to upper I/O layers. Thus, it is left
	to upper layers to deal with such errors.<footnote>
	  <para>This may result in a file system being remounted
	    read-only, for example.</para></footnote> This option does
	not ensure service continuity, and is hence not recommended
	for most users.</para>
    </formalpara>
    <formalpara id="fp-io-error-detach">
      <title>Hiding I/O errors</title>
      <para>
	If <option>on-io-error</option> is set to <code>detach</code>
	(the default), DRBD will automatically detach from the backing
	device upon occurrence of the first lower-level I/O error. The
	I/O error is hidden from upper layers while DRBD transparently
	fetches the affected block from the peer node, over the
	network. From then onwards, DRBD is said to operate in
	<emphasis>diskless mode</emphasis>, and carries out all
	subsequent I/O operations, read and write, on the peer node.
	Performance in this mode in inevitably expected to suffer, but
	the service continues without interruption, and can be moved
	to the peer node in a deliberate fashion at a convenient
	time.</para>
    </formalpara>
  </section>
  <section id="s-online-verify">
    <title>On-line device verification</title>
    <subtitle>Available in DRBD 8.2.4 and later</subtitle>
    <para></para>
  </section>
  <section id="s-integrity-check">
    <title>Replication traffic integrity checking</title>
    <subtitle>Available in DRBD 8.2.0 and later</subtitle>
    <para></para>
  </section>
  <section id="s-auto-split-brain-recovery">
    <title>Automatic split brain recovery</title>
    <subtitle>Available in DRBD 8.0 and later</subtitle>
  </section>
</chapter>
