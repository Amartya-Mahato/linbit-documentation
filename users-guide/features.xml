<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-features" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>DRBD Features</title>
  <para>
    <para>This chapter discusses various useful DRBD features, and
      gives some background information about them. Some of these
      features will be important to most users, some will only be
      relevant in very specific deployment scenarios.</para>
    <para><xref linkend="ch-admin"/> and <xref
      linkend="ch-troubleshooting"/> contain instructions on how to
      enable and use these features during day-to-day
      operation.</para>
  </para>
  <section id="s-single-primary-mode">
    <title>Single-primary mode</title>
    <para>In single-primary mode, any <link
	linkend="s-resources">resource</link> is, at any given time,
      in the primary role on only one cluster member. Since it is thus
      guaranteed that only one cluster node manipulates the data at
      any moment, this mode can be used with any conventional file
      system (ext3, ext4, XFS etc.).</para>
    <para>Deploying DRBD in single-primary mode is the canonical
      approach for high availability (fail-over capable)
      clusters.</para>
    </section>
  <section id="s-dual-primary-mode">
    <title>Dual-primary mode</title>
    <subtitle>Available in DRBD versions 8.0 and later</subtitle>
    <para>In dual-primary mode, any resource is, at any given time, in
      the primary role on both cluster nodes. Since concurrent access
      to the data is thus possible, this mode requires the use of a
      shared cluster file system that utilizes a distributed lock
      manager. Examples include <link linkend="ch-gfs">GFS</link> and
      OCFS2.</para>
    <para>Deploying DRBD in dual-primary mode is the preferred
      approach for load-balancing clusters which require concurrent
      data access from two nodes. This mode is disabled by default,
      and must be enabled explicitly in DRBD's configuration
      file.</para>
    <para>See <xref linkend="s-enable-dual-primary"/> for information
      on enabling dual-primary mode for specific resources.</para>
  </section>
  <section id="s-replication-protocols">
    <title>Replication modes</title>
    <para>DRBD supports three distinct replication modes, allowing
      three degrees of replication synchronicity.</para>
    <formalpara id="fp-protocol-a">
      <title>Protocol A</title>
      <para>Asynchronous replication protocol. Local write operations
	on the Primary node are considered completed as soon as the
	local disk write has occurred, and the replication packet has
	been placed in the local TCP send buffer. In the event of
	forced fail-over, data loss may occur. The data on the standby
	node is consistent after fail-over, however, the most recent
	updates performed prior to the crash could be lost.</para>
    </formalpara>
    <formalpara id="fp-protocol-b">
      <title>Protocol B</title>
      <para>Memory synchronous replication protocol. Local write
	operations on the Primary node are considered completed as
	soon as the local disk write has occurred, and the replication
	packet has reached the peer node. No writes are lost in case
	of forced fail-over. However, in the event of simultaneous
	power failure on both nodes and concurrent, irreversible
	destruction of the Primary's data store, the most recent
	writes completed on the Primary may be lost.</para> 
    </formalpara>
    <formalpara id="fp-protocol-c">
      <title>Protocol C</title>
      <para>Synchronous replication protocol. Local write operations
	on the Primary node are considered completed only after both
	the local and the remote disk write have been confirmed. As a
	result, loss of a single node is guaranteed not to lead to any
	data loss.</para> 
    </formalpara>
    <note>
      <para>Data loss is, of course, inevitable with every replication
	protocol if both nodes are irreversibly destroyed at the same
	time.</para>
    </note>
    <para>Note that the choice of replication protocol influences two
      factors of your deployment: <emphasis>protection</emphasis> and
      <emphasis>latency</emphasis>. <emphasis>Throughput</emphasis>,
      by contrast, is largely independent of the replication protocol
      selected.</para>
    <para>By far, the most commonly used replication protocol in DRBD
      setups is protocol C.</para>
    <para>See <xref linkend="s-configure-resource"/> for an example
      resource configuration which demonstrates replication protocol
      configuration.</para>
  </section>
  <section id="s-resync">
    <title>Efficient re-synchronization</title>
    <para>(Re-)synchronization is distinct from device replication.
      While replication occurs on any write event to a resource in the
      primary role, synchronization is decoupled from incoming writes.
      Rather, it affects the device as a whole.</para>
    <para>
      Re-synchronization is necessary if the replication link has been
      interrupted for any reason, be it due to failure of the primary
      node, failure of the secondary node, or interruption of the
      replication link. Re-synchronization is efficient in the sense
      that DRBD does not re-synchronize modified blocks in the order
      they were originally written, but in linear order, which has the
      following consequences:
      <itemizedlist>
	<listitem>
	  <para>Re-synchronization is fast, since blocks in which
	    several successive write operations occurred are only
	    synchronized once.</para>
	</listitem>
	<listitem>
	  <para>Re-synchronization is also associated with few disk
	    seeks, as blocks are synchronized according to the natural
	    on-disk block layout.</para>
	</listitem>
	<listitem>
	  <para>During re-synchronization, the data set on the standby
	    node is partly obsolete and partly already updated. This
	    state of data is called
	    <emphasis>inconsistent</emphasis>.</para></listitem>
      </itemizedlist>
    </para>
    <para>A node with inconsistent data generally cannot be put into
      operation, thus it is desirable to keep the time period during
      which a node is inconsistent as short as possible. The service
      continues to run uninterrupted on the active node, while
      background re-synchronization is in progress.</para>
    <para>See <xref linkend="s-configure-syncer-rate"/> for
      configuration suggestions with respect to
      re-synchronization.</para>
  </section>
  <section id="s-online-verify">
    <title>On-line device verification</title>
    <subtitle>Available in DRBD 8.2.4 and later</subtitle>
    <para>On-line device verification enables users to do a
      block-by-block data integrity check between nodes in a very
      efficient manner.</para>
    <para>It works by one node (the <emphasis>verification
	source</emphasis>) sequentially calculating a cryptographic
      digest<footnote>
	<para>For this purpose, DRBD can utilize any message digest
	  algorithm available via the kernel crypto API. MD5, SHA-1,
	  or CRC-32C are among the algorithms typically used for this
	  purpose.</para>
      </footnote>
      of every block stored on the lower-level storage device
      of a particular resource. DRBD then transmits that digest to the
      peer node (the <emphasis>verification target</emphasis>), where
      it is checked against a digest of the local copy of the affected
      block. If the digests do not match, the block is marked
      out-of-sync and may later be resynchronized. Because DRBD
      transmits just the digests, not the full blocks, on-line
      verification uses network bandwidth very efficiently.</para>
    <para>The process is termed <emphasis>on-line</emphasis>
      verification because it does not require that the DRBD resource
      being verified is unused at the time of verification. Thus,
      though it does carry a slight performance penalty while it is
      running, on-line verification does not cause service
      interruption or system down time &mdash; neither during the
      verification run nor during subsequent resynchronization.</para>
    <para>It is a common use case to have on-line verification managed
      by the local <code>cron</code> daemon, running it, for example,
      one a week or once a month.</para>
    <para>See <xref linkend="s-use-online-verify"/> for information on
      how to enable, invoke, and automate on-line verification.</para>
  </section>
  <section id="s-integrity-check">
    <title>Replication traffic integrity checking</title>
    <subtitle>Available in DRBD 8.2.0 and later</subtitle>
    <para>DRBD optionally performs end-to-end message integrity
      checking using cryptographic message digest algorithms such as
      MD5, SHA-1 or CRC-32C.<footnote>
	<para>These message digest algorithms are not
	  <emphasis>provided</emphasis> by DRBD. The Linux kernel
	  crypto API provides these; DRBD merely uses them. Thus, DRBD
	  is capable of utilizing any message digest algorithm
	  available in a particular system's kernel
	  configuration.</para>
      </footnote>
    </para>
    <para>With this feature enabled, DRBD generates a message digest
      of every data block it replicates to the peer, which the peer
      then uses to verify the integrity of the replication packet. If
      the replicated block can not be verified against the digest, the
      peer requests retransmission. Thus, DRBD replication is
      protected against several error sources, all of which, if
      unchecked, would potentially lead to data corruption during the
      replication process:
      <itemizedlist>
	<listitem>
	  <para>Bitwise errors ("bit flips") occurring on data in
	    transit between main memory and the network interface on
	    the sending node (which goes undetected by TCP
	    checksumming if it is offloaded to the network card, as is
	    common in recent implementations);</para>
	</listitem>
	<listitem>
	  <para>bit flips occuring on data in transit from the network
	    interface to main memory on the receiving node (the same
	    considerations apply for TCP checksum offloading);</para>
	</listitem>
	<listitem>
	  <para>any form of corruption due to a race conditions or
	    bugs in network interface firmware or drivers;</para>
	</listitem>
	<listitem>
	  <para>bit flips or random corruption injected by some
	    reassembling network component between nodes (if not using
	    direct, back-to-back connections).</para>
	</listitem>
      </itemizedlist>
    </para>
  </section>
  <section id="s-auto-split-brain-recovery">
    <title>Automatic split brain recovery</title>
    <subtitle>Available in DRBD 8.0 and later<footnote>
	<para>In fact, automatic split brain recovery was available in
	  DRBD 0.7, albeit using only the <quote>discard modifications
	    on the younger primary</quote> strategy, which was not
	  configurable. Automatic split brain recovery is disabled by
	  default from DRBD 8 onwards.</para>
      </footnote>
</subtitle>
    <para><emphasis>Split brain</emphasis> is a situation where, due
      to temporary failure of all network links between cluster nodes,
      and possibly due to intervention by a cluster management
      software or human error, both nodes switched to the primary role
      while disconnected. This is a potentially harmful state, as it
      implies that modifications to the data might have been made on
      either node, without have been replicated to the peer. Thus, it
      is likely in this situation that two diverging sets of data have
      been created, which cannot be trivially merged.</para>
    <para>While the recommended course of action in this scenario is
      to manually resolve the split brain and then eliminating its
      root cause, it may be desirable, in some cases, to automate the
      process. DRBD has several resolution algorithms available for
      doing so:
      <itemizedlist>
	<listitem>
	  <formalpara>
	    <title>Discarding modifications made on the
	      <quote>younger</quote> primary</title>
	    <para>In this mode, when the network connection is
	      re-established and split brain is discovered, DRBD will
	      discard modifications made, in the meantime, on the node
	      which switched to the primary role
	      <emphasis>last</emphasis>.
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title>Discarding modifications made on the
	      <quote>older</quote> primary</title>
	    <para>In this mode, DRBD will discard modifications made,
	      in the meantime, on the node which switched to the
	      primary role <emphasis>first</emphasis>.
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title>Discarding modifications on the primary with fewer changes</title>
	    <para>In this mode, DRBD will check which of the two nodes
	      has recorded fewer modifications, and will then discard
	      <emphasis>all</emphasis> modifications made on that
	      host.
	    </para>
	  </formalpara>
	</listitem>
	<listitem>
	  <formalpara>
	    <title>Graceful recovery from split brain if one host has
	      had no intermediate changes</title>
	    <para>In this mode, if one of the hosts has made no
	      modifications at all<footnote>
		<para>Note that this is a fairly unlikely scenario.
		  Even if both hosts only mounted the file system on
		  the DRBD block device (even read-only), the device
		  contents would be modified, ruling out the
		  possibility of automatic recovery.</para>
	      </footnote> during split brain, DRBD will simply recover
	      gracefully and declare the split brain resolved.</para>
	  </formalpara>
	</listitem>
      </itemizedlist>
    </para>
    <caution>
      <para>Whether or not automatic split brain recovery is
	acceptable depends largely on the individual application.
	Consider the example of DRBD hosting a database. The
	<quote>discard modifications from host with fewer
	  changes</quote> approach may be fine for a web application
	click-through database. By contrast, it may be totally
	unacceptable to automatically discard <emphasis>any</emphasis>
	modifications made to a financial database, requiring manual
	recovery in any split brain event. Consider your application's
	requirements carefully before enabling automatic split brain
	recovery.</para>
      </caution>
  </section>
  <section id="s-handling-disk-errors">
    <title>Disk error handling strategies</title>
        <para>If a hard drive that is used as a backing block device for
      DRBD on one of the nodes fails, DRBD may either pass on the I/O
      error to the upper layer (usually the file system) or it can
      hide I/O errors from upper layers from IO errors. This is set by
      the <option>on-io-error</option> option.</para>
    <formalpara id="fp-io-error-pass-on">
      <title>Passing on I/O errors</title>
      <para>If DRBD is configured to <quote>pass on</quote> I/O
	errors, any such errors occuring on the lower-level device are
	transparently passed to upper I/O layers. Thus, it is left to
	upper layers to deal with such errors.<footnote>
	  <para>This may result in a file system being remounted
	    read-only, for example.</para></footnote> This strategy
	does not ensure service continuity, and is hence not
	recommended for most users.</para>
    </formalpara>
    <formalpara id="fp-io-error-detach">
      <title>Hiding I/O errors</title>
      <para>
	If DRBD is configured to <emphasis>detach</emphasis> on
	lower-level I/O error, DRBD will do so, automatically, upon
	occurrence of the first lower-level I/O error. The I/O error
	is hidden from upper layers while DRBD transparently fetches
	the affected block from the peer node, over the network. From
	then onwards, DRBD is said to operate in <emphasis>diskless
	  mode</emphasis>, and carries out all subsequent I/O
	operations, read and write, on the peer node. Performance in
	this mode is inevitably expected to suffer, but the service
	continues without interruption, and can be moved to the peer
	node in a deliberate fashion at a convenient time.</para>
    </formalpara>
    <para>See <xref linkend="s-configure-io-error-behavior"/> for
      information on configuring I/O error handling strategies for
      DRBD.</para>
  </section>
  <section id="s-outdate">
    <title>Dealing with outdated data</title>
    <para>DRBD distinguishes between <emphasis>inconsistent</emphasis>
      and <emphasis>outdated</emphasis> data. Inconsistent data is
      data that cannot be expected to be accessible and useful in any
      manner. The prime example for this is data on a node that is
      currently the target of an on-going synchronization. Data on
      such a node is part obsolete, part up to date, and impossible
      to identify as either. Thus, for example, if the device holds a
      filesystem (as is commonly the case), that filesystem would be
      unexpected to mount or even pass an automatic filesystem
      check.</para>
    <para>Outdated data, by contrast, is data on a secondary node that
      is consistent, but no longer in sync with the primary node. This
      would occur in any interruption of the replication link, whether
      temporary or permanent. Data on an outdated, disconnected
      secondary node is expected to be clean, but it reflects a state
      of the primary node some time past.</para>
    <para>DRBD has interfaces that allows an external application to
      outdate a secondary node as soon as a network interruption
      occurs. DRBD will then refuse to switch the node to the primary
      role, preventing applications from using the outdated data. A
      complete implementation of this functionality exists for the
      <link linkend="ch-heartbeat">Heartbeat cluster management
	framework</link> (were it uses a communication channel
      separate from the DRBD replication link). However, the
      interfaces are generic and may be easily used by any other
      cluster management application.</para>
    <para>Whenever an outdated resource has its replication link
      re-established, it automatically changes to a synchronization
      target, efficiently brings its local disk up to date with
      respect to the current primary node, and returns to being a
      fully synchronized peer immediately thereafter.</para>
    <para>See <xref linkend="s-heartbeat-dopd"/> for an example
      DRBD/Heartbeat configuration enabling protection against
      inadvertent use of outdated data.</para>
  </section>
</chapter>
