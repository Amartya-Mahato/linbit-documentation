<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="ch-throughput" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Optimizing DRBD throughput</title>
  <section id="s-throughput-hardware">
    <title>Hardware considerations</title>
    <itemizedlist>
      <listitem>
	<formalpara>
	  <title>I/O subsystem throughput</title>
	  <para>I/O subsystem throughput is determined, largely, by
	    the number of disks that can be written to in parallel. A
	    single, reasonably recent, SCSI or SAS disk will typically
	    allow streaming writes of rougly 40MB/s to the single
	    disk. When deployed in a striping configuration, the I/O
	    subsystem will parallelize writes across disks,
	    effectively multiplying a single disk's throughput by the
	    number of stripes in the configuration. Thus the same,
	    40MB/s disks will allow effective throughput of 120MB/s in
	    a RAID-0 or RAID-1+0 configuration with three stripes, or
	    200MB/s with five stripes.</para>
	</formalpara>
	<note>
	  <para>Disk <emphasis>mirroring</emphasis> (RAID-1) in
	    hardware typically has little, if any effect on
	    throughput. Disk <emphasis>striping with parity</emphasis>
	    (RAID-5) does have an effect on throughput, usually an
	    adverse one when compared to striping.</para>
	</note>
      </listitem>
      <listitem>
	<formalpara>
	  <title>Network throughput</title>
	  <para>Network throughput is usually determined by the amount
	    of traffic present on the network, and on the throughput
	    of any routing/switching infrastructure present. These
	    concerns are, however, largely irrelevant in DRBD
	    replication links which are normally dedicated,
	    back-to-back network connections.</para>
	</formalpara>
	<para>Thus, network throughput may be improved either by
	  switching to a higher-throughput protocol (such as 10
	  Gigabit Ethernet), or by using link aggregation over several
	  network links, as one may do using the Linux
	  <code>bonding</code> network driver.</para>
      </listitem>
    </itemizedlist>
  </section>
  <section id="s-throughput-overhead-expectations">
    <title>Throughput overhead expectations</title>
    <para>When estimating the throughput overhead associated with
      DRBD, it is important to consider the following natural
      limitations:
	<itemizedlist>
	<listitem>
	  <para>DRBD throughput is limited by that of the raw I/O
	    subsystem.</para>
	</listitem>
	<listitem>
	  <para>DRBD throughput is limited by the available network
	    bandwidth.</para>
	</listitem>
	</itemizedlist>
    </para>
    <para>The <emphasis>minimum</emphasis> between the two establishes
      the theoretical throughput <emphasis>maximum</emphasis>
      available to DRBD. DRBD then reduces that throughput maximum by
      its additional throughput overhead, which can be expected to be
      less than 3 percent.</para>
    <itemizedlist>
      <listitem>
	<para>Consider the example of two cluster nodes containing I/O
	  subsystems capable of 200 MB/s throughput, with a Gigabit
	  Ethernet link available between them. Gigabit Ethernet can
	  be expected to produce 110 MB/s throughput for TCP
	  connections, thus the network connection would be the
	  <quote>bottleneck</quote> in this configuration and one
	  would expect about 107 MB/s maximum DRBD throughput.</para>
      </listitem>
      <listitem>
	<para>By contrast, if the I/O subsystem is capable of only 100
	  MB/s for sustained writes, then it constitutes the
	  bottleneck, and you would be able to expect only 97 MB/s
	  maximum DRBD throughput.</para>
      </listitem>
    </itemizedlist>
  </section>
  <section id="s-throughput-tuning">
    <title>DRBD tuning recommendations</title>
    <para>DRBD offers a number of configuration options which may have
      an effect on your system's throughput. This section list some
      recommendations for tuning for throughput. However, since
      throughput is largely hardware dependent, the effects of
      tweaking the options described here may vary greatly from system
      to system. It is important to understand that these
      recommendations should not be interpreted as <quote>silver
	bullets</quote> which would magically remove any and all
      throughput bottlenecks.</para>
    <section>
      <title>Setting <code>max-buffers</code> and
      <code>max-epoch-size</code></title>
      <para><xi:include href="todo.xml"/></para>
    </section>
    <section>
      <title>Tweaking the I/O unplug watermark</title>
      <para><xi:include href="todo.xml"/></para>
    </section>
    <section>
      <title>Tuning the TCP send buffer size</title>
      <para><xi:include href="todo.xml"/></para>
    </section>
    <section>
      <title>Tuning the Activity Log size</title>
      <para><xi:include href="todo.xml"/></para>
    </section>
  </section>
</chapter>
